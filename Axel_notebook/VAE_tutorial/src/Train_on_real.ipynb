{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import pandas as pd\n",
    "from skimage import io,img_as_float\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import model\n",
    "import model_conv\n",
    "import model_conv_bigger\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "matplotlib.style.use('ggplot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('/home/axtr7550/Chromosome_project/cellbgnet/')\n",
    "import cellbgnet\n",
    "import cellbgnet.utils\n",
    "\n",
    "from cellbgnet.datasets import DataSimulator\n",
    "from cellbgnet.utils.hardware import cpu, gpu\n",
    "from cellbgnet.model import CellBGModel\n",
    "from cellbgnet.simulation.psf_kernel import SMAPSplineCoefficient\n",
    "from cellbgnet.generic.emitter import EmitterSet\n",
    "from cellbgnet.train_loss_infer import generate_probmap_cells\n",
    "\n",
    "os.chdir('/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/src')\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.measure import label\n",
    "import random\n",
    "import edt\n",
    "from skimage.filters import gaussian\n",
    "from scipy.ndimage import rotate\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "#%matplotlib qt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = '/home/axtr7550/Chromosome_project/real_images/for_axel/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_coords = pd.read_csv(source_folder + 'fork_outputs/positions_df.csv')\n",
    "with open(source_folder + 'fork_outputs/train_cell_masks.txt', 'r') as f:\n",
    "    raw_image_names = f.readlines()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pos01_img_100.tif', 'Pos01_img_101.tif', 'Pos02_img_100.tif',\n",
       "       'Pos02_img_101.tif', 'Pos03_img_100.tif', 'Pos03_img_101.tif',\n",
       "       'Pos04_img_100.tif', 'Pos04_img_101.tif', 'Pos05_img_100.tif',\n",
       "       'Pos05_img_101.tif', 'Pos06_img_100.tif', 'Pos06_img_101.tif',\n",
       "       'Pos07_img_100.tif', 'Pos07_img_101.tif', 'Pos08_img_100.tif',\n",
       "       'Pos08_img_101.tif', 'Pos09_img_100.tif', 'Pos09_img_101.tif',\n",
       "       'Pos10_img_100.tif', 'Pos10_img_101.tif'], dtype='<U17')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_names = [name.strip().split('/')[-1] for  name in raw_image_names]\n",
    "image_names = np.array(image_names) \n",
    "image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_coords['img_name'] = image_names[image_coords['Image'].values.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>img_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>407</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>380</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>465</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>527</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>615</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6443</th>\n",
       "      <td>19</td>\n",
       "      <td>681</td>\n",
       "      <td>738</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6444</th>\n",
       "      <td>19</td>\n",
       "      <td>684</td>\n",
       "      <td>723</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6445</th>\n",
       "      <td>19</td>\n",
       "      <td>737</td>\n",
       "      <td>966</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6446</th>\n",
       "      <td>19</td>\n",
       "      <td>820</td>\n",
       "      <td>675</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6447</th>\n",
       "      <td>19</td>\n",
       "      <td>910</td>\n",
       "      <td>564</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6448 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Image    x    y           img_name\n",
       "0         0  101  407  Pos01_img_100.tif\n",
       "1         0  103  380  Pos01_img_100.tif\n",
       "2         0  106  465  Pos01_img_100.tif\n",
       "3         0  111  527  Pos01_img_100.tif\n",
       "4         0  114  615  Pos01_img_100.tif\n",
       "...     ...  ...  ...                ...\n",
       "6443     19  681  738  Pos10_img_101.tif\n",
       "6444     19  684  723  Pos10_img_101.tif\n",
       "6445     19  737  966  Pos10_img_101.tif\n",
       "6446     19  820  675  Pos10_img_101.tif\n",
       "6447     19  910  564  Pos10_img_101.tif\n",
       "\n",
       "[6448 rows x 4 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.zeros((20, 1041, 1302))\n",
    "\n",
    "for i in range(20):\n",
    "    img = io.imread(f'{source_folder}pool/{image_names[i]}').astype(np.int16)\n",
    "    images[i, ...] = torch.tensor(img)[img.shape[0]//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1041, 1302])\n"
     ]
    }
   ],
   "source": [
    "image = images[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val , min_val = images.max(), images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar, beta=1):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "\n",
    "    A beta here will affect how much the KLD matters\n",
    "\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "\n",
    "    return BCE + KLD*beta, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rois(image_coords, images):\n",
    "    image_data = torch.tensor(image_coords[[\"Image\", \"x\", \"y\"]].values, dtype=torch.int64)\n",
    "    rois = torch.zeros((image_data.shape[0], 24, 24))\n",
    "    \n",
    "    for i in range(image_data.shape[0]):\n",
    "        rois[i,:,:] = images[image_data[i,0], image_data[i,1]-12:image_data[i,1]+12, image_data[i,2]-12:image_data[i,2]+12]\n",
    "\n",
    "    #rois = rois/rois.max()\n",
    "    return rois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rois_paired(image_coords, images):\n",
    "    image_data = torch.tensor(image_coords[[\"Image\", \"x\", \"y\"]].values, dtype=torch.int64)\n",
    "    rois = torch.zeros((image_data.shape[0]//2,2, 24, 24))\n",
    "    \n",
    "    for i in range(0, image_data.shape[0],2):\n",
    "        for j in range(2):\n",
    "            try:\n",
    "                rois[i,j,:,:] = images[image_data[i+j,0], image_data[i+j,1]-12:image_data[i+j,1]+12, image_data[i+j,2]-12:image_data[i+j,2]+12]\n",
    "            except IndexError:\n",
    "                continue\n",
    "    #rois = rois/rois.max()\n",
    "    return rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, rois, batch_size=128, beta=1, zdim=1):\n",
    "    model.train()\n",
    "    \n",
    "    indeces = np.arange(rois.shape[0])\n",
    "    np.random.shuffle(indeces)\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(rois.shape[0]//batch_size):\n",
    "        data = rois[indeces[i*batch_size:(i+1)*batch_size], ...]\n",
    "        data = data.unsqueeze(1)\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar, _ = model(data)\n",
    "        bce_loss = criterion(reconstruction, data) # Reconstructs the noisy image\n",
    "        #se = torch.square(data - reconstruction)\n",
    "        #se = se*mask\n",
    "        #mse_loss = torch.sqrt(torch.sum(se))\n",
    "\n",
    "        loss, BCE, KLD = final_loss(bce_loss, mu, logvar, beta)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        save_mod = (1+1*epoch//10) if epoch < 100 else 10*(epoch//100)\n",
    "\n",
    "        if (rois.shape[0]//batch_size - 1) and (epoch % save_mod == 0):\n",
    "            num_rows = 8\n",
    "            both = torch.cat((data.view(batch_size, 1, 24, 24)[:8], \n",
    "                                reconstruction.view(batch_size, 1, 24, 24)[:8]))\n",
    "            save_image(both.cpu(), f\"/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/outputs/real_images/VAE_real_scale-1-7_beta{beta}_data_1_{epoch}_zdim_{zdim}.png\", nrow=num_rows, pad_value=1)\n",
    "\n",
    "    train_loss = running_loss/rois.shape[0]\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_conv_bigger.VAE_CNN_version_2_24x24(zDim=1).to(device)\n",
    "lr = 0.001\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "#criterion = nn.BCELoss(reduction='sum')\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                    mode='min',\n",
    "                                                    factor=0.2,\n",
    "                                                    patience=20,\n",
    "                                                    min_lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "type",
     "evalue": "tensor() missing 1 required positional arguments: \"data\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mtensor())\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor() missing 1 required positional arguments: \"data\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = create_rois(image_coords, images)\n",
    "#scaling = torch.quantile(torch.quantile(rois, 0.95, axis=1, keepdim=True), 0.95, axis=1, keepdim=True)\n",
    "scaling = rois.max(dim=1, keepdim=True)[0].max(dim=2, keepdim=True)[0]\n",
    "means = rois.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True)\n",
    "stds = rois.std(dim=1, keepdim=True).mean(dim=2, keepdim=True)\n",
    "scaling = torch.quantile(rois, 0.99)\n",
    "rois = (rois - means)/(stds)\n",
    "rois = rois.clamp(-1.5,1.5)\n",
    "rois = rois*0.33333+0.5\n",
    "rois = rois.clamp(0.0,1)\n",
    "\n",
    "#rois = rois/scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "rois = create_rois(image_coords, images)\n",
    "#scaling = torch.quantile(torch.quantile(rois, 0.95, axis=1, keepdim=True), 0.95, axis=1, keepdim=True)\n",
    "means = rois.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True)\n",
    "stds = rois.std(dim=1, keepdim=True).mean(dim=2, keepdim=True)\n",
    "rois = (rois - means)/(stds)\n",
    "rois = rois.clamp(-1, 7)\n",
    "rois = rois*1/8+1/8\n",
    "rois = rois.clamp(0.0,1)\n",
    "i = 1000\n",
    "plt.imshow(rois[i], cmap='gray', vmin=0, vmax=1)\n",
    "print(rois[i].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(567.) tensor(113.)\n",
      "tensor(17.8689) tensor(-3.4159)\n",
      "tensor(7.0711) tensor(-1.4600)\n",
      "tensor(7.0711) tensor(-1.4600)\n",
      "tensor(0.8071) tensor(-0.0460)\n",
      "tensor(0.8071) tensor(0.)\n",
      "tensor(0.8071)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rois = create_rois(image_coords, images)\n",
    "i = 150\n",
    "print(rois[i].max(), rois[i].min())\n",
    "\n",
    "#scaling = torch.quantile(torch.quantile(rois, 0.95, axis=1, keepdim=True), 0.95, axis=1, keepdim=True)\n",
    "means = rois.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True)\n",
    "\n",
    "stds = rois.std(dim=1, keepdim=True).mean(dim=2, keepdim=True)\n",
    "\n",
    "rois = (rois - means)/(stds)\n",
    "print(rois.max(), rois.min())\n",
    "\n",
    "print(rois[i].max(), rois[i].min())\n",
    "\n",
    "rois = rois.clamp(-10, 10)\n",
    "print(rois[i].max(), rois[i].min())\n",
    "\n",
    "rois = rois*1/10+1/10\n",
    "print(rois[i].max(), rois[i].min())\n",
    "\n",
    "rois = rois.clamp(0.0,1)\n",
    "print(rois[i].max(), rois[i].min())\n",
    "\n",
    "plt.imshow(rois[i], cmap='gray', vmin=0, vmax=1)\n",
    "print(rois[i].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones([1,1, 24, 24])\n",
    "\n",
    "for i in range(24):\n",
    "    for j in range(24):\n",
    "        mask[0,0,i,j] *= 1/(np.sqrt((12-i)**2+(12-j)**2)+ 2)\n",
    "mask = mask*10000\n",
    "mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_mse_loss(y_true, y_pred):\n",
    "    print(mask)\n",
    "    se = torch.sqrt((y_true - y_pred) ** 2)\n",
    "    se = se*mask\n",
    "    return torch.sum(se)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000) tensor(0.0527)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask[0,0,...])\n",
    "print(mask.cpu().max(), mask.cpu().min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(44723.4922)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se = torch.sqrt((rois - rois*mask) ** 2)\n",
    "se = se*mask\n",
    "mse_loss = torch.sum(se)\n",
    "mse_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8b9b63aa50>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "masked_roi = rois*mask\n",
    "plt.imshow(masked_roi[10,...], cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "type",
     "evalue": "can't assign a numpy.ndarray to a torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m paired_rois \u001b[39m=\u001b[39m create_rois_paired(image_coords, images)\n\u001b[1;32m      2\u001b[0m \u001b[39m#scaling = torch.quantile(torch.quantile(rois, 0.95, axis=1, keepdim=True), 0.95, axis=1, keepdim=True)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m means \u001b[39m=\u001b[39m paired_rois\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36mcreate_rois_paired\u001b[0;34m(image_coords, images)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m         rois[i,j,:,:] \u001b[39m=\u001b[39m images[image_data[i\u001b[39m+\u001b[39mj,\u001b[39m0\u001b[39m], image_data[i\u001b[39m+\u001b[39mj,\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m12\u001b[39m:image_data[i\u001b[39m+\u001b[39mj,\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m12\u001b[39m, image_data[i\u001b[39m+\u001b[39mj,\u001b[39m2\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m12\u001b[39m:image_data[i\u001b[39m+\u001b[39mj,\u001b[39m2\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m12\u001b[39m]\n\u001b[1;32m      9\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a numpy.ndarray to a torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "paired_rois = create_rois_paired(image_coords, images)\n",
    "#scaling = torch.quantile(torch.quantile(rois, 0.95, axis=1, keepdim=True), 0.95, axis=1, keepdim=True)\n",
    "means = paired_rois.mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "stds = paired_rois.std(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "paired_rois = (paired_rois - means)/(stds)\n",
    "paired_rois = paired_rois.clamp(-1, 7)\n",
    "paired_rois = paired_rois*1/8+1/8\n",
    "paired_rois = paired_rois.clamp(0.0,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000, 1010, 2):\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    axs[0].imshow(paired_rois[i,0], cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "    axs[1].imshow(paired_rois[i,1], cmap='gray', vmin=0, vmax=1)\n",
    "    fig.suptitle(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 5000\n",
      "Train Loss: 13.5044\n",
      "Epoch 2 of 5000\n",
      "Train Loss: 6.5918\n",
      "Epoch 3 of 5000\n",
      "Train Loss: 6.1960\n",
      "Epoch 4 of 5000\n",
      "Train Loss: 6.0584\n",
      "Epoch 5 of 5000\n",
      "Train Loss: 5.9597\n",
      "Epoch 6 of 5000\n",
      "Train Loss: 5.9370\n",
      "Epoch 7 of 5000\n",
      "Train Loss: 5.9106\n",
      "Epoch 8 of 5000\n",
      "Train Loss: 5.8840\n",
      "Epoch 9 of 5000\n",
      "Train Loss: 5.8707\n",
      "Epoch 10 of 5000\n",
      "Train Loss: 5.8622\n",
      "Epoch 11 of 5000\n",
      "Train Loss: 5.8619\n",
      "Epoch 12 of 5000\n",
      "Train Loss: 5.8533\n",
      "Epoch 13 of 5000\n",
      "Train Loss: 5.8493\n",
      "Epoch 14 of 5000\n",
      "Train Loss: 5.8303\n",
      "Epoch 15 of 5000\n",
      "Train Loss: 5.8132\n",
      "Epoch 16 of 5000\n",
      "Train Loss: 5.8172\n",
      "Epoch 17 of 5000\n",
      "Train Loss: 5.8359\n",
      "Epoch 18 of 5000\n",
      "Train Loss: 5.8250\n",
      "Epoch 19 of 5000\n",
      "Train Loss: 5.8223\n",
      "Epoch 20 of 5000\n",
      "Train Loss: 5.8240\n",
      "Epoch 21 of 5000\n",
      "Train Loss: 5.7995\n",
      "Epoch 22 of 5000\n",
      "Train Loss: 5.8056\n",
      "Epoch 23 of 5000\n",
      "Train Loss: 5.8045\n",
      "Epoch 24 of 5000\n",
      "Train Loss: 5.8096\n",
      "Epoch 25 of 5000\n",
      "Train Loss: 5.8194\n",
      "Epoch 26 of 5000\n",
      "Train Loss: 5.7649\n",
      "Epoch 27 of 5000\n",
      "Train Loss: 5.7969\n",
      "Epoch 28 of 5000\n",
      "Train Loss: 5.7815\n",
      "Epoch 29 of 5000\n",
      "Train Loss: 5.7868\n",
      "Epoch 30 of 5000\n",
      "Train Loss: 5.7979\n",
      "Epoch 31 of 5000\n",
      "Train Loss: 5.7712\n",
      "Epoch 32 of 5000\n",
      "Train Loss: 5.7981\n",
      "Epoch 33 of 5000\n",
      "Train Loss: 5.7925\n",
      "Epoch 34 of 5000\n",
      "Train Loss: 5.7910\n",
      "Epoch 35 of 5000\n",
      "Train Loss: 5.7905\n",
      "Epoch 36 of 5000\n",
      "Train Loss: 5.7754\n",
      "Epoch 37 of 5000\n",
      "Train Loss: 5.7723\n",
      "Epoch 38 of 5000\n",
      "Train Loss: 5.7858\n",
      "Epoch 39 of 5000\n",
      "Train Loss: 5.7651\n",
      "Epoch 40 of 5000\n",
      "Train Loss: 5.8101\n",
      "Epoch 41 of 5000\n",
      "Train Loss: 5.7873\n",
      "Epoch 42 of 5000\n",
      "Train Loss: 5.7494\n",
      "Epoch 43 of 5000\n",
      "Train Loss: 5.7813\n",
      "Epoch 44 of 5000\n",
      "Train Loss: 5.7605\n",
      "Epoch 45 of 5000\n",
      "Train Loss: 5.7594\n",
      "Epoch 46 of 5000\n",
      "Train Loss: 5.7680\n",
      "Epoch 47 of 5000\n",
      "Train Loss: 5.7672\n",
      "Epoch 48 of 5000\n",
      "Train Loss: 5.7544\n",
      "Epoch 49 of 5000\n",
      "Train Loss: 5.7754\n",
      "Epoch 50 of 5000\n",
      "Train Loss: 5.7698\n",
      "Epoch 51 of 5000\n",
      "Train Loss: 5.7587\n",
      "Epoch 52 of 5000\n",
      "Train Loss: 5.7582\n",
      "Epoch 53 of 5000\n",
      "Train Loss: 5.7729\n",
      "Epoch 54 of 5000\n",
      "Train Loss: 5.7329\n",
      "Epoch 55 of 5000\n",
      "Train Loss: 5.8035\n",
      "Epoch 56 of 5000\n",
      "Train Loss: 5.7664\n",
      "Epoch 57 of 5000\n",
      "Train Loss: 5.7620\n",
      "Epoch 58 of 5000\n",
      "Train Loss: 5.7572\n",
      "Epoch 59 of 5000\n",
      "Train Loss: 5.7552\n",
      "Epoch 60 of 5000\n",
      "Train Loss: 5.7603\n",
      "Epoch 61 of 5000\n",
      "Train Loss: 5.7685\n",
      "Epoch 62 of 5000\n",
      "Train Loss: 5.7485\n",
      "Epoch 63 of 5000\n",
      "Train Loss: 5.7821\n",
      "Epoch 64 of 5000\n",
      "Train Loss: 5.7659\n",
      "Epoch 65 of 5000\n",
      "Train Loss: 5.7581\n",
      "Epoch 66 of 5000\n",
      "Train Loss: 5.7589\n",
      "Epoch 67 of 5000\n",
      "Train Loss: 5.7538\n",
      "Epoch 68 of 5000\n",
      "Train Loss: 5.7570\n",
      "Epoch 69 of 5000\n",
      "Train Loss: 5.7708\n",
      "Epoch 70 of 5000\n",
      "Train Loss: 5.7602\n",
      "Epoch 71 of 5000\n",
      "Train Loss: 5.7564\n",
      "Epoch 72 of 5000\n",
      "Train Loss: 5.7537\n",
      "Epoch 73 of 5000\n",
      "Train Loss: 5.7386\n",
      "Epoch 74 of 5000\n",
      "Train Loss: 5.7425\n",
      "Epoch 75 of 5000\n",
      "Train Loss: 5.7421\n",
      "Epoch 76 of 5000\n",
      "Train Loss: 5.7652\n",
      "Epoch 77 of 5000\n",
      "Train Loss: 5.7558\n",
      "Epoch 78 of 5000\n",
      "Train Loss: 5.7564\n",
      "Epoch 79 of 5000\n",
      "Train Loss: 5.7726\n",
      "Epoch 80 of 5000\n",
      "Train Loss: 5.7616\n",
      "Epoch 81 of 5000\n",
      "Train Loss: 5.7464\n",
      "Epoch 82 of 5000\n",
      "Train Loss: 5.7465\n",
      "Epoch 83 of 5000\n",
      "Train Loss: 5.7587\n",
      "Epoch 84 of 5000\n",
      "Train Loss: 5.7184\n",
      "Epoch 85 of 5000\n",
      "Train Loss: 5.7466\n",
      "Epoch 86 of 5000\n",
      "Train Loss: 5.7520\n",
      "Epoch 87 of 5000\n",
      "Train Loss: 5.7389\n",
      "Epoch 88 of 5000\n",
      "Train Loss: 5.7595\n",
      "Epoch 89 of 5000\n",
      "Train Loss: 5.7560\n",
      "Epoch 90 of 5000\n",
      "Train Loss: 5.7478\n",
      "Epoch 91 of 5000\n",
      "Train Loss: 5.7537\n",
      "Epoch 92 of 5000\n",
      "Train Loss: 5.7519\n",
      "Epoch 93 of 5000\n",
      "Train Loss: 5.7277\n",
      "Epoch 94 of 5000\n",
      "Train Loss: 5.7428\n",
      "Epoch 95 of 5000\n",
      "Train Loss: 5.7590\n",
      "Epoch 96 of 5000\n",
      "Train Loss: 5.7380\n",
      "Epoch 97 of 5000\n",
      "Train Loss: 5.7468\n",
      "Epoch 98 of 5000\n",
      "Train Loss: 5.7415\n",
      "Epoch 99 of 5000\n",
      "Train Loss: 5.7588\n",
      "Epoch 100 of 5000\n",
      "Train Loss: 5.7633\n",
      "Epoch 101 of 5000\n",
      "Train Loss: 5.7423\n",
      "Epoch 102 of 5000\n",
      "Train Loss: 5.7394\n",
      "Epoch 103 of 5000\n",
      "Train Loss: 5.7403\n",
      "Epoch 104 of 5000\n",
      "Train Loss: 5.7661\n",
      "Epoch 105 of 5000\n",
      "Train Loss: 5.7567\n",
      "Epoch 106 of 5000\n",
      "Train Loss: 5.7411\n",
      "Epoch 107 of 5000\n",
      "Train Loss: 5.7328\n",
      "Epoch 108 of 5000\n",
      "Train Loss: 5.7548\n",
      "Epoch 109 of 5000\n",
      "Train Loss: 5.7385\n",
      "Epoch 110 of 5000\n",
      "Train Loss: 5.7482\n",
      "Epoch 111 of 5000\n",
      "Train Loss: 5.7299\n",
      "Epoch 112 of 5000\n",
      "Train Loss: 5.7355\n",
      "Epoch 113 of 5000\n",
      "Train Loss: 5.7641\n",
      "Epoch 114 of 5000\n",
      "Train Loss: 5.7376\n",
      "Epoch 115 of 5000\n",
      "Train Loss: 5.7519\n",
      "Epoch 116 of 5000\n",
      "Train Loss: 5.7449\n",
      "Epoch 117 of 5000\n",
      "Train Loss: 5.7323\n",
      "Epoch 118 of 5000\n",
      "Train Loss: 5.7497\n",
      "Epoch 119 of 5000\n",
      "Train Loss: 5.7305\n",
      "Epoch 120 of 5000\n",
      "Train Loss: 5.7539\n",
      "Epoch 121 of 5000\n",
      "Train Loss: 5.7313\n",
      "Epoch 122 of 5000\n",
      "Train Loss: 5.7591\n",
      "Epoch 123 of 5000\n",
      "Train Loss: 5.7413\n",
      "Epoch 124 of 5000\n",
      "Train Loss: 5.7298\n",
      "Epoch 125 of 5000\n",
      "Train Loss: 5.7470\n",
      "Epoch 126 of 5000\n",
      "Train Loss: 5.7492\n",
      "Epoch 127 of 5000\n",
      "Train Loss: 5.7397\n",
      "Epoch 128 of 5000\n",
      "Train Loss: 5.7308\n",
      "Epoch 129 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 130 of 5000\n",
      "Train Loss: 5.7332\n",
      "Epoch 131 of 5000\n",
      "Train Loss: 5.7291\n",
      "Epoch 132 of 5000\n",
      "Train Loss: 5.7433\n",
      "Epoch 133 of 5000\n",
      "Train Loss: 5.7439\n",
      "Epoch 134 of 5000\n",
      "Train Loss: 5.7190\n",
      "Epoch 135 of 5000\n",
      "Train Loss: 5.7338\n",
      "Epoch 136 of 5000\n",
      "Train Loss: 5.7299\n",
      "Epoch 137 of 5000\n",
      "Train Loss: 5.7335\n",
      "Epoch 138 of 5000\n",
      "Train Loss: 5.7417\n",
      "Epoch 139 of 5000\n",
      "Train Loss: 5.7452\n",
      "Epoch 140 of 5000\n",
      "Train Loss: 5.7322\n",
      "Epoch 141 of 5000\n",
      "Train Loss: 5.7366\n",
      "Epoch 142 of 5000\n",
      "Train Loss: 5.7330\n",
      "Epoch 143 of 5000\n",
      "Train Loss: 5.7139\n",
      "Epoch 144 of 5000\n",
      "Train Loss: 5.7326\n",
      "Epoch 145 of 5000\n",
      "Train Loss: 5.7465\n",
      "Epoch 146 of 5000\n",
      "Train Loss: 5.7558\n",
      "Epoch 147 of 5000\n",
      "Train Loss: 5.6986\n",
      "Epoch 148 of 5000\n",
      "Train Loss: 5.7188\n",
      "Epoch 149 of 5000\n",
      "Train Loss: 5.7291\n",
      "Epoch 150 of 5000\n",
      "Train Loss: 5.7125\n",
      "Epoch 151 of 5000\n",
      "Train Loss: 5.7519\n",
      "Epoch 152 of 5000\n",
      "Train Loss: 5.7390\n",
      "Epoch 153 of 5000\n",
      "Train Loss: 5.7670\n",
      "Epoch 154 of 5000\n",
      "Train Loss: 5.7413\n",
      "Epoch 155 of 5000\n",
      "Train Loss: 5.7186\n",
      "Epoch 156 of 5000\n",
      "Train Loss: 5.7253\n",
      "Epoch 157 of 5000\n",
      "Train Loss: 5.7531\n",
      "Epoch 158 of 5000\n",
      "Train Loss: 5.7374\n",
      "Epoch 159 of 5000\n",
      "Train Loss: 5.7264\n",
      "Epoch 160 of 5000\n",
      "Train Loss: 5.7176\n",
      "Epoch 161 of 5000\n",
      "Train Loss: 5.7501\n",
      "Epoch 162 of 5000\n",
      "Train Loss: 5.7323\n",
      "Epoch 163 of 5000\n",
      "Train Loss: 5.7236\n",
      "Epoch 164 of 5000\n",
      "Train Loss: 5.7560\n",
      "Epoch 165 of 5000\n",
      "Train Loss: 5.7202\n",
      "Epoch 166 of 5000\n",
      "Train Loss: 5.7122\n",
      "Epoch 167 of 5000\n",
      "Train Loss: 5.7350\n",
      "Epoch 168 of 5000\n",
      "Train Loss: 5.7494\n",
      "Epoch 169 of 5000\n",
      "Train Loss: 5.7705\n",
      "Epoch 170 of 5000\n",
      "Train Loss: 5.7342\n",
      "Epoch 171 of 5000\n",
      "Train Loss: 5.7265\n",
      "Epoch 172 of 5000\n",
      "Train Loss: 5.7480\n",
      "Epoch 173 of 5000\n",
      "Train Loss: 5.7143\n",
      "Epoch 174 of 5000\n",
      "Train Loss: 5.7428\n",
      "Epoch 175 of 5000\n",
      "Train Loss: 5.7313\n",
      "Epoch 176 of 5000\n",
      "Train Loss: 5.7396\n",
      "Epoch 177 of 5000\n",
      "Train Loss: 5.7613\n",
      "Epoch 178 of 5000\n",
      "Train Loss: 5.7415\n",
      "Epoch 179 of 5000\n",
      "Train Loss: 5.7300\n",
      "Epoch 180 of 5000\n",
      "Train Loss: 5.7199\n",
      "Epoch 181 of 5000\n",
      "Train Loss: 5.7226\n",
      "Epoch 182 of 5000\n",
      "Train Loss: 5.7309\n",
      "Epoch 183 of 5000\n",
      "Train Loss: 5.7384\n",
      "Epoch 184 of 5000\n",
      "Train Loss: 5.7386\n",
      "Epoch 185 of 5000\n",
      "Train Loss: 5.7359\n",
      "Epoch 186 of 5000\n",
      "Train Loss: 5.7212\n",
      "Epoch 187 of 5000\n",
      "Train Loss: 5.7312\n",
      "Epoch 188 of 5000\n",
      "Train Loss: 5.7500\n",
      "Epoch 189 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 190 of 5000\n",
      "Train Loss: 5.7164\n",
      "Epoch 191 of 5000\n",
      "Train Loss: 5.7408\n",
      "Epoch 192 of 5000\n",
      "Train Loss: 5.7511\n",
      "Epoch 193 of 5000\n",
      "Train Loss: 5.7339\n",
      "Epoch 194 of 5000\n",
      "Train Loss: 5.7363\n",
      "Epoch 195 of 5000\n",
      "Train Loss: 5.7444\n",
      "Epoch 196 of 5000\n",
      "Train Loss: 5.7229\n",
      "Epoch 197 of 5000\n",
      "Train Loss: 5.7261\n",
      "Epoch 198 of 5000\n",
      "Train Loss: 5.7474\n",
      "Epoch 199 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 200 of 5000\n",
      "Train Loss: 5.7460\n",
      "Epoch 201 of 5000\n",
      "Train Loss: 5.7445\n",
      "Epoch 202 of 5000\n",
      "Train Loss: 5.7512\n",
      "Epoch 203 of 5000\n",
      "Train Loss: 5.7139\n",
      "Epoch 204 of 5000\n",
      "Train Loss: 5.7470\n",
      "Epoch 205 of 5000\n",
      "Train Loss: 5.7385\n",
      "Epoch 206 of 5000\n",
      "Train Loss: 5.7457\n",
      "Epoch 207 of 5000\n",
      "Train Loss: 5.7441\n",
      "Epoch 208 of 5000\n",
      "Train Loss: 5.7250\n",
      "Epoch 209 of 5000\n",
      "Train Loss: 5.7254\n",
      "Epoch 210 of 5000\n",
      "Train Loss: 5.7149\n",
      "Epoch 211 of 5000\n",
      "Train Loss: 5.7390\n",
      "Epoch 212 of 5000\n",
      "Train Loss: 5.7181\n",
      "Epoch 213 of 5000\n",
      "Train Loss: 5.7194\n",
      "Epoch 214 of 5000\n",
      "Train Loss: 5.7325\n",
      "Epoch 215 of 5000\n",
      "Train Loss: 5.7400\n",
      "Epoch 216 of 5000\n",
      "Train Loss: 5.7499\n",
      "Epoch 217 of 5000\n",
      "Train Loss: 5.7524\n",
      "Epoch 218 of 5000\n",
      "Train Loss: 5.7137\n",
      "Epoch 219 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 220 of 5000\n",
      "Train Loss: 5.7244\n",
      "Epoch 221 of 5000\n",
      "Train Loss: 5.7385\n",
      "Epoch 222 of 5000\n",
      "Train Loss: 5.7321\n",
      "Epoch 223 of 5000\n",
      "Train Loss: 5.7381\n",
      "Epoch 224 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 225 of 5000\n",
      "Train Loss: 5.7279\n",
      "Epoch 226 of 5000\n",
      "Train Loss: 5.7273\n",
      "Epoch 227 of 5000\n",
      "Train Loss: 5.7182\n",
      "Epoch 228 of 5000\n",
      "Train Loss: 5.7299\n",
      "Epoch 229 of 5000\n",
      "Train Loss: 5.7260\n",
      "Epoch 230 of 5000\n",
      "Train Loss: 5.7151\n",
      "Epoch 231 of 5000\n",
      "Train Loss: 5.7521\n",
      "Epoch 232 of 5000\n",
      "Train Loss: 5.7304\n",
      "Epoch 233 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 234 of 5000\n",
      "Train Loss: 5.7268\n",
      "Epoch 235 of 5000\n",
      "Train Loss: 5.7299\n",
      "Epoch 236 of 5000\n",
      "Train Loss: 5.7357\n",
      "Epoch 237 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 238 of 5000\n",
      "Train Loss: 5.7445\n",
      "Epoch 239 of 5000\n",
      "Train Loss: 5.7282\n",
      "Epoch 240 of 5000\n",
      "Train Loss: 5.7390\n",
      "Epoch 241 of 5000\n",
      "Train Loss: 5.7285\n",
      "Epoch 242 of 5000\n",
      "Train Loss: 5.7254\n",
      "Epoch 243 of 5000\n",
      "Train Loss: 5.7289\n",
      "Epoch 244 of 5000\n",
      "Train Loss: 5.7239\n",
      "Epoch 245 of 5000\n",
      "Train Loss: 5.7485\n",
      "Epoch 246 of 5000\n",
      "Train Loss: 5.7243\n",
      "Epoch 247 of 5000\n",
      "Train Loss: 5.7172\n",
      "Epoch 248 of 5000\n",
      "Train Loss: 5.7169\n",
      "Epoch 249 of 5000\n",
      "Train Loss: 5.7525\n",
      "Epoch 250 of 5000\n",
      "Train Loss: 5.7288\n",
      "Epoch 251 of 5000\n",
      "Train Loss: 5.7193\n",
      "Epoch 252 of 5000\n",
      "Train Loss: 5.7229\n",
      "Epoch 253 of 5000\n",
      "Train Loss: 5.7253\n",
      "Epoch 254 of 5000\n",
      "Train Loss: 5.7168\n",
      "Epoch 255 of 5000\n",
      "Train Loss: 5.7334\n",
      "Epoch 256 of 5000\n",
      "Train Loss: 5.7206\n",
      "Epoch 257 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 258 of 5000\n",
      "Train Loss: 5.7328\n",
      "Epoch 259 of 5000\n",
      "Train Loss: 5.7198\n",
      "Epoch 260 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 261 of 5000\n",
      "Train Loss: 5.7056\n",
      "Epoch 262 of 5000\n",
      "Train Loss: 5.7306\n",
      "Epoch 263 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 264 of 5000\n",
      "Train Loss: 5.7147\n",
      "Epoch 265 of 5000\n",
      "Train Loss: 5.7434\n",
      "Epoch 266 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 267 of 5000\n",
      "Train Loss: 5.7225\n",
      "Epoch 268 of 5000\n",
      "Train Loss: 5.7480\n",
      "Epoch 269 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 270 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 271 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 272 of 5000\n",
      "Train Loss: 5.7143\n",
      "Epoch 273 of 5000\n",
      "Train Loss: 5.7059\n",
      "Epoch 274 of 5000\n",
      "Train Loss: 5.7375\n",
      "Epoch 275 of 5000\n",
      "Train Loss: 5.7485\n",
      "Epoch 276 of 5000\n",
      "Train Loss: 5.7246\n",
      "Epoch 277 of 5000\n",
      "Train Loss: 5.7570\n",
      "Epoch 278 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 279 of 5000\n",
      "Train Loss: 5.7441\n",
      "Epoch 280 of 5000\n",
      "Train Loss: 5.7232\n",
      "Epoch 281 of 5000\n",
      "Train Loss: 5.7283\n",
      "Epoch 282 of 5000\n",
      "Train Loss: 5.7253\n",
      "Epoch 283 of 5000\n",
      "Train Loss: 5.7256\n",
      "Epoch 284 of 5000\n",
      "Train Loss: 5.7365\n",
      "Epoch 285 of 5000\n",
      "Train Loss: 5.7336\n",
      "Epoch 286 of 5000\n",
      "Train Loss: 5.7165\n",
      "Epoch 287 of 5000\n",
      "Train Loss: 5.7303\n",
      "Epoch 288 of 5000\n",
      "Train Loss: 5.7281\n",
      "Epoch 289 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 290 of 5000\n",
      "Train Loss: 5.7105\n",
      "Epoch 291 of 5000\n",
      "Train Loss: 5.7233\n",
      "Epoch 292 of 5000\n",
      "Train Loss: 5.7041\n",
      "Epoch 293 of 5000\n",
      "Train Loss: 5.7413\n",
      "Epoch 294 of 5000\n",
      "Train Loss: 5.7239\n",
      "Epoch 295 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 296 of 5000\n",
      "Train Loss: 5.7213\n",
      "Epoch 297 of 5000\n",
      "Train Loss: 5.7370\n",
      "Epoch 298 of 5000\n",
      "Train Loss: 5.7258\n",
      "Epoch 299 of 5000\n",
      "Train Loss: 5.7199\n",
      "Epoch 300 of 5000\n",
      "Train Loss: 5.7220\n",
      "Epoch 301 of 5000\n",
      "Train Loss: 5.7338\n",
      "Epoch 302 of 5000\n",
      "Train Loss: 5.7174\n",
      "Epoch 303 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 304 of 5000\n",
      "Train Loss: 5.7157\n",
      "Epoch 305 of 5000\n",
      "Train Loss: 5.7106\n",
      "Epoch 306 of 5000\n",
      "Train Loss: 5.7309\n",
      "Epoch 307 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 308 of 5000\n",
      "Train Loss: 5.7122\n",
      "Epoch 309 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 310 of 5000\n",
      "Train Loss: 5.7485\n",
      "Epoch 311 of 5000\n",
      "Train Loss: 5.7220\n",
      "Epoch 312 of 5000\n",
      "Train Loss: 5.7191\n",
      "Epoch 313 of 5000\n",
      "Train Loss: 5.7226\n",
      "Epoch 314 of 5000\n",
      "Train Loss: 5.7442\n",
      "Epoch 315 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 316 of 5000\n",
      "Train Loss: 5.7160\n",
      "Epoch 317 of 5000\n",
      "Train Loss: 5.7386\n",
      "Epoch 318 of 5000\n",
      "Train Loss: 5.7174\n",
      "Epoch 319 of 5000\n",
      "Train Loss: 5.7145\n",
      "Epoch 320 of 5000\n",
      "Train Loss: 5.7235\n",
      "Epoch 321 of 5000\n",
      "Train Loss: 5.7332\n",
      "Epoch 322 of 5000\n",
      "Train Loss: 5.7249\n",
      "Epoch 323 of 5000\n",
      "Train Loss: 5.7135\n",
      "Epoch 324 of 5000\n",
      "Train Loss: 5.7117\n",
      "Epoch 325 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 326 of 5000\n",
      "Train Loss: 5.7158\n",
      "Epoch 327 of 5000\n",
      "Train Loss: 5.7410\n",
      "Epoch 328 of 5000\n",
      "Train Loss: 5.7128\n",
      "Epoch 329 of 5000\n",
      "Train Loss: 5.7324\n",
      "Epoch 330 of 5000\n",
      "Train Loss: 5.7274\n",
      "Epoch 331 of 5000\n",
      "Train Loss: 5.7315\n",
      "Epoch 332 of 5000\n",
      "Train Loss: 5.7196\n",
      "Epoch 333 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 334 of 5000\n",
      "Train Loss: 5.7358\n",
      "Epoch 335 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 336 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 337 of 5000\n",
      "Train Loss: 5.7117\n",
      "Epoch 338 of 5000\n",
      "Train Loss: 5.7148\n",
      "Epoch 339 of 5000\n",
      "Train Loss: 5.7228\n",
      "Epoch 340 of 5000\n",
      "Train Loss: 5.7270\n",
      "Epoch 341 of 5000\n",
      "Train Loss: 5.7174\n",
      "Epoch 342 of 5000\n",
      "Train Loss: 5.7323\n",
      "Epoch 343 of 5000\n",
      "Train Loss: 5.7146\n",
      "Epoch 344 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 345 of 5000\n",
      "Train Loss: 5.7054\n",
      "Epoch 346 of 5000\n",
      "Train Loss: 5.7124\n",
      "Epoch 347 of 5000\n",
      "Train Loss: 5.7271\n",
      "Epoch 348 of 5000\n",
      "Train Loss: 5.7154\n",
      "Epoch 349 of 5000\n",
      "Train Loss: 5.7326\n",
      "Epoch 350 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 351 of 5000\n",
      "Train Loss: 5.7202\n",
      "Epoch 352 of 5000\n",
      "Train Loss: 5.7265\n",
      "Epoch 353 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 354 of 5000\n",
      "Train Loss: 5.7204\n",
      "Epoch 355 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 356 of 5000\n",
      "Train Loss: 5.7140\n",
      "Epoch 357 of 5000\n",
      "Train Loss: 5.7270\n",
      "Epoch 358 of 5000\n",
      "Train Loss: 5.7176\n",
      "Epoch 359 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 360 of 5000\n",
      "Train Loss: 5.7186\n",
      "Epoch 361 of 5000\n",
      "Train Loss: 5.7029\n",
      "Epoch 362 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 363 of 5000\n",
      "Train Loss: 5.7247\n",
      "Epoch 364 of 5000\n",
      "Train Loss: 5.7220\n",
      "Epoch 365 of 5000\n",
      "Train Loss: 5.7202\n",
      "Epoch 366 of 5000\n",
      "Train Loss: 5.7171\n",
      "Epoch 367 of 5000\n",
      "Train Loss: 5.7169\n",
      "Epoch 368 of 5000\n",
      "Train Loss: 5.7307\n",
      "Epoch 369 of 5000\n",
      "Train Loss: 5.7095\n",
      "Epoch 370 of 5000\n",
      "Train Loss: 5.7179\n",
      "Epoch 371 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 372 of 5000\n",
      "Train Loss: 5.7100\n",
      "Epoch 373 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 374 of 5000\n",
      "Train Loss: 5.7340\n",
      "Epoch 375 of 5000\n",
      "Train Loss: 5.7305\n",
      "Epoch 376 of 5000\n",
      "Train Loss: 5.7145\n",
      "Epoch 377 of 5000\n",
      "Train Loss: 5.7448\n",
      "Epoch 378 of 5000\n",
      "Train Loss: 5.7219\n",
      "Epoch 379 of 5000\n",
      "Train Loss: 5.7167\n",
      "Epoch 380 of 5000\n",
      "Train Loss: 5.7205\n",
      "Epoch 381 of 5000\n",
      "Train Loss: 5.7165\n",
      "Epoch 382 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 383 of 5000\n",
      "Train Loss: 5.7250\n",
      "Epoch 384 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 385 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 386 of 5000\n",
      "Train Loss: 5.7249\n",
      "Epoch 387 of 5000\n",
      "Train Loss: 5.7217\n",
      "Epoch 388 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 389 of 5000\n",
      "Train Loss: 5.7295\n",
      "Epoch 390 of 5000\n",
      "Train Loss: 5.7168\n",
      "Epoch 391 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 392 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 393 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 394 of 5000\n",
      "Train Loss: 5.7238\n",
      "Epoch 395 of 5000\n",
      "Train Loss: 5.7132\n",
      "Epoch 396 of 5000\n",
      "Train Loss: 5.7215\n",
      "Epoch 397 of 5000\n",
      "Train Loss: 5.7371\n",
      "Epoch 398 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 399 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 400 of 5000\n",
      "Train Loss: 5.7483\n",
      "Epoch 401 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 402 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 403 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 404 of 5000\n",
      "Train Loss: 5.7100\n",
      "Epoch 405 of 5000\n",
      "Train Loss: 5.7098\n",
      "Epoch 406 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 407 of 5000\n",
      "Train Loss: 5.7167\n",
      "Epoch 408 of 5000\n",
      "Train Loss: 5.7296\n",
      "Epoch 409 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 410 of 5000\n",
      "Train Loss: 5.7120\n",
      "Epoch 411 of 5000\n",
      "Train Loss: 5.7234\n",
      "Epoch 412 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 413 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 414 of 5000\n",
      "Train Loss: 5.7136\n",
      "Epoch 415 of 5000\n",
      "Train Loss: 5.7165\n",
      "Epoch 416 of 5000\n",
      "Train Loss: 5.7193\n",
      "Epoch 417 of 5000\n",
      "Train Loss: 5.7161\n",
      "Epoch 418 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 419 of 5000\n",
      "Train Loss: 5.7115\n",
      "Epoch 420 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 421 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 422 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 423 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 424 of 5000\n",
      "Train Loss: 5.7245\n",
      "Epoch 425 of 5000\n",
      "Train Loss: 5.7090\n",
      "Epoch 426 of 5000\n",
      "Train Loss: 5.7346\n",
      "Epoch 427 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 428 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 429 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 430 of 5000\n",
      "Train Loss: 5.7206\n",
      "Epoch 431 of 5000\n",
      "Train Loss: 5.7411\n",
      "Epoch 432 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 433 of 5000\n",
      "Train Loss: 5.7144\n",
      "Epoch 434 of 5000\n",
      "Train Loss: 5.7158\n",
      "Epoch 435 of 5000\n",
      "Train Loss: 5.7137\n",
      "Epoch 436 of 5000\n",
      "Train Loss: 5.7098\n",
      "Epoch 437 of 5000\n",
      "Train Loss: 5.7191\n",
      "Epoch 438 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 439 of 5000\n",
      "Train Loss: 5.7276\n",
      "Epoch 440 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 441 of 5000\n",
      "Train Loss: 5.7070\n",
      "Epoch 442 of 5000\n",
      "Train Loss: 5.7162\n",
      "Epoch 443 of 5000\n",
      "Train Loss: 5.7352\n",
      "Epoch 444 of 5000\n",
      "Train Loss: 5.7239\n",
      "Epoch 445 of 5000\n",
      "Train Loss: 5.7201\n",
      "Epoch 446 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 447 of 5000\n",
      "Train Loss: 5.7198\n",
      "Epoch 448 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 449 of 5000\n",
      "Train Loss: 5.7039\n",
      "Epoch 450 of 5000\n",
      "Train Loss: 5.7292\n",
      "Epoch 451 of 5000\n",
      "Train Loss: 5.7187\n",
      "Epoch 452 of 5000\n",
      "Train Loss: 5.7312\n",
      "Epoch 453 of 5000\n",
      "Train Loss: 5.7276\n",
      "Epoch 454 of 5000\n",
      "Train Loss: 5.7209\n",
      "Epoch 455 of 5000\n",
      "Train Loss: 5.7185\n",
      "Epoch 456 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 457 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 458 of 5000\n",
      "Train Loss: 5.7289\n",
      "Epoch 459 of 5000\n",
      "Train Loss: 5.7215\n",
      "Epoch 460 of 5000\n",
      "Train Loss: 5.7309\n",
      "Epoch 461 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 462 of 5000\n",
      "Train Loss: 5.7131\n",
      "Epoch 463 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 464 of 5000\n",
      "Train Loss: 5.7360\n",
      "Epoch 465 of 5000\n",
      "Train Loss: 5.7256\n",
      "Epoch 466 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 467 of 5000\n",
      "Train Loss: 5.7099\n",
      "Epoch 468 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 469 of 5000\n",
      "Train Loss: 5.7366\n",
      "Epoch 470 of 5000\n",
      "Train Loss: 5.7170\n",
      "Epoch 471 of 5000\n",
      "Train Loss: 5.7402\n",
      "Epoch 472 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 473 of 5000\n",
      "Train Loss: 5.7230\n",
      "Epoch 474 of 5000\n",
      "Train Loss: 5.7145\n",
      "Epoch 475 of 5000\n",
      "Train Loss: 5.7108\n",
      "Epoch 476 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 477 of 5000\n",
      "Train Loss: 5.7229\n",
      "Epoch 478 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 479 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 480 of 5000\n",
      "Train Loss: 5.7145\n",
      "Epoch 481 of 5000\n",
      "Train Loss: 5.7168\n",
      "Epoch 482 of 5000\n",
      "Train Loss: 5.7277\n",
      "Epoch 483 of 5000\n",
      "Train Loss: 5.7148\n",
      "Epoch 484 of 5000\n",
      "Train Loss: 5.7209\n",
      "Epoch 485 of 5000\n",
      "Train Loss: 5.7306\n",
      "Epoch 486 of 5000\n",
      "Train Loss: 5.7181\n",
      "Epoch 487 of 5000\n",
      "Train Loss: 5.7354\n",
      "Epoch 488 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 489 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 490 of 5000\n",
      "Train Loss: 5.7150\n",
      "Epoch 491 of 5000\n",
      "Train Loss: 5.7208\n",
      "Epoch 492 of 5000\n",
      "Train Loss: 5.7068\n",
      "Epoch 493 of 5000\n",
      "Train Loss: 5.7291\n",
      "Epoch 494 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 495 of 5000\n",
      "Train Loss: 5.7128\n",
      "Epoch 496 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 497 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 498 of 5000\n",
      "Train Loss: 5.7167\n",
      "Epoch 499 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 500 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 501 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 502 of 5000\n",
      "Train Loss: 5.7145\n",
      "Epoch 503 of 5000\n",
      "Train Loss: 5.7107\n",
      "Epoch 504 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 505 of 5000\n",
      "Train Loss: 5.7266\n",
      "Epoch 506 of 5000\n",
      "Train Loss: 5.7092\n",
      "Epoch 507 of 5000\n",
      "Train Loss: 5.7106\n",
      "Epoch 508 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 509 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 510 of 5000\n",
      "Train Loss: 5.7187\n",
      "Epoch 511 of 5000\n",
      "Train Loss: 5.7229\n",
      "Epoch 512 of 5000\n",
      "Train Loss: 5.7156\n",
      "Epoch 513 of 5000\n",
      "Train Loss: 5.7264\n",
      "Epoch 514 of 5000\n",
      "Train Loss: 5.7097\n",
      "Epoch 515 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 516 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 517 of 5000\n",
      "Train Loss: 5.7154\n",
      "Epoch 518 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 519 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 520 of 5000\n",
      "Train Loss: 5.7157\n",
      "Epoch 521 of 5000\n",
      "Train Loss: 5.7163\n",
      "Epoch 522 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 523 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 524 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 525 of 5000\n",
      "Train Loss: 5.7170\n",
      "Epoch 526 of 5000\n",
      "Train Loss: 5.7013\n",
      "Epoch 527 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 528 of 5000\n",
      "Train Loss: 5.7144\n",
      "Epoch 529 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 530 of 5000\n",
      "Train Loss: 5.7219\n",
      "Epoch 531 of 5000\n",
      "Train Loss: 5.7170\n",
      "Epoch 532 of 5000\n",
      "Train Loss: 5.6940\n",
      "Epoch 533 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 534 of 5000\n",
      "Train Loss: 5.7125\n",
      "Epoch 535 of 5000\n",
      "Train Loss: 5.7119\n",
      "Epoch 536 of 5000\n",
      "Train Loss: 5.7243\n",
      "Epoch 537 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 538 of 5000\n",
      "Train Loss: 5.7184\n",
      "Epoch 539 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 540 of 5000\n",
      "Train Loss: 5.7252\n",
      "Epoch 541 of 5000\n",
      "Train Loss: 5.7063\n",
      "Epoch 542 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 543 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 544 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 545 of 5000\n",
      "Train Loss: 5.7242\n",
      "Epoch 546 of 5000\n",
      "Train Loss: 5.6892\n",
      "Epoch 547 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 548 of 5000\n",
      "Train Loss: 5.7437\n",
      "Epoch 549 of 5000\n",
      "Train Loss: 5.7242\n",
      "Epoch 550 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 551 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 552 of 5000\n",
      "Train Loss: 5.7176\n",
      "Epoch 553 of 5000\n",
      "Train Loss: 5.7099\n",
      "Epoch 554 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 555 of 5000\n",
      "Train Loss: 5.7211\n",
      "Epoch 556 of 5000\n",
      "Train Loss: 5.7265\n",
      "Epoch 557 of 5000\n",
      "Train Loss: 5.7110\n",
      "Epoch 558 of 5000\n",
      "Train Loss: 5.7146\n",
      "Epoch 559 of 5000\n",
      "Train Loss: 5.7161\n",
      "Epoch 560 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 561 of 5000\n",
      "Train Loss: 5.7106\n",
      "Epoch 562 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 563 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 564 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 565 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 566 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 567 of 5000\n",
      "Train Loss: 5.7128\n",
      "Epoch 568 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 569 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 570 of 5000\n",
      "Train Loss: 5.7018\n",
      "Epoch 571 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 572 of 5000\n",
      "Train Loss: 5.7293\n",
      "Epoch 573 of 5000\n",
      "Train Loss: 5.7135\n",
      "Epoch 574 of 5000\n",
      "Train Loss: 5.7204\n",
      "Epoch 575 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 576 of 5000\n",
      "Train Loss: 5.7366\n",
      "Epoch 577 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 578 of 5000\n",
      "Train Loss: 5.7009\n",
      "Epoch 579 of 5000\n",
      "Train Loss: 5.7230\n",
      "Epoch 580 of 5000\n",
      "Train Loss: 5.7132\n",
      "Epoch 581 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 582 of 5000\n",
      "Train Loss: 5.6992\n",
      "Epoch 583 of 5000\n",
      "Train Loss: 5.7161\n",
      "Epoch 584 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 585 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 586 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 587 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 588 of 5000\n",
      "Train Loss: 5.7216\n",
      "Epoch 589 of 5000\n",
      "Train Loss: 5.7131\n",
      "Epoch 590 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 591 of 5000\n",
      "Train Loss: 5.6653\n",
      "Epoch 592 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 593 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 594 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 595 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 596 of 5000\n",
      "Train Loss: 5.6822\n",
      "Epoch 597 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 598 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 599 of 5000\n",
      "Train Loss: 5.7229\n",
      "Epoch 600 of 5000\n",
      "Train Loss: 5.6871\n",
      "Epoch 601 of 5000\n",
      "Train Loss: 5.7167\n",
      "Epoch 602 of 5000\n",
      "Train Loss: 5.6819\n",
      "Epoch 603 of 5000\n",
      "Train Loss: 5.7222\n",
      "Epoch 604 of 5000\n",
      "Train Loss: 5.7029\n",
      "Epoch 605 of 5000\n",
      "Train Loss: 5.7201\n",
      "Epoch 606 of 5000\n",
      "Train Loss: 5.7276\n",
      "Epoch 607 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 608 of 5000\n",
      "Train Loss: 5.7246\n",
      "Epoch 609 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 610 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 611 of 5000\n",
      "Train Loss: 5.7114\n",
      "Epoch 612 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 613 of 5000\n",
      "Train Loss: 5.7133\n",
      "Epoch 614 of 5000\n",
      "Train Loss: 5.7241\n",
      "Epoch 615 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 616 of 5000\n",
      "Train Loss: 5.7134\n",
      "Epoch 617 of 5000\n",
      "Train Loss: 5.7210\n",
      "Epoch 618 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 619 of 5000\n",
      "Train Loss: 5.7140\n",
      "Epoch 620 of 5000\n",
      "Train Loss: 5.7097\n",
      "Epoch 621 of 5000\n",
      "Train Loss: 5.7192\n",
      "Epoch 622 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 623 of 5000\n",
      "Train Loss: 5.7106\n",
      "Epoch 624 of 5000\n",
      "Train Loss: 5.7391\n",
      "Epoch 625 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 626 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 627 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 628 of 5000\n",
      "Train Loss: 5.7216\n",
      "Epoch 629 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 630 of 5000\n",
      "Train Loss: 5.7105\n",
      "Epoch 631 of 5000\n",
      "Train Loss: 5.7246\n",
      "Epoch 632 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 633 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 634 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 635 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 636 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 637 of 5000\n",
      "Train Loss: 5.7197\n",
      "Epoch 638 of 5000\n",
      "Train Loss: 5.7222\n",
      "Epoch 639 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 640 of 5000\n",
      "Train Loss: 5.7148\n",
      "Epoch 641 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 642 of 5000\n",
      "Train Loss: 5.7160\n",
      "Epoch 643 of 5000\n",
      "Train Loss: 5.7192\n",
      "Epoch 644 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 645 of 5000\n",
      "Train Loss: 5.6986\n",
      "Epoch 646 of 5000\n",
      "Train Loss: 5.7237\n",
      "Epoch 647 of 5000\n",
      "Train Loss: 5.7218\n",
      "Epoch 648 of 5000\n",
      "Train Loss: 5.6872\n",
      "Epoch 649 of 5000\n",
      "Train Loss: 5.7078\n",
      "Epoch 650 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 651 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 652 of 5000\n",
      "Train Loss: 5.7362\n",
      "Epoch 653 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 654 of 5000\n",
      "Train Loss: 5.6989\n",
      "Epoch 655 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 656 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 657 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 658 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 659 of 5000\n",
      "Train Loss: 5.7133\n",
      "Epoch 660 of 5000\n",
      "Train Loss: 5.7235\n",
      "Epoch 661 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 662 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 663 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 664 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 665 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 666 of 5000\n",
      "Train Loss: 5.7188\n",
      "Epoch 667 of 5000\n",
      "Train Loss: 5.7219\n",
      "Epoch 668 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 669 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 670 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 671 of 5000\n",
      "Train Loss: 5.7109\n",
      "Epoch 672 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 673 of 5000\n",
      "Train Loss: 5.7312\n",
      "Epoch 674 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 675 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 676 of 5000\n",
      "Train Loss: 5.7213\n",
      "Epoch 677 of 5000\n",
      "Train Loss: 5.7123\n",
      "Epoch 678 of 5000\n",
      "Train Loss: 5.7204\n",
      "Epoch 679 of 5000\n",
      "Train Loss: 5.7068\n",
      "Epoch 680 of 5000\n",
      "Train Loss: 5.6901\n",
      "Epoch 681 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 682 of 5000\n",
      "Train Loss: 5.7009\n",
      "Epoch 683 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 684 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 685 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 686 of 5000\n",
      "Train Loss: 5.7095\n",
      "Epoch 687 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 688 of 5000\n",
      "Train Loss: 5.6992\n",
      "Epoch 689 of 5000\n",
      "Train Loss: 5.7171\n",
      "Epoch 690 of 5000\n",
      "Train Loss: 5.7184\n",
      "Epoch 691 of 5000\n",
      "Train Loss: 5.7418\n",
      "Epoch 692 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 693 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 694 of 5000\n",
      "Train Loss: 5.7161\n",
      "Epoch 695 of 5000\n",
      "Train Loss: 5.7102\n",
      "Epoch 696 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 697 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 698 of 5000\n",
      "Train Loss: 5.7123\n",
      "Epoch 699 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 700 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 701 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 702 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 703 of 5000\n",
      "Train Loss: 5.7163\n",
      "Epoch 704 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 705 of 5000\n",
      "Train Loss: 5.7271\n",
      "Epoch 706 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 707 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 708 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 709 of 5000\n",
      "Train Loss: 5.7350\n",
      "Epoch 710 of 5000\n",
      "Train Loss: 5.7164\n",
      "Epoch 711 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 712 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 713 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 714 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 715 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 716 of 5000\n",
      "Train Loss: 5.7115\n",
      "Epoch 717 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 718 of 5000\n",
      "Train Loss: 5.7292\n",
      "Epoch 719 of 5000\n",
      "Train Loss: 5.7318\n",
      "Epoch 720 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 721 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 722 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 723 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 724 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 725 of 5000\n",
      "Train Loss: 5.7140\n",
      "Epoch 726 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 727 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 728 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 729 of 5000\n",
      "Train Loss: 5.7263\n",
      "Epoch 730 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 731 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 732 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 733 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 734 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 735 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 736 of 5000\n",
      "Train Loss: 5.7077\n",
      "Epoch 737 of 5000\n",
      "Train Loss: 5.7115\n",
      "Epoch 738 of 5000\n",
      "Train Loss: 5.7123\n",
      "Epoch 739 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 740 of 5000\n",
      "Train Loss: 5.7198\n",
      "Epoch 741 of 5000\n",
      "Train Loss: 5.7157\n",
      "Epoch 742 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 743 of 5000\n",
      "Train Loss: 5.7076\n",
      "Epoch 744 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 745 of 5000\n",
      "Train Loss: 5.7212\n",
      "Epoch 746 of 5000\n",
      "Train Loss: 5.7234\n",
      "Epoch 747 of 5000\n",
      "Train Loss: 5.7178\n",
      "Epoch 748 of 5000\n",
      "Train Loss: 5.7346\n",
      "Epoch 749 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 750 of 5000\n",
      "Train Loss: 5.7019\n",
      "Epoch 751 of 5000\n",
      "Train Loss: 5.7019\n",
      "Epoch 752 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 753 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 754 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 755 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 756 of 5000\n",
      "Train Loss: 5.7341\n",
      "Epoch 757 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 758 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 759 of 5000\n",
      "Train Loss: 5.7041\n",
      "Epoch 760 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 761 of 5000\n",
      "Train Loss: 5.7160\n",
      "Epoch 762 of 5000\n",
      "Train Loss: 5.6839\n",
      "Epoch 763 of 5000\n",
      "Train Loss: 5.7188\n",
      "Epoch 764 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 765 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 766 of 5000\n",
      "Train Loss: 5.7234\n",
      "Epoch 767 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 768 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 769 of 5000\n",
      "Train Loss: 5.7100\n",
      "Epoch 770 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 771 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 772 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 773 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 774 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 775 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 776 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 777 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 778 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 779 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 780 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 781 of 5000\n",
      "Train Loss: 5.7267\n",
      "Epoch 782 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 783 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 784 of 5000\n",
      "Train Loss: 5.7117\n",
      "Epoch 785 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 786 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 787 of 5000\n",
      "Train Loss: 5.6961\n",
      "Epoch 788 of 5000\n",
      "Train Loss: 5.7293\n",
      "Epoch 789 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 790 of 5000\n",
      "Train Loss: 5.7198\n",
      "Epoch 791 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 792 of 5000\n",
      "Train Loss: 5.7169\n",
      "Epoch 793 of 5000\n",
      "Train Loss: 5.7124\n",
      "Epoch 794 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 795 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 796 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 797 of 5000\n",
      "Train Loss: 5.7327\n",
      "Epoch 798 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 799 of 5000\n",
      "Train Loss: 5.7159\n",
      "Epoch 800 of 5000\n",
      "Train Loss: 5.7115\n",
      "Epoch 801 of 5000\n",
      "Train Loss: 5.7070\n",
      "Epoch 802 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 803 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 804 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 805 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 806 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 807 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 808 of 5000\n",
      "Train Loss: 5.7018\n",
      "Epoch 809 of 5000\n",
      "Train Loss: 5.7240\n",
      "Epoch 810 of 5000\n",
      "Train Loss: 5.6696\n",
      "Epoch 811 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 812 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 813 of 5000\n",
      "Train Loss: 5.7114\n",
      "Epoch 814 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 815 of 5000\n",
      "Train Loss: 5.7428\n",
      "Epoch 816 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 817 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 818 of 5000\n",
      "Train Loss: 5.6801\n",
      "Epoch 819 of 5000\n",
      "Train Loss: 5.7290\n",
      "Epoch 820 of 5000\n",
      "Train Loss: 5.7120\n",
      "Epoch 821 of 5000\n",
      "Train Loss: 5.7139\n",
      "Epoch 822 of 5000\n",
      "Train Loss: 5.6736\n",
      "Epoch 823 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 824 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 825 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 826 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 827 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 828 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 829 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 830 of 5000\n",
      "Train Loss: 5.7123\n",
      "Epoch 831 of 5000\n",
      "Train Loss: 5.6733\n",
      "Epoch 832 of 5000\n",
      "Train Loss: 5.7080\n",
      "Epoch 833 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 834 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 835 of 5000\n",
      "Train Loss: 5.7252\n",
      "Epoch 836 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 837 of 5000\n",
      "Train Loss: 5.7204\n",
      "Epoch 838 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 839 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 840 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 841 of 5000\n",
      "Train Loss: 5.6989\n",
      "Epoch 842 of 5000\n",
      "Train Loss: 5.7287\n",
      "Epoch 843 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 844 of 5000\n",
      "Train Loss: 5.7076\n",
      "Epoch 845 of 5000\n",
      "Train Loss: 5.7096\n",
      "Epoch 846 of 5000\n",
      "Train Loss: 5.7153\n",
      "Epoch 847 of 5000\n",
      "Train Loss: 5.7208\n",
      "Epoch 848 of 5000\n",
      "Train Loss: 5.7290\n",
      "Epoch 849 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 850 of 5000\n",
      "Train Loss: 5.7056\n",
      "Epoch 851 of 5000\n",
      "Train Loss: 5.7063\n",
      "Epoch 852 of 5000\n",
      "Train Loss: 5.7070\n",
      "Epoch 853 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 854 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 855 of 5000\n",
      "Train Loss: 5.7170\n",
      "Epoch 856 of 5000\n",
      "Train Loss: 5.7148\n",
      "Epoch 857 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 858 of 5000\n",
      "Train Loss: 5.7095\n",
      "Epoch 859 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 860 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 861 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 862 of 5000\n",
      "Train Loss: 5.7159\n",
      "Epoch 863 of 5000\n",
      "Train Loss: 5.7056\n",
      "Epoch 864 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 865 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 866 of 5000\n",
      "Train Loss: 5.7169\n",
      "Epoch 867 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 868 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 869 of 5000\n",
      "Train Loss: 5.7104\n",
      "Epoch 870 of 5000\n",
      "Train Loss: 5.7172\n",
      "Epoch 871 of 5000\n",
      "Train Loss: 5.7107\n",
      "Epoch 872 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 873 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 874 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 875 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 876 of 5000\n",
      "Train Loss: 5.6875\n",
      "Epoch 877 of 5000\n",
      "Train Loss: 5.7194\n",
      "Epoch 878 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 879 of 5000\n",
      "Train Loss: 5.7120\n",
      "Epoch 880 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 881 of 5000\n",
      "Train Loss: 5.7137\n",
      "Epoch 882 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 883 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 884 of 5000\n",
      "Train Loss: 5.7062\n",
      "Epoch 885 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 886 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 887 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 888 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 889 of 5000\n",
      "Train Loss: 5.7173\n",
      "Epoch 890 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 891 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 892 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 893 of 5000\n",
      "Train Loss: 5.7104\n",
      "Epoch 894 of 5000\n",
      "Train Loss: 5.6834\n",
      "Epoch 895 of 5000\n",
      "Train Loss: 5.7140\n",
      "Epoch 896 of 5000\n",
      "Train Loss: 5.6786\n",
      "Epoch 897 of 5000\n",
      "Train Loss: 5.7176\n",
      "Epoch 898 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 899 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 900 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 901 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 902 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 903 of 5000\n",
      "Train Loss: 5.7197\n",
      "Epoch 904 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 905 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 906 of 5000\n",
      "Train Loss: 5.7146\n",
      "Epoch 907 of 5000\n",
      "Train Loss: 5.7176\n",
      "Epoch 908 of 5000\n",
      "Train Loss: 5.6818\n",
      "Epoch 909 of 5000\n",
      "Train Loss: 5.7209\n",
      "Epoch 910 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 911 of 5000\n",
      "Train Loss: 5.7128\n",
      "Epoch 912 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 913 of 5000\n",
      "Train Loss: 5.7153\n",
      "Epoch 914 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 915 of 5000\n",
      "Train Loss: 5.7076\n",
      "Epoch 916 of 5000\n",
      "Train Loss: 5.7212\n",
      "Epoch 917 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 918 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 919 of 5000\n",
      "Train Loss: 5.7068\n",
      "Epoch 920 of 5000\n",
      "Train Loss: 5.7156\n",
      "Epoch 921 of 5000\n",
      "Train Loss: 5.7096\n",
      "Epoch 922 of 5000\n",
      "Train Loss: 5.7176\n",
      "Epoch 923 of 5000\n",
      "Train Loss: 5.7189\n",
      "Epoch 924 of 5000\n",
      "Train Loss: 5.7150\n",
      "Epoch 925 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 926 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 927 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 928 of 5000\n",
      "Train Loss: 5.7149\n",
      "Epoch 929 of 5000\n",
      "Train Loss: 5.6777\n",
      "Epoch 930 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 931 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 932 of 5000\n",
      "Train Loss: 5.7082\n",
      "Epoch 933 of 5000\n",
      "Train Loss: 5.7131\n",
      "Epoch 934 of 5000\n",
      "Train Loss: 5.7068\n",
      "Epoch 935 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 936 of 5000\n",
      "Train Loss: 5.6948\n",
      "Epoch 937 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 938 of 5000\n",
      "Train Loss: 5.7220\n",
      "Epoch 939 of 5000\n",
      "Train Loss: 5.7270\n",
      "Epoch 940 of 5000\n",
      "Train Loss: 5.7240\n",
      "Epoch 941 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 942 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 943 of 5000\n",
      "Train Loss: 5.7123\n",
      "Epoch 944 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 945 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 946 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 947 of 5000\n",
      "Train Loss: 5.7094\n",
      "Epoch 948 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 949 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 950 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 951 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 952 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 953 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 954 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 955 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 956 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 957 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 958 of 5000\n",
      "Train Loss: 5.7122\n",
      "Epoch 959 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 960 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 961 of 5000\n",
      "Train Loss: 5.7122\n",
      "Epoch 962 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 963 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 964 of 5000\n",
      "Train Loss: 5.6767\n",
      "Epoch 965 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 966 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 967 of 5000\n",
      "Train Loss: 5.7095\n",
      "Epoch 968 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 969 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 970 of 5000\n",
      "Train Loss: 5.6998\n",
      "Epoch 971 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 972 of 5000\n",
      "Train Loss: 5.6854\n",
      "Epoch 973 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 974 of 5000\n",
      "Train Loss: 5.6958\n",
      "Epoch 975 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 976 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 977 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 978 of 5000\n",
      "Train Loss: 5.7180\n",
      "Epoch 979 of 5000\n",
      "Train Loss: 5.7236\n",
      "Epoch 980 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 981 of 5000\n",
      "Train Loss: 5.7013\n",
      "Epoch 982 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 983 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 984 of 5000\n",
      "Train Loss: 5.7157\n",
      "Epoch 985 of 5000\n",
      "Train Loss: 5.7102\n",
      "Epoch 986 of 5000\n",
      "Train Loss: 5.7219\n",
      "Epoch 987 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 988 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 989 of 5000\n",
      "Train Loss: 5.7190\n",
      "Epoch 990 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 991 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 992 of 5000\n",
      "Train Loss: 5.7117\n",
      "Epoch 993 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 994 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 995 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 996 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 997 of 5000\n",
      "Train Loss: 5.7188\n",
      "Epoch 998 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 999 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 1000 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 1001 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 1002 of 5000\n",
      "Train Loss: 5.7096\n",
      "Epoch 1003 of 5000\n",
      "Train Loss: 5.7179\n",
      "Epoch 1004 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 1005 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 1006 of 5000\n",
      "Train Loss: 5.7256\n",
      "Epoch 1007 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 1008 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 1009 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 1010 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 1011 of 5000\n",
      "Train Loss: 5.6786\n",
      "Epoch 1012 of 5000\n",
      "Train Loss: 5.7080\n",
      "Epoch 1013 of 5000\n",
      "Train Loss: 5.6854\n",
      "Epoch 1014 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 1015 of 5000\n",
      "Train Loss: 5.6812\n",
      "Epoch 1016 of 5000\n",
      "Train Loss: 5.7059\n",
      "Epoch 1017 of 5000\n",
      "Train Loss: 5.7080\n",
      "Epoch 1018 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 1019 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 1020 of 5000\n",
      "Train Loss: 5.6872\n",
      "Epoch 1021 of 5000\n",
      "Train Loss: 5.7297\n",
      "Epoch 1022 of 5000\n",
      "Train Loss: 5.7063\n",
      "Epoch 1023 of 5000\n",
      "Train Loss: 5.6851\n",
      "Epoch 1024 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 1025 of 5000\n",
      "Train Loss: 5.7269\n",
      "Epoch 1026 of 5000\n",
      "Train Loss: 5.6767\n",
      "Epoch 1027 of 5000\n",
      "Train Loss: 5.6829\n",
      "Epoch 1028 of 5000\n",
      "Train Loss: 5.7156\n",
      "Epoch 1029 of 5000\n",
      "Train Loss: 5.7136\n",
      "Epoch 1030 of 5000\n",
      "Train Loss: 5.6846\n",
      "Epoch 1031 of 5000\n",
      "Train Loss: 5.7133\n",
      "Epoch 1032 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 1033 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 1034 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 1035 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 1036 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 1037 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 1038 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 1039 of 5000\n",
      "Train Loss: 5.7062\n",
      "Epoch 1040 of 5000\n",
      "Train Loss: 5.7236\n",
      "Epoch 1041 of 5000\n",
      "Train Loss: 5.7166\n",
      "Epoch 1042 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 1043 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 1044 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 1045 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 1046 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 1047 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 1048 of 5000\n",
      "Train Loss: 5.7131\n",
      "Epoch 1049 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 1050 of 5000\n",
      "Train Loss: 5.7193\n",
      "Epoch 1051 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 1052 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 1053 of 5000\n",
      "Train Loss: 5.7105\n",
      "Epoch 1054 of 5000\n",
      "Train Loss: 5.7166\n",
      "Epoch 1055 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 1056 of 5000\n",
      "Train Loss: 5.7106\n",
      "Epoch 1057 of 5000\n",
      "Train Loss: 5.7115\n",
      "Epoch 1058 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 1059 of 5000\n",
      "Train Loss: 5.7203\n",
      "Epoch 1060 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 1061 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 1062 of 5000\n",
      "Train Loss: 5.7228\n",
      "Epoch 1063 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 1064 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 1065 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 1066 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 1067 of 5000\n",
      "Train Loss: 5.7182\n",
      "Epoch 1068 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 1069 of 5000\n",
      "Train Loss: 5.7245\n",
      "Epoch 1070 of 5000\n",
      "Train Loss: 5.7226\n",
      "Epoch 1071 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 1072 of 5000\n",
      "Train Loss: 5.7026\n",
      "Epoch 1073 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 1074 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 1075 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 1076 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 1077 of 5000\n",
      "Train Loss: 5.6998\n",
      "Epoch 1078 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 1079 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 1080 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 1081 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 1082 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 1083 of 5000\n",
      "Train Loss: 5.7095\n",
      "Epoch 1084 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 1085 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 1086 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 1087 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 1088 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 1089 of 5000\n",
      "Train Loss: 5.7062\n",
      "Epoch 1090 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 1091 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 1092 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 1093 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 1094 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 1095 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 1096 of 5000\n",
      "Train Loss: 5.7163\n",
      "Epoch 1097 of 5000\n",
      "Train Loss: 5.7115\n",
      "Epoch 1098 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 1099 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 1100 of 5000\n",
      "Train Loss: 5.7176\n",
      "Epoch 1101 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 1102 of 5000\n",
      "Train Loss: 5.7294\n",
      "Epoch 1103 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 1104 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 1105 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 1106 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 1107 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 1108 of 5000\n",
      "Train Loss: 5.6742\n",
      "Epoch 1109 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 1110 of 5000\n",
      "Train Loss: 5.7210\n",
      "Epoch 1111 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 1112 of 5000\n",
      "Train Loss: 5.7252\n",
      "Epoch 1113 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 1114 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 1115 of 5000\n",
      "Train Loss: 5.6836\n",
      "Epoch 1116 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 1117 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 1118 of 5000\n",
      "Train Loss: 5.7246\n",
      "Epoch 1119 of 5000\n",
      "Train Loss: 5.7197\n",
      "Epoch 1120 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 1121 of 5000\n",
      "Train Loss: 5.7184\n",
      "Epoch 1122 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 1123 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 1124 of 5000\n",
      "Train Loss: 5.6880\n",
      "Epoch 1125 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 1126 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 1127 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 1128 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 1129 of 5000\n",
      "Train Loss: 5.7174\n",
      "Epoch 1130 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 1131 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 1132 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 1133 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 1134 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 1135 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 1136 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 1137 of 5000\n",
      "Train Loss: 5.7109\n",
      "Epoch 1138 of 5000\n",
      "Train Loss: 5.6816\n",
      "Epoch 1139 of 5000\n",
      "Train Loss: 5.7202\n",
      "Epoch 1140 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 1141 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 1142 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 1143 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 1144 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 1145 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 1146 of 5000\n",
      "Train Loss: 5.7131\n",
      "Epoch 1147 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 1148 of 5000\n",
      "Train Loss: 5.7176\n",
      "Epoch 1149 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 1150 of 5000\n",
      "Train Loss: 5.7167\n",
      "Epoch 1151 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 1152 of 5000\n",
      "Train Loss: 5.7286\n",
      "Epoch 1153 of 5000\n",
      "Train Loss: 5.7161\n",
      "Epoch 1154 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 1155 of 5000\n",
      "Train Loss: 5.7073\n",
      "Epoch 1156 of 5000\n",
      "Train Loss: 5.7108\n",
      "Epoch 1157 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 1158 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 1159 of 5000\n",
      "Train Loss: 5.7128\n",
      "Epoch 1160 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 1161 of 5000\n",
      "Train Loss: 5.7174\n",
      "Epoch 1162 of 5000\n",
      "Train Loss: 5.6851\n",
      "Epoch 1163 of 5000\n",
      "Train Loss: 5.7178\n",
      "Epoch 1164 of 5000\n",
      "Train Loss: 5.6826\n",
      "Epoch 1165 of 5000\n",
      "Train Loss: 5.7144\n",
      "Epoch 1166 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 1167 of 5000\n",
      "Train Loss: 5.6868\n",
      "Epoch 1168 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 1169 of 5000\n",
      "Train Loss: 5.7073\n",
      "Epoch 1170 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 1171 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 1172 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 1173 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 1174 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 1175 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 1176 of 5000\n",
      "Train Loss: 5.7096\n",
      "Epoch 1177 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 1178 of 5000\n",
      "Train Loss: 5.7078\n",
      "Epoch 1179 of 5000\n",
      "Train Loss: 5.7099\n",
      "Epoch 1180 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 1181 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 1182 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 1183 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 1184 of 5000\n",
      "Train Loss: 5.7104\n",
      "Epoch 1185 of 5000\n",
      "Train Loss: 5.6783\n",
      "Epoch 1186 of 5000\n",
      "Train Loss: 5.6930\n",
      "Epoch 1187 of 5000\n",
      "Train Loss: 5.6961\n",
      "Epoch 1188 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 1189 of 5000\n",
      "Train Loss: 5.7039\n",
      "Epoch 1190 of 5000\n",
      "Train Loss: 5.7194\n",
      "Epoch 1191 of 5000\n",
      "Train Loss: 5.6987\n",
      "Epoch 1192 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 1193 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 1194 of 5000\n",
      "Train Loss: 5.7184\n",
      "Epoch 1195 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 1196 of 5000\n",
      "Train Loss: 5.6797\n",
      "Epoch 1197 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 1198 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 1199 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 1200 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 1201 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 1202 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 1203 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 1204 of 5000\n",
      "Train Loss: 5.6842\n",
      "Epoch 1205 of 5000\n",
      "Train Loss: 5.7239\n",
      "Epoch 1206 of 5000\n",
      "Train Loss: 5.7096\n",
      "Epoch 1207 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 1208 of 5000\n",
      "Train Loss: 5.6930\n",
      "Epoch 1209 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 1210 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 1211 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 1212 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 1213 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 1214 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 1215 of 5000\n",
      "Train Loss: 5.7092\n",
      "Epoch 1216 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 1217 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 1218 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 1219 of 5000\n",
      "Train Loss: 5.7114\n",
      "Epoch 1220 of 5000\n",
      "Train Loss: 5.6998\n",
      "Epoch 1221 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 1222 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 1223 of 5000\n",
      "Train Loss: 5.6829\n",
      "Epoch 1224 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 1225 of 5000\n",
      "Train Loss: 5.6716\n",
      "Epoch 1226 of 5000\n",
      "Train Loss: 5.7252\n",
      "Epoch 1227 of 5000\n",
      "Train Loss: 5.7164\n",
      "Epoch 1228 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 1229 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 1230 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 1231 of 5000\n",
      "Train Loss: 5.7209\n",
      "Epoch 1232 of 5000\n",
      "Train Loss: 5.7231\n",
      "Epoch 1233 of 5000\n",
      "Train Loss: 5.6769\n",
      "Epoch 1234 of 5000\n",
      "Train Loss: 5.7145\n",
      "Epoch 1235 of 5000\n",
      "Train Loss: 5.7111\n",
      "Epoch 1236 of 5000\n",
      "Train Loss: 5.7109\n",
      "Epoch 1237 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 1238 of 5000\n",
      "Train Loss: 5.7029\n",
      "Epoch 1239 of 5000\n",
      "Train Loss: 5.6825\n",
      "Epoch 1240 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 1241 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 1242 of 5000\n",
      "Train Loss: 5.6961\n",
      "Epoch 1243 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 1244 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 1245 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 1246 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 1247 of 5000\n",
      "Train Loss: 5.7230\n",
      "Epoch 1248 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 1249 of 5000\n",
      "Train Loss: 5.7094\n",
      "Epoch 1250 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 1251 of 5000\n",
      "Train Loss: 5.7211\n",
      "Epoch 1252 of 5000\n",
      "Train Loss: 5.6987\n",
      "Epoch 1253 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 1254 of 5000\n",
      "Train Loss: 5.7094\n",
      "Epoch 1255 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 1256 of 5000\n",
      "Train Loss: 5.7039\n",
      "Epoch 1257 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 1258 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 1259 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 1260 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 1261 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 1262 of 5000\n",
      "Train Loss: 5.7264\n",
      "Epoch 1263 of 5000\n",
      "Train Loss: 5.6958\n",
      "Epoch 1264 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 1265 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 1266 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 1267 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 1268 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 1269 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 1270 of 5000\n",
      "Train Loss: 5.7100\n",
      "Epoch 1271 of 5000\n",
      "Train Loss: 5.7009\n",
      "Epoch 1272 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 1273 of 5000\n",
      "Train Loss: 5.7171\n",
      "Epoch 1274 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 1275 of 5000\n",
      "Train Loss: 5.7164\n",
      "Epoch 1276 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 1277 of 5000\n",
      "Train Loss: 5.7161\n",
      "Epoch 1278 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 1279 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 1280 of 5000\n",
      "Train Loss: 5.7029\n",
      "Epoch 1281 of 5000\n",
      "Train Loss: 5.7169\n",
      "Epoch 1282 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 1283 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 1284 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 1285 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 1286 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 1287 of 5000\n",
      "Train Loss: 5.7059\n",
      "Epoch 1288 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 1289 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 1290 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 1291 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 1292 of 5000\n",
      "Train Loss: 5.7058\n",
      "Epoch 1293 of 5000\n",
      "Train Loss: 5.7194\n",
      "Epoch 1294 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 1295 of 5000\n",
      "Train Loss: 5.6798\n",
      "Epoch 1296 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 1297 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 1298 of 5000\n",
      "Train Loss: 5.6708\n",
      "Epoch 1299 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 1300 of 5000\n",
      "Train Loss: 5.6758\n",
      "Epoch 1301 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 1302 of 5000\n",
      "Train Loss: 5.7194\n",
      "Epoch 1303 of 5000\n",
      "Train Loss: 5.7179\n",
      "Epoch 1304 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 1305 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 1306 of 5000\n",
      "Train Loss: 5.7335\n",
      "Epoch 1307 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 1308 of 5000\n",
      "Train Loss: 5.7187\n",
      "Epoch 1309 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 1310 of 5000\n",
      "Train Loss: 5.6756\n",
      "Epoch 1311 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 1312 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 1313 of 5000\n",
      "Train Loss: 5.7196\n",
      "Epoch 1314 of 5000\n",
      "Train Loss: 5.7059\n",
      "Epoch 1315 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 1316 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 1317 of 5000\n",
      "Train Loss: 5.7063\n",
      "Epoch 1318 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 1319 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 1320 of 5000\n",
      "Train Loss: 5.6748\n",
      "Epoch 1321 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 1322 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 1323 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 1324 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 1325 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 1326 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 1327 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 1328 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 1329 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 1330 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 1331 of 5000\n",
      "Train Loss: 5.7191\n",
      "Epoch 1332 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 1333 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 1334 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 1335 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 1336 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 1337 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 1338 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 1339 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 1340 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 1341 of 5000\n",
      "Train Loss: 5.7097\n",
      "Epoch 1342 of 5000\n",
      "Train Loss: 5.7059\n",
      "Epoch 1343 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 1344 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 1345 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 1346 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 1347 of 5000\n",
      "Train Loss: 5.7214\n",
      "Epoch 1348 of 5000\n",
      "Train Loss: 5.7029\n",
      "Epoch 1349 of 5000\n",
      "Train Loss: 5.6853\n",
      "Epoch 1350 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 1351 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 1352 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 1353 of 5000\n",
      "Train Loss: 5.7098\n",
      "Epoch 1354 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 1355 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 1356 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 1357 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 1358 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 1359 of 5000\n",
      "Train Loss: 5.7201\n",
      "Epoch 1360 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 1361 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 1362 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 1363 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 1364 of 5000\n",
      "Train Loss: 5.6689\n",
      "Epoch 1365 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 1366 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 1367 of 5000\n",
      "Train Loss: 5.7110\n",
      "Epoch 1368 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 1369 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 1370 of 5000\n",
      "Train Loss: 5.7105\n",
      "Epoch 1371 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 1372 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 1373 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 1374 of 5000\n",
      "Train Loss: 5.7230\n",
      "Epoch 1375 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 1376 of 5000\n",
      "Train Loss: 5.7120\n",
      "Epoch 1377 of 5000\n",
      "Train Loss: 5.7104\n",
      "Epoch 1378 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 1379 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 1380 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 1381 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 1382 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 1383 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 1384 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 1385 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 1386 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 1387 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 1388 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 1389 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 1390 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 1391 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 1392 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 1393 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 1394 of 5000\n",
      "Train Loss: 5.7240\n",
      "Epoch 1395 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 1396 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 1397 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 1398 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 1399 of 5000\n",
      "Train Loss: 5.7141\n",
      "Epoch 1400 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 1401 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 1402 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 1403 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 1404 of 5000\n",
      "Train Loss: 5.7140\n",
      "Epoch 1405 of 5000\n",
      "Train Loss: 5.6612\n",
      "Epoch 1406 of 5000\n",
      "Train Loss: 5.7131\n",
      "Epoch 1407 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 1408 of 5000\n",
      "Train Loss: 5.7095\n",
      "Epoch 1409 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 1410 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 1411 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 1412 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 1413 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 1414 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 1415 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 1416 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 1417 of 5000\n",
      "Train Loss: 5.6846\n",
      "Epoch 1418 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 1419 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 1420 of 5000\n",
      "Train Loss: 5.7307\n",
      "Epoch 1421 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 1422 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 1423 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 1424 of 5000\n",
      "Train Loss: 5.7102\n",
      "Epoch 1425 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 1426 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 1427 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 1428 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 1429 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 1430 of 5000\n",
      "Train Loss: 5.7272\n",
      "Epoch 1431 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 1432 of 5000\n",
      "Train Loss: 5.7078\n",
      "Epoch 1433 of 5000\n",
      "Train Loss: 5.7188\n",
      "Epoch 1434 of 5000\n",
      "Train Loss: 5.6747\n",
      "Epoch 1435 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 1436 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 1437 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 1438 of 5000\n",
      "Train Loss: 5.7104\n",
      "Epoch 1439 of 5000\n",
      "Train Loss: 5.6837\n",
      "Epoch 1440 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 1441 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 1442 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 1443 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 1444 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 1445 of 5000\n",
      "Train Loss: 5.7076\n",
      "Epoch 1446 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 1447 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 1448 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 1449 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 1450 of 5000\n",
      "Train Loss: 5.6775\n",
      "Epoch 1451 of 5000\n",
      "Train Loss: 5.6783\n",
      "Epoch 1452 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 1453 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 1454 of 5000\n",
      "Train Loss: 5.6870\n",
      "Epoch 1455 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 1456 of 5000\n",
      "Train Loss: 5.7076\n",
      "Epoch 1457 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 1458 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 1459 of 5000\n",
      "Train Loss: 5.6771\n",
      "Epoch 1460 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 1461 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 1462 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 1463 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 1464 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 1465 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 1466 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 1467 of 5000\n",
      "Train Loss: 5.6948\n",
      "Epoch 1468 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 1469 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 1470 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 1471 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 1472 of 5000\n",
      "Train Loss: 5.7217\n",
      "Epoch 1473 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 1474 of 5000\n",
      "Train Loss: 5.7135\n",
      "Epoch 1475 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 1476 of 5000\n",
      "Train Loss: 5.7073\n",
      "Epoch 1477 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 1478 of 5000\n",
      "Train Loss: 5.6650\n",
      "Epoch 1479 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 1480 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 1481 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 1482 of 5000\n",
      "Train Loss: 5.7107\n",
      "Epoch 1483 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 1484 of 5000\n",
      "Train Loss: 5.7172\n",
      "Epoch 1485 of 5000\n",
      "Train Loss: 5.6793\n",
      "Epoch 1486 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 1487 of 5000\n",
      "Train Loss: 5.7251\n",
      "Epoch 1488 of 5000\n",
      "Train Loss: 5.6852\n",
      "Epoch 1489 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 1490 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 1491 of 5000\n",
      "Train Loss: 5.7105\n",
      "Epoch 1492 of 5000\n",
      "Train Loss: 5.6790\n",
      "Epoch 1493 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 1494 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 1495 of 5000\n",
      "Train Loss: 5.6832\n",
      "Epoch 1496 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 1497 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 1498 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 1499 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 1500 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 1501 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 1502 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 1503 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 1504 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 1505 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 1506 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 1507 of 5000\n",
      "Train Loss: 5.6809\n",
      "Epoch 1508 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 1509 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 1510 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 1511 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 1512 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 1513 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 1514 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 1515 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 1516 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 1517 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 1518 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 1519 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 1520 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 1521 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 1522 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 1523 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 1524 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 1525 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 1526 of 5000\n",
      "Train Loss: 5.7166\n",
      "Epoch 1527 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 1528 of 5000\n",
      "Train Loss: 5.6759\n",
      "Epoch 1529 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 1530 of 5000\n",
      "Train Loss: 5.6869\n",
      "Epoch 1531 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 1532 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 1533 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 1534 of 5000\n",
      "Train Loss: 5.7026\n",
      "Epoch 1535 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 1536 of 5000\n",
      "Train Loss: 5.6777\n",
      "Epoch 1537 of 5000\n",
      "Train Loss: 5.6828\n",
      "Epoch 1538 of 5000\n",
      "Train Loss: 5.7220\n",
      "Epoch 1539 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 1540 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 1541 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 1542 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 1543 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 1544 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 1545 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 1546 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 1547 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 1548 of 5000\n",
      "Train Loss: 5.6695\n",
      "Epoch 1549 of 5000\n",
      "Train Loss: 5.7013\n",
      "Epoch 1550 of 5000\n",
      "Train Loss: 5.7059\n",
      "Epoch 1551 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 1552 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 1553 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 1554 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 1555 of 5000\n",
      "Train Loss: 5.7110\n",
      "Epoch 1556 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 1557 of 5000\n",
      "Train Loss: 5.7169\n",
      "Epoch 1558 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 1559 of 5000\n",
      "Train Loss: 5.7094\n",
      "Epoch 1560 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 1561 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 1562 of 5000\n",
      "Train Loss: 5.7147\n",
      "Epoch 1563 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 1564 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 1565 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 1566 of 5000\n",
      "Train Loss: 5.7076\n",
      "Epoch 1567 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 1568 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 1569 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 1570 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 1571 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 1572 of 5000\n",
      "Train Loss: 5.7265\n",
      "Epoch 1573 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 1574 of 5000\n",
      "Train Loss: 5.6851\n",
      "Epoch 1575 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 1576 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 1577 of 5000\n",
      "Train Loss: 5.6725\n",
      "Epoch 1578 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 1579 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 1580 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 1581 of 5000\n",
      "Train Loss: 5.7070\n",
      "Epoch 1582 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 1583 of 5000\n",
      "Train Loss: 5.6852\n",
      "Epoch 1584 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 1585 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 1586 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 1587 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 1588 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 1589 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 1590 of 5000\n",
      "Train Loss: 5.7136\n",
      "Epoch 1591 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 1592 of 5000\n",
      "Train Loss: 5.7163\n",
      "Epoch 1593 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 1594 of 5000\n",
      "Train Loss: 5.6846\n",
      "Epoch 1595 of 5000\n",
      "Train Loss: 5.7192\n",
      "Epoch 1596 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 1597 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 1598 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 1599 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 1600 of 5000\n",
      "Train Loss: 5.6839\n",
      "Epoch 1601 of 5000\n",
      "Train Loss: 5.7029\n",
      "Epoch 1602 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 1603 of 5000\n",
      "Train Loss: 5.6989\n",
      "Epoch 1604 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 1605 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 1606 of 5000\n",
      "Train Loss: 5.7153\n",
      "Epoch 1607 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 1608 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 1609 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 1610 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 1611 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 1612 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 1613 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 1614 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 1615 of 5000\n",
      "Train Loss: 5.6775\n",
      "Epoch 1616 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 1617 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 1618 of 5000\n",
      "Train Loss: 5.7162\n",
      "Epoch 1619 of 5000\n",
      "Train Loss: 5.6835\n",
      "Epoch 1620 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 1621 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 1622 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 1623 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 1624 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 1625 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 1626 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 1627 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 1628 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 1629 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 1630 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 1631 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 1632 of 5000\n",
      "Train Loss: 5.6743\n",
      "Epoch 1633 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 1634 of 5000\n",
      "Train Loss: 5.6875\n",
      "Epoch 1635 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 1636 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 1637 of 5000\n",
      "Train Loss: 5.6870\n",
      "Epoch 1638 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 1639 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 1640 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 1641 of 5000\n",
      "Train Loss: 5.7097\n",
      "Epoch 1642 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 1643 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 1644 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 1645 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 1646 of 5000\n",
      "Train Loss: 5.6875\n",
      "Epoch 1647 of 5000\n",
      "Train Loss: 5.6715\n",
      "Epoch 1648 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 1649 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 1650 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 1651 of 5000\n",
      "Train Loss: 5.7123\n",
      "Epoch 1652 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 1653 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 1654 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 1655 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 1656 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 1657 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 1658 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 1659 of 5000\n",
      "Train Loss: 5.7092\n",
      "Epoch 1660 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 1661 of 5000\n",
      "Train Loss: 5.6851\n",
      "Epoch 1662 of 5000\n",
      "Train Loss: 5.7018\n",
      "Epoch 1663 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 1664 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 1665 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 1666 of 5000\n",
      "Train Loss: 5.7104\n",
      "Epoch 1667 of 5000\n",
      "Train Loss: 5.7128\n",
      "Epoch 1668 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 1669 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 1670 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 1671 of 5000\n",
      "Train Loss: 5.7114\n",
      "Epoch 1672 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 1673 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 1674 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 1675 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 1676 of 5000\n",
      "Train Loss: 5.6834\n",
      "Epoch 1677 of 5000\n",
      "Train Loss: 5.6804\n",
      "Epoch 1678 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 1679 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 1680 of 5000\n",
      "Train Loss: 5.6705\n",
      "Epoch 1681 of 5000\n",
      "Train Loss: 5.6812\n",
      "Epoch 1682 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 1683 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 1684 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 1685 of 5000\n",
      "Train Loss: 5.7106\n",
      "Epoch 1686 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 1687 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 1688 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 1689 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 1690 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 1691 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 1692 of 5000\n",
      "Train Loss: 5.7190\n",
      "Epoch 1693 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 1694 of 5000\n",
      "Train Loss: 5.6768\n",
      "Epoch 1695 of 5000\n",
      "Train Loss: 5.7100\n",
      "Epoch 1696 of 5000\n",
      "Train Loss: 5.7098\n",
      "Epoch 1697 of 5000\n",
      "Train Loss: 5.6680\n",
      "Epoch 1698 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 1699 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 1700 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 1701 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 1702 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 1703 of 5000\n",
      "Train Loss: 5.6740\n",
      "Epoch 1704 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 1705 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 1706 of 5000\n",
      "Train Loss: 5.6794\n",
      "Epoch 1707 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 1708 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 1709 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 1710 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 1711 of 5000\n",
      "Train Loss: 5.7147\n",
      "Epoch 1712 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 1713 of 5000\n",
      "Train Loss: 5.6766\n",
      "Epoch 1714 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 1715 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 1716 of 5000\n",
      "Train Loss: 5.7019\n",
      "Epoch 1717 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 1718 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 1719 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 1720 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 1721 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 1722 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 1723 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 1724 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 1725 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 1726 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 1727 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 1728 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 1729 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 1730 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 1731 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 1732 of 5000\n",
      "Train Loss: 5.6887\n",
      "Epoch 1733 of 5000\n",
      "Train Loss: 5.6742\n",
      "Epoch 1734 of 5000\n",
      "Train Loss: 5.7095\n",
      "Epoch 1735 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 1736 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 1737 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 1738 of 5000\n",
      "Train Loss: 5.6694\n",
      "Epoch 1739 of 5000\n",
      "Train Loss: 5.7013\n",
      "Epoch 1740 of 5000\n",
      "Train Loss: 5.6829\n",
      "Epoch 1741 of 5000\n",
      "Train Loss: 5.6871\n",
      "Epoch 1742 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 1743 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 1744 of 5000\n",
      "Train Loss: 5.7146\n",
      "Epoch 1745 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 1746 of 5000\n",
      "Train Loss: 5.7063\n",
      "Epoch 1747 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 1748 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 1749 of 5000\n",
      "Train Loss: 5.6836\n",
      "Epoch 1750 of 5000\n",
      "Train Loss: 5.7063\n",
      "Epoch 1751 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 1752 of 5000\n",
      "Train Loss: 5.6987\n",
      "Epoch 1753 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 1754 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 1755 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 1756 of 5000\n",
      "Train Loss: 5.6716\n",
      "Epoch 1757 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 1758 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 1759 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 1760 of 5000\n",
      "Train Loss: 5.6756\n",
      "Epoch 1761 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 1762 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 1763 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 1764 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 1765 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 1766 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 1767 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 1768 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 1769 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 1770 of 5000\n",
      "Train Loss: 5.7114\n",
      "Epoch 1771 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 1772 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 1773 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 1774 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 1775 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 1776 of 5000\n",
      "Train Loss: 5.7162\n",
      "Epoch 1777 of 5000\n",
      "Train Loss: 5.7146\n",
      "Epoch 1778 of 5000\n",
      "Train Loss: 5.6822\n",
      "Epoch 1779 of 5000\n",
      "Train Loss: 5.7226\n",
      "Epoch 1780 of 5000\n",
      "Train Loss: 5.7019\n",
      "Epoch 1781 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 1782 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 1783 of 5000\n",
      "Train Loss: 5.6989\n",
      "Epoch 1784 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 1785 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 1786 of 5000\n",
      "Train Loss: 5.7072\n",
      "Epoch 1787 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 1788 of 5000\n",
      "Train Loss: 5.6732\n",
      "Epoch 1789 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 1790 of 5000\n",
      "Train Loss: 5.6793\n",
      "Epoch 1791 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 1792 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 1793 of 5000\n",
      "Train Loss: 5.7039\n",
      "Epoch 1794 of 5000\n",
      "Train Loss: 5.7122\n",
      "Epoch 1795 of 5000\n",
      "Train Loss: 5.6862\n",
      "Epoch 1796 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 1797 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 1798 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 1799 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 1800 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 1801 of 5000\n",
      "Train Loss: 5.6685\n",
      "Epoch 1802 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 1803 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 1804 of 5000\n",
      "Train Loss: 5.7100\n",
      "Epoch 1805 of 5000\n",
      "Train Loss: 5.6789\n",
      "Epoch 1806 of 5000\n",
      "Train Loss: 5.6810\n",
      "Epoch 1807 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 1808 of 5000\n",
      "Train Loss: 5.7248\n",
      "Epoch 1809 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 1810 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 1811 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 1812 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 1813 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 1814 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 1815 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 1816 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 1817 of 5000\n",
      "Train Loss: 5.6640\n",
      "Epoch 1818 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 1819 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 1820 of 5000\n",
      "Train Loss: 5.6672\n",
      "Epoch 1821 of 5000\n",
      "Train Loss: 5.6839\n",
      "Epoch 1822 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 1823 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 1824 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 1825 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 1826 of 5000\n",
      "Train Loss: 5.7078\n",
      "Epoch 1827 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 1828 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 1829 of 5000\n",
      "Train Loss: 5.6799\n",
      "Epoch 1830 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 1831 of 5000\n",
      "Train Loss: 5.7094\n",
      "Epoch 1832 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 1833 of 5000\n",
      "Train Loss: 5.7090\n",
      "Epoch 1834 of 5000\n",
      "Train Loss: 5.6891\n",
      "Epoch 1835 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 1836 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 1837 of 5000\n",
      "Train Loss: 5.7213\n",
      "Epoch 1838 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 1839 of 5000\n",
      "Train Loss: 5.6656\n",
      "Epoch 1840 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 1841 of 5000\n",
      "Train Loss: 5.7155\n",
      "Epoch 1842 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 1843 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 1844 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 1845 of 5000\n",
      "Train Loss: 5.7013\n",
      "Epoch 1846 of 5000\n",
      "Train Loss: 5.6875\n",
      "Epoch 1847 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 1848 of 5000\n",
      "Train Loss: 5.6729\n",
      "Epoch 1849 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 1850 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 1851 of 5000\n",
      "Train Loss: 5.7305\n",
      "Epoch 1852 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 1853 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 1854 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 1855 of 5000\n",
      "Train Loss: 5.6798\n",
      "Epoch 1856 of 5000\n",
      "Train Loss: 5.6819\n",
      "Epoch 1857 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 1858 of 5000\n",
      "Train Loss: 5.7105\n",
      "Epoch 1859 of 5000\n",
      "Train Loss: 5.6901\n",
      "Epoch 1860 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 1861 of 5000\n",
      "Train Loss: 5.6805\n",
      "Epoch 1862 of 5000\n",
      "Train Loss: 5.6819\n",
      "Epoch 1863 of 5000\n",
      "Train Loss: 5.6791\n",
      "Epoch 1864 of 5000\n",
      "Train Loss: 5.6862\n",
      "Epoch 1865 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 1866 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 1867 of 5000\n",
      "Train Loss: 5.7209\n",
      "Epoch 1868 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 1869 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 1870 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 1871 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 1872 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 1873 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 1874 of 5000\n",
      "Train Loss: 5.6868\n",
      "Epoch 1875 of 5000\n",
      "Train Loss: 5.7163\n",
      "Epoch 1876 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 1877 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 1878 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 1879 of 5000\n",
      "Train Loss: 5.7122\n",
      "Epoch 1880 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 1881 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 1882 of 5000\n",
      "Train Loss: 5.7090\n",
      "Epoch 1883 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 1884 of 5000\n",
      "Train Loss: 5.6986\n",
      "Epoch 1885 of 5000\n",
      "Train Loss: 5.6872\n",
      "Epoch 1886 of 5000\n",
      "Train Loss: 5.6853\n",
      "Epoch 1887 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 1888 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 1889 of 5000\n",
      "Train Loss: 5.7184\n",
      "Epoch 1890 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 1891 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 1892 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 1893 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 1894 of 5000\n",
      "Train Loss: 5.6930\n",
      "Epoch 1895 of 5000\n",
      "Train Loss: 5.6791\n",
      "Epoch 1896 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 1897 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 1898 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 1899 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 1900 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 1901 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 1902 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 1903 of 5000\n",
      "Train Loss: 5.6829\n",
      "Epoch 1904 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 1905 of 5000\n",
      "Train Loss: 5.6930\n",
      "Epoch 1906 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 1907 of 5000\n",
      "Train Loss: 5.6819\n",
      "Epoch 1908 of 5000\n",
      "Train Loss: 5.6837\n",
      "Epoch 1909 of 5000\n",
      "Train Loss: 5.7185\n",
      "Epoch 1910 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 1911 of 5000\n",
      "Train Loss: 5.7232\n",
      "Epoch 1912 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 1913 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 1914 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 1915 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 1916 of 5000\n",
      "Train Loss: 5.7193\n",
      "Epoch 1917 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 1918 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 1919 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 1920 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 1921 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 1922 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 1923 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 1924 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 1925 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 1926 of 5000\n",
      "Train Loss: 5.6948\n",
      "Epoch 1927 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 1928 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 1929 of 5000\n",
      "Train Loss: 5.6753\n",
      "Epoch 1930 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 1931 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 1932 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 1933 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 1934 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 1935 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 1936 of 5000\n",
      "Train Loss: 5.7019\n",
      "Epoch 1937 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 1938 of 5000\n",
      "Train Loss: 5.6840\n",
      "Epoch 1939 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 1940 of 5000\n",
      "Train Loss: 5.6891\n",
      "Epoch 1941 of 5000\n",
      "Train Loss: 5.6807\n",
      "Epoch 1942 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 1943 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 1944 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 1945 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 1946 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 1947 of 5000\n",
      "Train Loss: 5.6758\n",
      "Epoch 1948 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 1949 of 5000\n",
      "Train Loss: 5.6998\n",
      "Epoch 1950 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 1951 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 1952 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 1953 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 1954 of 5000\n",
      "Train Loss: 5.6709\n",
      "Epoch 1955 of 5000\n",
      "Train Loss: 5.7054\n",
      "Epoch 1956 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 1957 of 5000\n",
      "Train Loss: 5.6731\n",
      "Epoch 1958 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 1959 of 5000\n",
      "Train Loss: 5.7182\n",
      "Epoch 1960 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 1961 of 5000\n",
      "Train Loss: 5.7125\n",
      "Epoch 1962 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 1963 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 1964 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 1965 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 1966 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 1967 of 5000\n",
      "Train Loss: 5.7140\n",
      "Epoch 1968 of 5000\n",
      "Train Loss: 5.7201\n",
      "Epoch 1969 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 1970 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 1971 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 1972 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 1973 of 5000\n",
      "Train Loss: 5.6762\n",
      "Epoch 1974 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 1975 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 1976 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 1977 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 1978 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 1979 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 1980 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 1981 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 1982 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 1983 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 1984 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 1985 of 5000\n",
      "Train Loss: 5.7089\n",
      "Epoch 1986 of 5000\n",
      "Train Loss: 5.6880\n",
      "Epoch 1987 of 5000\n",
      "Train Loss: 5.6731\n",
      "Epoch 1988 of 5000\n",
      "Train Loss: 5.7111\n",
      "Epoch 1989 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 1990 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 1991 of 5000\n",
      "Train Loss: 5.6880\n",
      "Epoch 1992 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 1993 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 1994 of 5000\n",
      "Train Loss: 5.6635\n",
      "Epoch 1995 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 1996 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 1997 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 1998 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 1999 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 2000 of 5000\n",
      "Train Loss: 5.7161\n",
      "Epoch 2001 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 2002 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 2003 of 5000\n",
      "Train Loss: 5.6840\n",
      "Epoch 2004 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 2005 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 2006 of 5000\n",
      "Train Loss: 5.6915\n",
      "Epoch 2007 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 2008 of 5000\n",
      "Train Loss: 5.7165\n",
      "Epoch 2009 of 5000\n",
      "Train Loss: 5.6666\n",
      "Epoch 2010 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 2011 of 5000\n",
      "Train Loss: 5.7072\n",
      "Epoch 2012 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 2013 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 2014 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 2015 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 2016 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 2017 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 2018 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 2019 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 2020 of 5000\n",
      "Train Loss: 5.7013\n",
      "Epoch 2021 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 2022 of 5000\n",
      "Train Loss: 5.7133\n",
      "Epoch 2023 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 2024 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 2025 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 2026 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 2027 of 5000\n",
      "Train Loss: 5.6930\n",
      "Epoch 2028 of 5000\n",
      "Train Loss: 5.7068\n",
      "Epoch 2029 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 2030 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 2031 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 2032 of 5000\n",
      "Train Loss: 5.7282\n",
      "Epoch 2033 of 5000\n",
      "Train Loss: 5.7097\n",
      "Epoch 2034 of 5000\n",
      "Train Loss: 5.6686\n",
      "Epoch 2035 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 2036 of 5000\n",
      "Train Loss: 5.6795\n",
      "Epoch 2037 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 2038 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 2039 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 2040 of 5000\n",
      "Train Loss: 5.7099\n",
      "Epoch 2041 of 5000\n",
      "Train Loss: 5.7029\n",
      "Epoch 2042 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 2043 of 5000\n",
      "Train Loss: 5.6891\n",
      "Epoch 2044 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 2045 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 2046 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 2047 of 5000\n",
      "Train Loss: 5.6987\n",
      "Epoch 2048 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 2049 of 5000\n",
      "Train Loss: 5.7134\n",
      "Epoch 2050 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 2051 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 2052 of 5000\n",
      "Train Loss: 5.7152\n",
      "Epoch 2053 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 2054 of 5000\n",
      "Train Loss: 5.6785\n",
      "Epoch 2055 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 2056 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 2057 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 2058 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 2059 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 2060 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 2061 of 5000\n",
      "Train Loss: 5.7039\n",
      "Epoch 2062 of 5000\n",
      "Train Loss: 5.7068\n",
      "Epoch 2063 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 2064 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 2065 of 5000\n",
      "Train Loss: 5.6707\n",
      "Epoch 2066 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 2067 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 2068 of 5000\n",
      "Train Loss: 5.6812\n",
      "Epoch 2069 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 2070 of 5000\n",
      "Train Loss: 5.7143\n",
      "Epoch 2071 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 2072 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 2073 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 2074 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 2075 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 2076 of 5000\n",
      "Train Loss: 5.6747\n",
      "Epoch 2077 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 2078 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 2079 of 5000\n",
      "Train Loss: 5.6797\n",
      "Epoch 2080 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 2081 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 2082 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 2083 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 2084 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 2085 of 5000\n",
      "Train Loss: 5.7100\n",
      "Epoch 2086 of 5000\n",
      "Train Loss: 5.6795\n",
      "Epoch 2087 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 2088 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 2089 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 2090 of 5000\n",
      "Train Loss: 5.7237\n",
      "Epoch 2091 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 2092 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 2093 of 5000\n",
      "Train Loss: 5.7168\n",
      "Epoch 2094 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 2095 of 5000\n",
      "Train Loss: 5.6961\n",
      "Epoch 2096 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 2097 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 2098 of 5000\n",
      "Train Loss: 5.7058\n",
      "Epoch 2099 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 2100 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 2101 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 2102 of 5000\n",
      "Train Loss: 5.7139\n",
      "Epoch 2103 of 5000\n",
      "Train Loss: 5.6790\n",
      "Epoch 2104 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 2105 of 5000\n",
      "Train Loss: 5.6745\n",
      "Epoch 2106 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 2107 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 2108 of 5000\n",
      "Train Loss: 5.6754\n",
      "Epoch 2109 of 5000\n",
      "Train Loss: 5.6549\n",
      "Epoch 2110 of 5000\n",
      "Train Loss: 5.6834\n",
      "Epoch 2111 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 2112 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 2113 of 5000\n",
      "Train Loss: 5.6791\n",
      "Epoch 2114 of 5000\n",
      "Train Loss: 5.6986\n",
      "Epoch 2115 of 5000\n",
      "Train Loss: 5.6745\n",
      "Epoch 2116 of 5000\n",
      "Train Loss: 5.6807\n",
      "Epoch 2117 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 2118 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 2119 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 2120 of 5000\n",
      "Train Loss: 5.6808\n",
      "Epoch 2121 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 2122 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 2123 of 5000\n",
      "Train Loss: 5.6783\n",
      "Epoch 2124 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 2125 of 5000\n",
      "Train Loss: 5.7153\n",
      "Epoch 2126 of 5000\n",
      "Train Loss: 5.6742\n",
      "Epoch 2127 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 2128 of 5000\n",
      "Train Loss: 5.6816\n",
      "Epoch 2129 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 2130 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 2131 of 5000\n",
      "Train Loss: 5.7211\n",
      "Epoch 2132 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 2133 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 2134 of 5000\n",
      "Train Loss: 5.7092\n",
      "Epoch 2135 of 5000\n",
      "Train Loss: 5.6869\n",
      "Epoch 2136 of 5000\n",
      "Train Loss: 5.6915\n",
      "Epoch 2137 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 2138 of 5000\n",
      "Train Loss: 5.6733\n",
      "Epoch 2139 of 5000\n",
      "Train Loss: 5.6740\n",
      "Epoch 2140 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 2141 of 5000\n",
      "Train Loss: 5.6940\n",
      "Epoch 2142 of 5000\n",
      "Train Loss: 5.6761\n",
      "Epoch 2143 of 5000\n",
      "Train Loss: 5.6684\n",
      "Epoch 2144 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 2145 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 2146 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 2147 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 2148 of 5000\n",
      "Train Loss: 5.6901\n",
      "Epoch 2149 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 2150 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 2151 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 2152 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 2153 of 5000\n",
      "Train Loss: 5.7013\n",
      "Epoch 2154 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 2155 of 5000\n",
      "Train Loss: 5.6887\n",
      "Epoch 2156 of 5000\n",
      "Train Loss: 5.6820\n",
      "Epoch 2157 of 5000\n",
      "Train Loss: 5.6777\n",
      "Epoch 2158 of 5000\n",
      "Train Loss: 5.7164\n",
      "Epoch 2159 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 2160 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 2161 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 2162 of 5000\n",
      "Train Loss: 5.7013\n",
      "Epoch 2163 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 2164 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 2165 of 5000\n",
      "Train Loss: 5.6819\n",
      "Epoch 2166 of 5000\n",
      "Train Loss: 5.6753\n",
      "Epoch 2167 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 2168 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 2169 of 5000\n",
      "Train Loss: 5.7143\n",
      "Epoch 2170 of 5000\n",
      "Train Loss: 5.7041\n",
      "Epoch 2171 of 5000\n",
      "Train Loss: 5.6763\n",
      "Epoch 2172 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 2173 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 2174 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 2175 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 2176 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 2177 of 5000\n",
      "Train Loss: 5.7029\n",
      "Epoch 2178 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 2179 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 2180 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 2181 of 5000\n",
      "Train Loss: 5.6875\n",
      "Epoch 2182 of 5000\n",
      "Train Loss: 5.6986\n",
      "Epoch 2183 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 2184 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 2185 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 2186 of 5000\n",
      "Train Loss: 5.6776\n",
      "Epoch 2187 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 2188 of 5000\n",
      "Train Loss: 5.7054\n",
      "Epoch 2189 of 5000\n",
      "Train Loss: 5.6760\n",
      "Epoch 2190 of 5000\n",
      "Train Loss: 5.6826\n",
      "Epoch 2191 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 2192 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 2193 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 2194 of 5000\n",
      "Train Loss: 5.7072\n",
      "Epoch 2195 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 2196 of 5000\n",
      "Train Loss: 5.7078\n",
      "Epoch 2197 of 5000\n",
      "Train Loss: 5.7258\n",
      "Epoch 2198 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 2199 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 2200 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 2201 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 2202 of 5000\n",
      "Train Loss: 5.6844\n",
      "Epoch 2203 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 2204 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 2205 of 5000\n",
      "Train Loss: 5.6830\n",
      "Epoch 2206 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 2207 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 2208 of 5000\n",
      "Train Loss: 5.6811\n",
      "Epoch 2209 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 2210 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 2211 of 5000\n",
      "Train Loss: 5.6852\n",
      "Epoch 2212 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 2213 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 2214 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 2215 of 5000\n",
      "Train Loss: 5.7137\n",
      "Epoch 2216 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 2217 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 2218 of 5000\n",
      "Train Loss: 5.6843\n",
      "Epoch 2219 of 5000\n",
      "Train Loss: 5.7102\n",
      "Epoch 2220 of 5000\n",
      "Train Loss: 5.6789\n",
      "Epoch 2221 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 2222 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 2223 of 5000\n",
      "Train Loss: 5.7056\n",
      "Epoch 2224 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 2225 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 2226 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 2227 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 2228 of 5000\n",
      "Train Loss: 5.7125\n",
      "Epoch 2229 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 2230 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 2231 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 2232 of 5000\n",
      "Train Loss: 5.7056\n",
      "Epoch 2233 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 2234 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 2235 of 5000\n",
      "Train Loss: 5.7078\n",
      "Epoch 2236 of 5000\n",
      "Train Loss: 5.6763\n",
      "Epoch 2237 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 2238 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 2239 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 2240 of 5000\n",
      "Train Loss: 5.6797\n",
      "Epoch 2241 of 5000\n",
      "Train Loss: 5.6940\n",
      "Epoch 2242 of 5000\n",
      "Train Loss: 5.7056\n",
      "Epoch 2243 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 2244 of 5000\n",
      "Train Loss: 5.6797\n",
      "Epoch 2245 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 2246 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 2247 of 5000\n",
      "Train Loss: 5.6788\n",
      "Epoch 2248 of 5000\n",
      "Train Loss: 5.6657\n",
      "Epoch 2249 of 5000\n",
      "Train Loss: 5.6811\n",
      "Epoch 2250 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 2251 of 5000\n",
      "Train Loss: 5.6846\n",
      "Epoch 2252 of 5000\n",
      "Train Loss: 5.6863\n",
      "Epoch 2253 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 2254 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 2255 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 2256 of 5000\n",
      "Train Loss: 5.7092\n",
      "Epoch 2257 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 2258 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 2259 of 5000\n",
      "Train Loss: 5.6717\n",
      "Epoch 2260 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 2261 of 5000\n",
      "Train Loss: 5.6839\n",
      "Epoch 2262 of 5000\n",
      "Train Loss: 5.6818\n",
      "Epoch 2263 of 5000\n",
      "Train Loss: 5.6958\n",
      "Epoch 2264 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 2265 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 2266 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 2267 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 2268 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 2269 of 5000\n",
      "Train Loss: 5.6818\n",
      "Epoch 2270 of 5000\n",
      "Train Loss: 5.7099\n",
      "Epoch 2271 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 2272 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 2273 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 2274 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 2275 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 2276 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 2277 of 5000\n",
      "Train Loss: 5.6801\n",
      "Epoch 2278 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 2279 of 5000\n",
      "Train Loss: 5.7198\n",
      "Epoch 2280 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 2281 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 2282 of 5000\n",
      "Train Loss: 5.6891\n",
      "Epoch 2283 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 2284 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 2285 of 5000\n",
      "Train Loss: 5.6783\n",
      "Epoch 2286 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 2287 of 5000\n",
      "Train Loss: 5.6892\n",
      "Epoch 2288 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 2289 of 5000\n",
      "Train Loss: 5.7138\n",
      "Epoch 2290 of 5000\n",
      "Train Loss: 5.6930\n",
      "Epoch 2291 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 2292 of 5000\n",
      "Train Loss: 5.6852\n",
      "Epoch 2293 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 2294 of 5000\n",
      "Train Loss: 5.7062\n",
      "Epoch 2295 of 5000\n",
      "Train Loss: 5.6843\n",
      "Epoch 2296 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 2297 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 2298 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 2299 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 2300 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 2301 of 5000\n",
      "Train Loss: 5.6748\n",
      "Epoch 2302 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 2303 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 2304 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 2305 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 2306 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 2307 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 2308 of 5000\n",
      "Train Loss: 5.6788\n",
      "Epoch 2309 of 5000\n",
      "Train Loss: 5.6840\n",
      "Epoch 2310 of 5000\n",
      "Train Loss: 5.6836\n",
      "Epoch 2311 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 2312 of 5000\n",
      "Train Loss: 5.6675\n",
      "Epoch 2313 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 2314 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 2315 of 5000\n",
      "Train Loss: 5.6808\n",
      "Epoch 2316 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 2317 of 5000\n",
      "Train Loss: 5.6774\n",
      "Epoch 2318 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 2319 of 5000\n",
      "Train Loss: 5.7166\n",
      "Epoch 2320 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 2321 of 5000\n",
      "Train Loss: 5.7169\n",
      "Epoch 2322 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 2323 of 5000\n",
      "Train Loss: 5.7196\n",
      "Epoch 2324 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 2325 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 2326 of 5000\n",
      "Train Loss: 5.6776\n",
      "Epoch 2327 of 5000\n",
      "Train Loss: 5.7164\n",
      "Epoch 2328 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 2329 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 2330 of 5000\n",
      "Train Loss: 5.6655\n",
      "Epoch 2331 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 2332 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 2333 of 5000\n",
      "Train Loss: 5.7191\n",
      "Epoch 2334 of 5000\n",
      "Train Loss: 5.6887\n",
      "Epoch 2335 of 5000\n",
      "Train Loss: 5.6794\n",
      "Epoch 2336 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 2337 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 2338 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 2339 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 2340 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 2341 of 5000\n",
      "Train Loss: 5.6825\n",
      "Epoch 2342 of 5000\n",
      "Train Loss: 5.6784\n",
      "Epoch 2343 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 2344 of 5000\n",
      "Train Loss: 5.7079\n",
      "Epoch 2345 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 2346 of 5000\n",
      "Train Loss: 5.6757\n",
      "Epoch 2347 of 5000\n",
      "Train Loss: 5.6755\n",
      "Epoch 2348 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 2349 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 2350 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 2351 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 2352 of 5000\n",
      "Train Loss: 5.7170\n",
      "Epoch 2353 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 2354 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 2355 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 2356 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 2357 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 2358 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 2359 of 5000\n",
      "Train Loss: 5.6940\n",
      "Epoch 2360 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 2361 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 2362 of 5000\n",
      "Train Loss: 5.6693\n",
      "Epoch 2363 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 2364 of 5000\n",
      "Train Loss: 5.6645\n",
      "Epoch 2365 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 2366 of 5000\n",
      "Train Loss: 5.6865\n",
      "Epoch 2367 of 5000\n",
      "Train Loss: 5.6961\n",
      "Epoch 2368 of 5000\n",
      "Train Loss: 5.6875\n",
      "Epoch 2369 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 2370 of 5000\n",
      "Train Loss: 5.6721\n",
      "Epoch 2371 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 2372 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 2373 of 5000\n",
      "Train Loss: 5.6801\n",
      "Epoch 2374 of 5000\n",
      "Train Loss: 5.6805\n",
      "Epoch 2375 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 2376 of 5000\n",
      "Train Loss: 5.7098\n",
      "Epoch 2377 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 2378 of 5000\n",
      "Train Loss: 5.6825\n",
      "Epoch 2379 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 2380 of 5000\n",
      "Train Loss: 5.7122\n",
      "Epoch 2381 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 2382 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 2383 of 5000\n",
      "Train Loss: 5.6557\n",
      "Epoch 2384 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 2385 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 2386 of 5000\n",
      "Train Loss: 5.6832\n",
      "Epoch 2387 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 2388 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 2389 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 2390 of 5000\n",
      "Train Loss: 5.6820\n",
      "Epoch 2391 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 2392 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 2393 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 2394 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 2395 of 5000\n",
      "Train Loss: 5.7150\n",
      "Epoch 2396 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 2397 of 5000\n",
      "Train Loss: 5.7136\n",
      "Epoch 2398 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 2399 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 2400 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 2401 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 2402 of 5000\n",
      "Train Loss: 5.6798\n",
      "Epoch 2403 of 5000\n",
      "Train Loss: 5.6746\n",
      "Epoch 2404 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 2405 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 2406 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 2407 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 2408 of 5000\n",
      "Train Loss: 5.7154\n",
      "Epoch 2409 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 2410 of 5000\n",
      "Train Loss: 5.6652\n",
      "Epoch 2411 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 2412 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 2413 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 2414 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 2415 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 2416 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 2417 of 5000\n",
      "Train Loss: 5.6803\n",
      "Epoch 2418 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 2419 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 2420 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 2421 of 5000\n",
      "Train Loss: 5.6670\n",
      "Epoch 2422 of 5000\n",
      "Train Loss: 5.6799\n",
      "Epoch 2423 of 5000\n",
      "Train Loss: 5.6748\n",
      "Epoch 2424 of 5000\n",
      "Train Loss: 5.7009\n",
      "Epoch 2425 of 5000\n",
      "Train Loss: 5.6820\n",
      "Epoch 2426 of 5000\n",
      "Train Loss: 5.6863\n",
      "Epoch 2427 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 2428 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 2429 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 2430 of 5000\n",
      "Train Loss: 5.6940\n",
      "Epoch 2431 of 5000\n",
      "Train Loss: 5.7156\n",
      "Epoch 2432 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 2433 of 5000\n",
      "Train Loss: 5.6703\n",
      "Epoch 2434 of 5000\n",
      "Train Loss: 5.7089\n",
      "Epoch 2435 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 2436 of 5000\n",
      "Train Loss: 5.7136\n",
      "Epoch 2437 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 2438 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 2439 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 2440 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 2441 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 2442 of 5000\n",
      "Train Loss: 5.6765\n",
      "Epoch 2443 of 5000\n",
      "Train Loss: 5.6854\n",
      "Epoch 2444 of 5000\n",
      "Train Loss: 5.7152\n",
      "Epoch 2445 of 5000\n",
      "Train Loss: 5.7206\n",
      "Epoch 2446 of 5000\n",
      "Train Loss: 5.6533\n",
      "Epoch 2447 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 2448 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 2449 of 5000\n",
      "Train Loss: 5.6819\n",
      "Epoch 2450 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 2451 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 2452 of 5000\n",
      "Train Loss: 5.6659\n",
      "Epoch 2453 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 2454 of 5000\n",
      "Train Loss: 5.6823\n",
      "Epoch 2455 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 2456 of 5000\n",
      "Train Loss: 5.6717\n",
      "Epoch 2457 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 2458 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 2459 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 2460 of 5000\n",
      "Train Loss: 5.6853\n",
      "Epoch 2461 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 2462 of 5000\n",
      "Train Loss: 5.6654\n",
      "Epoch 2463 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 2464 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 2465 of 5000\n",
      "Train Loss: 5.6940\n",
      "Epoch 2466 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 2467 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 2468 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 2469 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 2470 of 5000\n",
      "Train Loss: 5.7117\n",
      "Epoch 2471 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 2472 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 2473 of 5000\n",
      "Train Loss: 5.7097\n",
      "Epoch 2474 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 2475 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 2476 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 2477 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 2478 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 2479 of 5000\n",
      "Train Loss: 5.7205\n",
      "Epoch 2480 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 2481 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 2482 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 2483 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 2484 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 2485 of 5000\n",
      "Train Loss: 5.6830\n",
      "Epoch 2486 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 2487 of 5000\n",
      "Train Loss: 5.6753\n",
      "Epoch 2488 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 2489 of 5000\n",
      "Train Loss: 5.6880\n",
      "Epoch 2490 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 2491 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 2492 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 2493 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 2494 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 2495 of 5000\n",
      "Train Loss: 5.6723\n",
      "Epoch 2496 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 2497 of 5000\n",
      "Train Loss: 5.7141\n",
      "Epoch 2498 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 2499 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 2500 of 5000\n",
      "Train Loss: 5.6862\n",
      "Epoch 2501 of 5000\n",
      "Train Loss: 5.7009\n",
      "Epoch 2502 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 2503 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 2504 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 2505 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 2506 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 2507 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 2508 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 2509 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 2510 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 2511 of 5000\n",
      "Train Loss: 5.7140\n",
      "Epoch 2512 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 2513 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 2514 of 5000\n",
      "Train Loss: 5.7108\n",
      "Epoch 2515 of 5000\n",
      "Train Loss: 5.6718\n",
      "Epoch 2516 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 2517 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 2518 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 2519 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 2520 of 5000\n",
      "Train Loss: 5.7077\n",
      "Epoch 2521 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 2522 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 2523 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 2524 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 2525 of 5000\n",
      "Train Loss: 5.6822\n",
      "Epoch 2526 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 2527 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 2528 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 2529 of 5000\n",
      "Train Loss: 5.6872\n",
      "Epoch 2530 of 5000\n",
      "Train Loss: 5.7026\n",
      "Epoch 2531 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 2532 of 5000\n",
      "Train Loss: 5.6875\n",
      "Epoch 2533 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 2534 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 2535 of 5000\n",
      "Train Loss: 5.6739\n",
      "Epoch 2536 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 2537 of 5000\n",
      "Train Loss: 5.6632\n",
      "Epoch 2538 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 2539 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 2540 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 2541 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 2542 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 2543 of 5000\n",
      "Train Loss: 5.6768\n",
      "Epoch 2544 of 5000\n",
      "Train Loss: 5.6695\n",
      "Epoch 2545 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 2546 of 5000\n",
      "Train Loss: 5.6777\n",
      "Epoch 2547 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 2548 of 5000\n",
      "Train Loss: 5.7174\n",
      "Epoch 2549 of 5000\n",
      "Train Loss: 5.7134\n",
      "Epoch 2550 of 5000\n",
      "Train Loss: 5.6698\n",
      "Epoch 2551 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 2552 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 2553 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 2554 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 2555 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 2556 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 2557 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 2558 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 2559 of 5000\n",
      "Train Loss: 5.7070\n",
      "Epoch 2560 of 5000\n",
      "Train Loss: 5.6761\n",
      "Epoch 2561 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 2562 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 2563 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 2564 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 2565 of 5000\n",
      "Train Loss: 5.6766\n",
      "Epoch 2566 of 5000\n",
      "Train Loss: 5.6794\n",
      "Epoch 2567 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 2568 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 2569 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 2570 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 2571 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 2572 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 2573 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 2574 of 5000\n",
      "Train Loss: 5.7219\n",
      "Epoch 2575 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 2576 of 5000\n",
      "Train Loss: 5.6834\n",
      "Epoch 2577 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 2578 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 2579 of 5000\n",
      "Train Loss: 5.6961\n",
      "Epoch 2580 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 2581 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 2582 of 5000\n",
      "Train Loss: 5.6733\n",
      "Epoch 2583 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 2584 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 2585 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 2586 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 2587 of 5000\n",
      "Train Loss: 5.7122\n",
      "Epoch 2588 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 2589 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 2590 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 2591 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 2592 of 5000\n",
      "Train Loss: 5.6813\n",
      "Epoch 2593 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 2594 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 2595 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 2596 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 2597 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 2598 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 2599 of 5000\n",
      "Train Loss: 5.6763\n",
      "Epoch 2600 of 5000\n",
      "Train Loss: 5.7114\n",
      "Epoch 2601 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 2602 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 2603 of 5000\n",
      "Train Loss: 5.6790\n",
      "Epoch 2604 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 2605 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 2606 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 2607 of 5000\n",
      "Train Loss: 5.7123\n",
      "Epoch 2608 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 2609 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 2610 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 2611 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 2612 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 2613 of 5000\n",
      "Train Loss: 5.7193\n",
      "Epoch 2614 of 5000\n",
      "Train Loss: 5.7125\n",
      "Epoch 2615 of 5000\n",
      "Train Loss: 5.6818\n",
      "Epoch 2616 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 2617 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 2618 of 5000\n",
      "Train Loss: 5.7247\n",
      "Epoch 2619 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 2620 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 2621 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 2622 of 5000\n",
      "Train Loss: 5.7107\n",
      "Epoch 2623 of 5000\n",
      "Train Loss: 5.6837\n",
      "Epoch 2624 of 5000\n",
      "Train Loss: 5.6961\n",
      "Epoch 2625 of 5000\n",
      "Train Loss: 5.7073\n",
      "Epoch 2626 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 2627 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 2628 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 2629 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 2630 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 2631 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 2632 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 2633 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 2634 of 5000\n",
      "Train Loss: 5.6812\n",
      "Epoch 2635 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 2636 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 2637 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 2638 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 2639 of 5000\n",
      "Train Loss: 5.7100\n",
      "Epoch 2640 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 2641 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 2642 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 2643 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 2644 of 5000\n",
      "Train Loss: 5.6679\n",
      "Epoch 2645 of 5000\n",
      "Train Loss: 5.6875\n",
      "Epoch 2646 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 2647 of 5000\n",
      "Train Loss: 5.7120\n",
      "Epoch 2648 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 2649 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 2650 of 5000\n",
      "Train Loss: 5.6781\n",
      "Epoch 2651 of 5000\n",
      "Train Loss: 5.7073\n",
      "Epoch 2652 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 2653 of 5000\n",
      "Train Loss: 5.7135\n",
      "Epoch 2654 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 2655 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 2656 of 5000\n",
      "Train Loss: 5.6814\n",
      "Epoch 2657 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 2658 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 2659 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 2660 of 5000\n",
      "Train Loss: 5.7266\n",
      "Epoch 2661 of 5000\n",
      "Train Loss: 5.6807\n",
      "Epoch 2662 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 2663 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 2664 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 2665 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 2666 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 2667 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 2668 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 2669 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 2670 of 5000\n",
      "Train Loss: 5.6734\n",
      "Epoch 2671 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 2672 of 5000\n",
      "Train Loss: 5.6836\n",
      "Epoch 2673 of 5000\n",
      "Train Loss: 5.6765\n",
      "Epoch 2674 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 2675 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 2676 of 5000\n",
      "Train Loss: 5.6769\n",
      "Epoch 2677 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 2678 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 2679 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 2680 of 5000\n",
      "Train Loss: 5.6729\n",
      "Epoch 2681 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 2682 of 5000\n",
      "Train Loss: 5.7041\n",
      "Epoch 2683 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 2684 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 2685 of 5000\n",
      "Train Loss: 5.7179\n",
      "Epoch 2686 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 2687 of 5000\n",
      "Train Loss: 5.7217\n",
      "Epoch 2688 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 2689 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 2690 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 2691 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 2692 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 2693 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 2694 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 2695 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 2696 of 5000\n",
      "Train Loss: 5.6887\n",
      "Epoch 2697 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 2698 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 2699 of 5000\n",
      "Train Loss: 5.6811\n",
      "Epoch 2700 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 2701 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 2702 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 2703 of 5000\n",
      "Train Loss: 5.6863\n",
      "Epoch 2704 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 2705 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 2706 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 2707 of 5000\n",
      "Train Loss: 5.6842\n",
      "Epoch 2708 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 2709 of 5000\n",
      "Train Loss: 5.7216\n",
      "Epoch 2710 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 2711 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 2712 of 5000\n",
      "Train Loss: 5.6834\n",
      "Epoch 2713 of 5000\n",
      "Train Loss: 5.6668\n",
      "Epoch 2714 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 2715 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 2716 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 2717 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 2718 of 5000\n",
      "Train Loss: 5.6986\n",
      "Epoch 2719 of 5000\n",
      "Train Loss: 5.7068\n",
      "Epoch 2720 of 5000\n",
      "Train Loss: 5.6892\n",
      "Epoch 2721 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 2722 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 2723 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 2724 of 5000\n",
      "Train Loss: 5.6791\n",
      "Epoch 2725 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 2726 of 5000\n",
      "Train Loss: 5.6794\n",
      "Epoch 2727 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 2728 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 2729 of 5000\n",
      "Train Loss: 5.6704\n",
      "Epoch 2730 of 5000\n",
      "Train Loss: 5.6865\n",
      "Epoch 2731 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 2732 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 2733 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 2734 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 2735 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 2736 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 2737 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 2738 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 2739 of 5000\n",
      "Train Loss: 5.6854\n",
      "Epoch 2740 of 5000\n",
      "Train Loss: 5.6820\n",
      "Epoch 2741 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 2742 of 5000\n",
      "Train Loss: 5.6828\n",
      "Epoch 2743 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 2744 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 2745 of 5000\n",
      "Train Loss: 5.6853\n",
      "Epoch 2746 of 5000\n",
      "Train Loss: 5.7026\n",
      "Epoch 2747 of 5000\n",
      "Train Loss: 5.7077\n",
      "Epoch 2748 of 5000\n",
      "Train Loss: 5.6756\n",
      "Epoch 2749 of 5000\n",
      "Train Loss: 5.6753\n",
      "Epoch 2750 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 2751 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 2752 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 2753 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 2754 of 5000\n",
      "Train Loss: 5.7134\n",
      "Epoch 2755 of 5000\n",
      "Train Loss: 5.6753\n",
      "Epoch 2756 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 2757 of 5000\n",
      "Train Loss: 5.6810\n",
      "Epoch 2758 of 5000\n",
      "Train Loss: 5.7098\n",
      "Epoch 2759 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 2760 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 2761 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 2762 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 2763 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 2764 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 2765 of 5000\n",
      "Train Loss: 5.7110\n",
      "Epoch 2766 of 5000\n",
      "Train Loss: 5.6595\n",
      "Epoch 2767 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 2768 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 2769 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 2770 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 2771 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 2772 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 2773 of 5000\n",
      "Train Loss: 5.6707\n",
      "Epoch 2774 of 5000\n",
      "Train Loss: 5.7090\n",
      "Epoch 2775 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 2776 of 5000\n",
      "Train Loss: 5.6756\n",
      "Epoch 2777 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 2778 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 2779 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 2780 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 2781 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 2782 of 5000\n",
      "Train Loss: 5.6986\n",
      "Epoch 2783 of 5000\n",
      "Train Loss: 5.6805\n",
      "Epoch 2784 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 2785 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 2786 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 2787 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 2788 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 2789 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 2790 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 2791 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 2792 of 5000\n",
      "Train Loss: 5.7204\n",
      "Epoch 2793 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 2794 of 5000\n",
      "Train Loss: 5.6811\n",
      "Epoch 2795 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 2796 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 2797 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 2798 of 5000\n",
      "Train Loss: 5.6692\n",
      "Epoch 2799 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 2800 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 2801 of 5000\n",
      "Train Loss: 5.6745\n",
      "Epoch 2802 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 2803 of 5000\n",
      "Train Loss: 5.6837\n",
      "Epoch 2804 of 5000\n",
      "Train Loss: 5.6627\n",
      "Epoch 2805 of 5000\n",
      "Train Loss: 5.7201\n",
      "Epoch 2806 of 5000\n",
      "Train Loss: 5.6710\n",
      "Epoch 2807 of 5000\n",
      "Train Loss: 5.7070\n",
      "Epoch 2808 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 2809 of 5000\n",
      "Train Loss: 5.6813\n",
      "Epoch 2810 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 2811 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 2812 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 2813 of 5000\n",
      "Train Loss: 5.6832\n",
      "Epoch 2814 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 2815 of 5000\n",
      "Train Loss: 5.7073\n",
      "Epoch 2816 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 2817 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 2818 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 2819 of 5000\n",
      "Train Loss: 5.7171\n",
      "Epoch 2820 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 2821 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 2822 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 2823 of 5000\n",
      "Train Loss: 5.6694\n",
      "Epoch 2824 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 2825 of 5000\n",
      "Train Loss: 5.6868\n",
      "Epoch 2826 of 5000\n",
      "Train Loss: 5.7273\n",
      "Epoch 2827 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 2828 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 2829 of 5000\n",
      "Train Loss: 5.6901\n",
      "Epoch 2830 of 5000\n",
      "Train Loss: 5.7098\n",
      "Epoch 2831 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 2832 of 5000\n",
      "Train Loss: 5.6723\n",
      "Epoch 2833 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 2834 of 5000\n",
      "Train Loss: 5.6880\n",
      "Epoch 2835 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 2836 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 2837 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 2838 of 5000\n",
      "Train Loss: 5.6771\n",
      "Epoch 2839 of 5000\n",
      "Train Loss: 5.6868\n",
      "Epoch 2840 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 2841 of 5000\n",
      "Train Loss: 5.7108\n",
      "Epoch 2842 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 2843 of 5000\n",
      "Train Loss: 5.6789\n",
      "Epoch 2844 of 5000\n",
      "Train Loss: 5.6758\n",
      "Epoch 2845 of 5000\n",
      "Train Loss: 5.6891\n",
      "Epoch 2846 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 2847 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 2848 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 2849 of 5000\n",
      "Train Loss: 5.6771\n",
      "Epoch 2850 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 2851 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 2852 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 2853 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 2854 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 2855 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 2856 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 2857 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 2858 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 2859 of 5000\n",
      "Train Loss: 5.7112\n",
      "Epoch 2860 of 5000\n",
      "Train Loss: 5.6605\n",
      "Epoch 2861 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 2862 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 2863 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 2864 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 2865 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 2866 of 5000\n",
      "Train Loss: 5.6745\n",
      "Epoch 2867 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 2868 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 2869 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 2870 of 5000\n",
      "Train Loss: 5.7120\n",
      "Epoch 2871 of 5000\n",
      "Train Loss: 5.6728\n",
      "Epoch 2872 of 5000\n",
      "Train Loss: 5.6840\n",
      "Epoch 2873 of 5000\n",
      "Train Loss: 5.6700\n",
      "Epoch 2874 of 5000\n",
      "Train Loss: 5.6725\n",
      "Epoch 2875 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 2876 of 5000\n",
      "Train Loss: 5.6789\n",
      "Epoch 2877 of 5000\n",
      "Train Loss: 5.6852\n",
      "Epoch 2878 of 5000\n",
      "Train Loss: 5.6684\n",
      "Epoch 2879 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 2880 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 2881 of 5000\n",
      "Train Loss: 5.6801\n",
      "Epoch 2882 of 5000\n",
      "Train Loss: 5.7168\n",
      "Epoch 2883 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 2884 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 2885 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 2886 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 2887 of 5000\n",
      "Train Loss: 5.6729\n",
      "Epoch 2888 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 2889 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 2890 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 2891 of 5000\n",
      "Train Loss: 5.6795\n",
      "Epoch 2892 of 5000\n",
      "Train Loss: 5.6989\n",
      "Epoch 2893 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 2894 of 5000\n",
      "Train Loss: 5.6839\n",
      "Epoch 2895 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 2896 of 5000\n",
      "Train Loss: 5.6829\n",
      "Epoch 2897 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 2898 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 2899 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 2900 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 2901 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 2902 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 2903 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 2904 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 2905 of 5000\n",
      "Train Loss: 5.6824\n",
      "Epoch 2906 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 2907 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 2908 of 5000\n",
      "Train Loss: 5.7199\n",
      "Epoch 2909 of 5000\n",
      "Train Loss: 5.6844\n",
      "Epoch 2910 of 5000\n",
      "Train Loss: 5.7257\n",
      "Epoch 2911 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 2912 of 5000\n",
      "Train Loss: 5.7106\n",
      "Epoch 2913 of 5000\n",
      "Train Loss: 5.6679\n",
      "Epoch 2914 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 2915 of 5000\n",
      "Train Loss: 5.6871\n",
      "Epoch 2916 of 5000\n",
      "Train Loss: 5.6771\n",
      "Epoch 2917 of 5000\n",
      "Train Loss: 5.6998\n",
      "Epoch 2918 of 5000\n",
      "Train Loss: 5.7183\n",
      "Epoch 2919 of 5000\n",
      "Train Loss: 5.7054\n",
      "Epoch 2920 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 2921 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 2922 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 2923 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 2924 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 2925 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 2926 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 2927 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 2928 of 5000\n",
      "Train Loss: 5.6825\n",
      "Epoch 2929 of 5000\n",
      "Train Loss: 5.6891\n",
      "Epoch 2930 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 2931 of 5000\n",
      "Train Loss: 5.6785\n",
      "Epoch 2932 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 2933 of 5000\n",
      "Train Loss: 5.7110\n",
      "Epoch 2934 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 2935 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 2936 of 5000\n",
      "Train Loss: 5.7018\n",
      "Epoch 2937 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 2938 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 2939 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 2940 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 2941 of 5000\n",
      "Train Loss: 5.7140\n",
      "Epoch 2942 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 2943 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 2944 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 2945 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 2946 of 5000\n",
      "Train Loss: 5.6816\n",
      "Epoch 2947 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 2948 of 5000\n",
      "Train Loss: 5.6930\n",
      "Epoch 2949 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 2950 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 2951 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 2952 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 2953 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 2954 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 2955 of 5000\n",
      "Train Loss: 5.7090\n",
      "Epoch 2956 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 2957 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 2958 of 5000\n",
      "Train Loss: 5.6733\n",
      "Epoch 2959 of 5000\n",
      "Train Loss: 5.6752\n",
      "Epoch 2960 of 5000\n",
      "Train Loss: 5.6940\n",
      "Epoch 2961 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 2962 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 2963 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 2964 of 5000\n",
      "Train Loss: 5.6790\n",
      "Epoch 2965 of 5000\n",
      "Train Loss: 5.6766\n",
      "Epoch 2966 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 2967 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 2968 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 2969 of 5000\n",
      "Train Loss: 5.6706\n",
      "Epoch 2970 of 5000\n",
      "Train Loss: 5.7018\n",
      "Epoch 2971 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 2972 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 2973 of 5000\n",
      "Train Loss: 5.7180\n",
      "Epoch 2974 of 5000\n",
      "Train Loss: 5.7187\n",
      "Epoch 2975 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 2976 of 5000\n",
      "Train Loss: 5.7114\n",
      "Epoch 2977 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 2978 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 2979 of 5000\n",
      "Train Loss: 5.7073\n",
      "Epoch 2980 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 2981 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 2982 of 5000\n",
      "Train Loss: 5.6738\n",
      "Epoch 2983 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 2984 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 2985 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 2986 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 2987 of 5000\n",
      "Train Loss: 5.6820\n",
      "Epoch 2988 of 5000\n",
      "Train Loss: 5.6677\n",
      "Epoch 2989 of 5000\n",
      "Train Loss: 5.7126\n",
      "Epoch 2990 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 2991 of 5000\n",
      "Train Loss: 5.6958\n",
      "Epoch 2992 of 5000\n",
      "Train Loss: 5.7169\n",
      "Epoch 2993 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 2994 of 5000\n",
      "Train Loss: 5.6823\n",
      "Epoch 2995 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 2996 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 2997 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 2998 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 2999 of 5000\n",
      "Train Loss: 5.6814\n",
      "Epoch 3000 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 3001 of 5000\n",
      "Train Loss: 5.6661\n",
      "Epoch 3002 of 5000\n",
      "Train Loss: 5.6818\n",
      "Epoch 3003 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 3004 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 3005 of 5000\n",
      "Train Loss: 5.6723\n",
      "Epoch 3006 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 3007 of 5000\n",
      "Train Loss: 5.6677\n",
      "Epoch 3008 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 3009 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 3010 of 5000\n",
      "Train Loss: 5.7126\n",
      "Epoch 3011 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 3012 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 3013 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 3014 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 3015 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 3016 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 3017 of 5000\n",
      "Train Loss: 5.6958\n",
      "Epoch 3018 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 3019 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 3020 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 3021 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 3022 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 3023 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 3024 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 3025 of 5000\n",
      "Train Loss: 5.6809\n",
      "Epoch 3026 of 5000\n",
      "Train Loss: 5.7237\n",
      "Epoch 3027 of 5000\n",
      "Train Loss: 5.6797\n",
      "Epoch 3028 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 3029 of 5000\n",
      "Train Loss: 5.7266\n",
      "Epoch 3030 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 3031 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 3032 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 3033 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 3034 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 3035 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 3036 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 3037 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 3038 of 5000\n",
      "Train Loss: 5.6989\n",
      "Epoch 3039 of 5000\n",
      "Train Loss: 5.6757\n",
      "Epoch 3040 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 3041 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 3042 of 5000\n",
      "Train Loss: 5.6862\n",
      "Epoch 3043 of 5000\n",
      "Train Loss: 5.6776\n",
      "Epoch 3044 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 3045 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 3046 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 3047 of 5000\n",
      "Train Loss: 5.7211\n",
      "Epoch 3048 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 3049 of 5000\n",
      "Train Loss: 5.7123\n",
      "Epoch 3050 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 3051 of 5000\n",
      "Train Loss: 5.7148\n",
      "Epoch 3052 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 3053 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 3054 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 3055 of 5000\n",
      "Train Loss: 5.7041\n",
      "Epoch 3056 of 5000\n",
      "Train Loss: 5.6682\n",
      "Epoch 3057 of 5000\n",
      "Train Loss: 5.7082\n",
      "Epoch 3058 of 5000\n",
      "Train Loss: 5.6722\n",
      "Epoch 3059 of 5000\n",
      "Train Loss: 5.7026\n",
      "Epoch 3060 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 3061 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 3062 of 5000\n",
      "Train Loss: 5.6813\n",
      "Epoch 3063 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 3064 of 5000\n",
      "Train Loss: 5.6783\n",
      "Epoch 3065 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 3066 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 3067 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 3068 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 3069 of 5000\n",
      "Train Loss: 5.6809\n",
      "Epoch 3070 of 5000\n",
      "Train Loss: 5.6709\n",
      "Epoch 3071 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 3072 of 5000\n",
      "Train Loss: 5.6809\n",
      "Epoch 3073 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 3074 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 3075 of 5000\n",
      "Train Loss: 5.6749\n",
      "Epoch 3076 of 5000\n",
      "Train Loss: 5.6794\n",
      "Epoch 3077 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 3078 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 3079 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 3080 of 5000\n",
      "Train Loss: 5.6721\n",
      "Epoch 3081 of 5000\n",
      "Train Loss: 5.7096\n",
      "Epoch 3082 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 3083 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 3084 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 3085 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 3086 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 3087 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 3088 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 3089 of 5000\n",
      "Train Loss: 5.7248\n",
      "Epoch 3090 of 5000\n",
      "Train Loss: 5.6823\n",
      "Epoch 3091 of 5000\n",
      "Train Loss: 5.6869\n",
      "Epoch 3092 of 5000\n",
      "Train Loss: 5.6869\n",
      "Epoch 3093 of 5000\n",
      "Train Loss: 5.7072\n",
      "Epoch 3094 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 3095 of 5000\n",
      "Train Loss: 5.6824\n",
      "Epoch 3096 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 3097 of 5000\n",
      "Train Loss: 5.6826\n",
      "Epoch 3098 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 3099 of 5000\n",
      "Train Loss: 5.6734\n",
      "Epoch 3100 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 3101 of 5000\n",
      "Train Loss: 5.7173\n",
      "Epoch 3102 of 5000\n",
      "Train Loss: 5.7058\n",
      "Epoch 3103 of 5000\n",
      "Train Loss: 5.7139\n",
      "Epoch 3104 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 3105 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 3106 of 5000\n",
      "Train Loss: 5.6619\n",
      "Epoch 3107 of 5000\n",
      "Train Loss: 5.6709\n",
      "Epoch 3108 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 3109 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 3110 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 3111 of 5000\n",
      "Train Loss: 5.6813\n",
      "Epoch 3112 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 3113 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 3114 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 3115 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 3116 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 3117 of 5000\n",
      "Train Loss: 5.6871\n",
      "Epoch 3118 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 3119 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 3120 of 5000\n",
      "Train Loss: 5.6697\n",
      "Epoch 3121 of 5000\n",
      "Train Loss: 5.7155\n",
      "Epoch 3122 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 3123 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 3124 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 3125 of 5000\n",
      "Train Loss: 5.6851\n",
      "Epoch 3126 of 5000\n",
      "Train Loss: 5.6704\n",
      "Epoch 3127 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 3128 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 3129 of 5000\n",
      "Train Loss: 5.6765\n",
      "Epoch 3130 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 3131 of 5000\n",
      "Train Loss: 5.7039\n",
      "Epoch 3132 of 5000\n",
      "Train Loss: 5.6862\n",
      "Epoch 3133 of 5000\n",
      "Train Loss: 5.7137\n",
      "Epoch 3134 of 5000\n",
      "Train Loss: 5.7169\n",
      "Epoch 3135 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 3136 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 3137 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 3138 of 5000\n",
      "Train Loss: 5.6801\n",
      "Epoch 3139 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 3140 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 3141 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 3142 of 5000\n",
      "Train Loss: 5.6868\n",
      "Epoch 3143 of 5000\n",
      "Train Loss: 5.7197\n",
      "Epoch 3144 of 5000\n",
      "Train Loss: 5.7115\n",
      "Epoch 3145 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 3146 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 3147 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 3148 of 5000\n",
      "Train Loss: 5.7129\n",
      "Epoch 3149 of 5000\n",
      "Train Loss: 5.7157\n",
      "Epoch 3150 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 3151 of 5000\n",
      "Train Loss: 5.6726\n",
      "Epoch 3152 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 3153 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 3154 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 3155 of 5000\n",
      "Train Loss: 5.6787\n",
      "Epoch 3156 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 3157 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 3158 of 5000\n",
      "Train Loss: 5.6871\n",
      "Epoch 3159 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 3160 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 3161 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 3162 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 3163 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 3164 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 3165 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 3166 of 5000\n",
      "Train Loss: 5.7109\n",
      "Epoch 3167 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 3168 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 3169 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 3170 of 5000\n",
      "Train Loss: 5.6686\n",
      "Epoch 3171 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 3172 of 5000\n",
      "Train Loss: 5.6675\n",
      "Epoch 3173 of 5000\n",
      "Train Loss: 5.6680\n",
      "Epoch 3174 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 3175 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 3176 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 3177 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 3178 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 3179 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 3180 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 3181 of 5000\n",
      "Train Loss: 5.6830\n",
      "Epoch 3182 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 3183 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 3184 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 3185 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 3186 of 5000\n",
      "Train Loss: 5.6870\n",
      "Epoch 3187 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 3188 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 3189 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 3190 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 3191 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 3192 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 3193 of 5000\n",
      "Train Loss: 5.6872\n",
      "Epoch 3194 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 3195 of 5000\n",
      "Train Loss: 5.7102\n",
      "Epoch 3196 of 5000\n",
      "Train Loss: 5.7114\n",
      "Epoch 3197 of 5000\n",
      "Train Loss: 5.6788\n",
      "Epoch 3198 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 3199 of 5000\n",
      "Train Loss: 5.7071\n",
      "Epoch 3200 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 3201 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 3202 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 3203 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 3204 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 3205 of 5000\n",
      "Train Loss: 5.6748\n",
      "Epoch 3206 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 3207 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 3208 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 3209 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 3210 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 3211 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 3212 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 3213 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 3214 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 3215 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 3216 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 3217 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 3218 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 3219 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 3220 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 3221 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 3222 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 3223 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 3224 of 5000\n",
      "Train Loss: 5.6744\n",
      "Epoch 3225 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 3226 of 5000\n",
      "Train Loss: 5.6639\n",
      "Epoch 3227 of 5000\n",
      "Train Loss: 5.6742\n",
      "Epoch 3228 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 3229 of 5000\n",
      "Train Loss: 5.6794\n",
      "Epoch 3230 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 3231 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 3232 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 3233 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 3234 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 3235 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 3236 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 3237 of 5000\n",
      "Train Loss: 5.7108\n",
      "Epoch 3238 of 5000\n",
      "Train Loss: 5.7094\n",
      "Epoch 3239 of 5000\n",
      "Train Loss: 5.6809\n",
      "Epoch 3240 of 5000\n",
      "Train Loss: 5.6948\n",
      "Epoch 3241 of 5000\n",
      "Train Loss: 5.6784\n",
      "Epoch 3242 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 3243 of 5000\n",
      "Train Loss: 5.6865\n",
      "Epoch 3244 of 5000\n",
      "Train Loss: 5.6871\n",
      "Epoch 3245 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 3246 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 3247 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 3248 of 5000\n",
      "Train Loss: 5.6622\n",
      "Epoch 3249 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 3250 of 5000\n",
      "Train Loss: 5.6743\n",
      "Epoch 3251 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 3252 of 5000\n",
      "Train Loss: 5.6851\n",
      "Epoch 3253 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 3254 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 3255 of 5000\n",
      "Train Loss: 5.6657\n",
      "Epoch 3256 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 3257 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 3258 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 3259 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 3260 of 5000\n",
      "Train Loss: 5.6766\n",
      "Epoch 3261 of 5000\n",
      "Train Loss: 5.6842\n",
      "Epoch 3262 of 5000\n",
      "Train Loss: 5.6846\n",
      "Epoch 3263 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 3264 of 5000\n",
      "Train Loss: 5.7212\n",
      "Epoch 3265 of 5000\n",
      "Train Loss: 5.7183\n",
      "Epoch 3266 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 3267 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 3268 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 3269 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 3270 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 3271 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 3272 of 5000\n",
      "Train Loss: 5.6870\n",
      "Epoch 3273 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 3274 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 3275 of 5000\n",
      "Train Loss: 5.7111\n",
      "Epoch 3276 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 3277 of 5000\n",
      "Train Loss: 5.7104\n",
      "Epoch 3278 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 3279 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 3280 of 5000\n",
      "Train Loss: 5.6801\n",
      "Epoch 3281 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 3282 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 3283 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 3284 of 5000\n",
      "Train Loss: 5.7063\n",
      "Epoch 3285 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 3286 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 3287 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 3288 of 5000\n",
      "Train Loss: 5.6823\n",
      "Epoch 3289 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 3290 of 5000\n",
      "Train Loss: 5.7203\n",
      "Epoch 3291 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 3292 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 3293 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 3294 of 5000\n",
      "Train Loss: 5.6728\n",
      "Epoch 3295 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 3296 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 3297 of 5000\n",
      "Train Loss: 5.7148\n",
      "Epoch 3298 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 3299 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 3300 of 5000\n",
      "Train Loss: 5.6746\n",
      "Epoch 3301 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 3302 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 3303 of 5000\n",
      "Train Loss: 5.6786\n",
      "Epoch 3304 of 5000\n",
      "Train Loss: 5.6658\n",
      "Epoch 3305 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 3306 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 3307 of 5000\n",
      "Train Loss: 5.6778\n",
      "Epoch 3308 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 3309 of 5000\n",
      "Train Loss: 5.6764\n",
      "Epoch 3310 of 5000\n",
      "Train Loss: 5.7161\n",
      "Epoch 3311 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 3312 of 5000\n",
      "Train Loss: 5.6777\n",
      "Epoch 3313 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 3314 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 3315 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 3316 of 5000\n",
      "Train Loss: 5.6776\n",
      "Epoch 3317 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 3318 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 3319 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 3320 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 3321 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 3322 of 5000\n",
      "Train Loss: 5.6737\n",
      "Epoch 3323 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 3324 of 5000\n",
      "Train Loss: 5.6813\n",
      "Epoch 3325 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 3326 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 3327 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 3328 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 3329 of 5000\n",
      "Train Loss: 5.7100\n",
      "Epoch 3330 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 3331 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 3332 of 5000\n",
      "Train Loss: 5.6948\n",
      "Epoch 3333 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 3334 of 5000\n",
      "Train Loss: 5.7111\n",
      "Epoch 3335 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 3336 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 3337 of 5000\n",
      "Train Loss: 5.6870\n",
      "Epoch 3338 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 3339 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 3340 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 3341 of 5000\n",
      "Train Loss: 5.6599\n",
      "Epoch 3342 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 3343 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 3344 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 3345 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 3346 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 3347 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 3348 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 3349 of 5000\n",
      "Train Loss: 5.6810\n",
      "Epoch 3350 of 5000\n",
      "Train Loss: 5.6824\n",
      "Epoch 3351 of 5000\n",
      "Train Loss: 5.6816\n",
      "Epoch 3352 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 3353 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 3354 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 3355 of 5000\n",
      "Train Loss: 5.6784\n",
      "Epoch 3356 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 3357 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 3358 of 5000\n",
      "Train Loss: 5.6763\n",
      "Epoch 3359 of 5000\n",
      "Train Loss: 5.6689\n",
      "Epoch 3360 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 3361 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 3362 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 3363 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 3364 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 3365 of 5000\n",
      "Train Loss: 5.7136\n",
      "Epoch 3366 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 3367 of 5000\n",
      "Train Loss: 5.7332\n",
      "Epoch 3368 of 5000\n",
      "Train Loss: 5.6880\n",
      "Epoch 3369 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 3370 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 3371 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 3372 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 3373 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 3374 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 3375 of 5000\n",
      "Train Loss: 5.6992\n",
      "Epoch 3376 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 3377 of 5000\n",
      "Train Loss: 5.7078\n",
      "Epoch 3378 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 3379 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 3380 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 3381 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 3382 of 5000\n",
      "Train Loss: 5.6829\n",
      "Epoch 3383 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 3384 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 3385 of 5000\n",
      "Train Loss: 5.6763\n",
      "Epoch 3386 of 5000\n",
      "Train Loss: 5.6771\n",
      "Epoch 3387 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 3388 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 3389 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 3390 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 3391 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 3392 of 5000\n",
      "Train Loss: 5.6788\n",
      "Epoch 3393 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 3394 of 5000\n",
      "Train Loss: 5.6709\n",
      "Epoch 3395 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 3396 of 5000\n",
      "Train Loss: 5.6731\n",
      "Epoch 3397 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 3398 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 3399 of 5000\n",
      "Train Loss: 5.6609\n",
      "Epoch 3400 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 3401 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 3402 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 3403 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 3404 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 3405 of 5000\n",
      "Train Loss: 5.6803\n",
      "Epoch 3406 of 5000\n",
      "Train Loss: 5.7184\n",
      "Epoch 3407 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 3408 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 3409 of 5000\n",
      "Train Loss: 5.6871\n",
      "Epoch 3410 of 5000\n",
      "Train Loss: 5.7080\n",
      "Epoch 3411 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 3412 of 5000\n",
      "Train Loss: 5.6852\n",
      "Epoch 3413 of 5000\n",
      "Train Loss: 5.7123\n",
      "Epoch 3414 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 3415 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 3416 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 3417 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 3418 of 5000\n",
      "Train Loss: 5.7089\n",
      "Epoch 3419 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 3420 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 3421 of 5000\n",
      "Train Loss: 5.6961\n",
      "Epoch 3422 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 3423 of 5000\n",
      "Train Loss: 5.6761\n",
      "Epoch 3424 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 3425 of 5000\n",
      "Train Loss: 5.7041\n",
      "Epoch 3426 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 3427 of 5000\n",
      "Train Loss: 5.6709\n",
      "Epoch 3428 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 3429 of 5000\n",
      "Train Loss: 5.7124\n",
      "Epoch 3430 of 5000\n",
      "Train Loss: 5.6692\n",
      "Epoch 3431 of 5000\n",
      "Train Loss: 5.7009\n",
      "Epoch 3432 of 5000\n",
      "Train Loss: 5.7115\n",
      "Epoch 3433 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 3434 of 5000\n",
      "Train Loss: 5.7135\n",
      "Epoch 3435 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 3436 of 5000\n",
      "Train Loss: 5.6777\n",
      "Epoch 3437 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 3438 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 3439 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 3440 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 3441 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 3442 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 3443 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 3444 of 5000\n",
      "Train Loss: 5.6630\n",
      "Epoch 3445 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 3446 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 3447 of 5000\n",
      "Train Loss: 5.6868\n",
      "Epoch 3448 of 5000\n",
      "Train Loss: 5.6961\n",
      "Epoch 3449 of 5000\n",
      "Train Loss: 5.7043\n",
      "Epoch 3450 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 3451 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 3452 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 3453 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 3454 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 3455 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 3456 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 3457 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 3458 of 5000\n",
      "Train Loss: 5.7138\n",
      "Epoch 3459 of 5000\n",
      "Train Loss: 5.7251\n",
      "Epoch 3460 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 3461 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 3462 of 5000\n",
      "Train Loss: 5.6863\n",
      "Epoch 3463 of 5000\n",
      "Train Loss: 5.7173\n",
      "Epoch 3464 of 5000\n",
      "Train Loss: 5.6840\n",
      "Epoch 3465 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 3466 of 5000\n",
      "Train Loss: 5.6779\n",
      "Epoch 3467 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 3468 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 3469 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 3470 of 5000\n",
      "Train Loss: 5.6775\n",
      "Epoch 3471 of 5000\n",
      "Train Loss: 5.6992\n",
      "Epoch 3472 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 3473 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 3474 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 3475 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 3476 of 5000\n",
      "Train Loss: 5.7179\n",
      "Epoch 3477 of 5000\n",
      "Train Loss: 5.7240\n",
      "Epoch 3478 of 5000\n",
      "Train Loss: 5.6797\n",
      "Epoch 3479 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 3480 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 3481 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 3482 of 5000\n",
      "Train Loss: 5.6987\n",
      "Epoch 3483 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 3484 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 3485 of 5000\n",
      "Train Loss: 5.6779\n",
      "Epoch 3486 of 5000\n",
      "Train Loss: 5.7167\n",
      "Epoch 3487 of 5000\n",
      "Train Loss: 5.6758\n",
      "Epoch 3488 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 3489 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 3490 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 3491 of 5000\n",
      "Train Loss: 5.6790\n",
      "Epoch 3492 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 3493 of 5000\n",
      "Train Loss: 5.6998\n",
      "Epoch 3494 of 5000\n",
      "Train Loss: 5.6811\n",
      "Epoch 3495 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 3496 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 3497 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 3498 of 5000\n",
      "Train Loss: 5.6936\n",
      "Epoch 3499 of 5000\n",
      "Train Loss: 5.6770\n",
      "Epoch 3500 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 3501 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 3502 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 3503 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 3504 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 3505 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 3506 of 5000\n",
      "Train Loss: 5.6785\n",
      "Epoch 3507 of 5000\n",
      "Train Loss: 5.7115\n",
      "Epoch 3508 of 5000\n",
      "Train Loss: 5.7098\n",
      "Epoch 3509 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 3510 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 3511 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 3512 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 3513 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 3514 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 3515 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 3516 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 3517 of 5000\n",
      "Train Loss: 5.6830\n",
      "Epoch 3518 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 3519 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 3520 of 5000\n",
      "Train Loss: 5.7133\n",
      "Epoch 3521 of 5000\n",
      "Train Loss: 5.6868\n",
      "Epoch 3522 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 3523 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 3524 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 3525 of 5000\n",
      "Train Loss: 5.6756\n",
      "Epoch 3526 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 3527 of 5000\n",
      "Train Loss: 5.6820\n",
      "Epoch 3528 of 5000\n",
      "Train Loss: 5.6798\n",
      "Epoch 3529 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 3530 of 5000\n",
      "Train Loss: 5.7221\n",
      "Epoch 3531 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 3532 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 3533 of 5000\n",
      "Train Loss: 5.6762\n",
      "Epoch 3534 of 5000\n",
      "Train Loss: 5.6656\n",
      "Epoch 3535 of 5000\n",
      "Train Loss: 5.6940\n",
      "Epoch 3536 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 3537 of 5000\n",
      "Train Loss: 5.6604\n",
      "Epoch 3538 of 5000\n",
      "Train Loss: 5.6986\n",
      "Epoch 3539 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 3540 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 3541 of 5000\n",
      "Train Loss: 5.6676\n",
      "Epoch 3542 of 5000\n",
      "Train Loss: 5.6958\n",
      "Epoch 3543 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 3544 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 3545 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 3546 of 5000\n",
      "Train Loss: 5.6844\n",
      "Epoch 3547 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 3548 of 5000\n",
      "Train Loss: 5.6812\n",
      "Epoch 3549 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 3550 of 5000\n",
      "Train Loss: 5.6742\n",
      "Epoch 3551 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 3552 of 5000\n",
      "Train Loss: 5.6834\n",
      "Epoch 3553 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 3554 of 5000\n",
      "Train Loss: 5.6823\n",
      "Epoch 3555 of 5000\n",
      "Train Loss: 5.6740\n",
      "Epoch 3556 of 5000\n",
      "Train Loss: 5.6780\n",
      "Epoch 3557 of 5000\n",
      "Train Loss: 5.7054\n",
      "Epoch 3558 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 3559 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 3560 of 5000\n",
      "Train Loss: 5.6797\n",
      "Epoch 3561 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 3562 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 3563 of 5000\n",
      "Train Loss: 5.6764\n",
      "Epoch 3564 of 5000\n",
      "Train Loss: 5.6791\n",
      "Epoch 3565 of 5000\n",
      "Train Loss: 5.6826\n",
      "Epoch 3566 of 5000\n",
      "Train Loss: 5.7231\n",
      "Epoch 3567 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 3568 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 3569 of 5000\n",
      "Train Loss: 5.6842\n",
      "Epoch 3570 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 3571 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 3572 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 3573 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 3574 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 3575 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 3576 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 3577 of 5000\n",
      "Train Loss: 5.6808\n",
      "Epoch 3578 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 3579 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 3580 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 3581 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 3582 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 3583 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 3584 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 3585 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 3586 of 5000\n",
      "Train Loss: 5.6958\n",
      "Epoch 3587 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 3588 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 3589 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 3590 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 3591 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 3592 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 3593 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 3594 of 5000\n",
      "Train Loss: 5.6769\n",
      "Epoch 3595 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 3596 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 3597 of 5000\n",
      "Train Loss: 5.7128\n",
      "Epoch 3598 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 3599 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 3600 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 3601 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 3602 of 5000\n",
      "Train Loss: 5.6835\n",
      "Epoch 3603 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 3604 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 3605 of 5000\n",
      "Train Loss: 5.7089\n",
      "Epoch 3606 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 3607 of 5000\n",
      "Train Loss: 5.7078\n",
      "Epoch 3608 of 5000\n",
      "Train Loss: 5.6799\n",
      "Epoch 3609 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 3610 of 5000\n",
      "Train Loss: 5.7090\n",
      "Epoch 3611 of 5000\n",
      "Train Loss: 5.6779\n",
      "Epoch 3612 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 3613 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 3614 of 5000\n",
      "Train Loss: 5.6844\n",
      "Epoch 3615 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 3616 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 3617 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 3618 of 5000\n",
      "Train Loss: 5.6794\n",
      "Epoch 3619 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 3620 of 5000\n",
      "Train Loss: 5.6835\n",
      "Epoch 3621 of 5000\n",
      "Train Loss: 5.7126\n",
      "Epoch 3622 of 5000\n",
      "Train Loss: 5.6892\n",
      "Epoch 3623 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 3624 of 5000\n",
      "Train Loss: 5.6731\n",
      "Epoch 3625 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 3626 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 3627 of 5000\n",
      "Train Loss: 5.6809\n",
      "Epoch 3628 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 3629 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 3630 of 5000\n",
      "Train Loss: 5.6684\n",
      "Epoch 3631 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 3632 of 5000\n",
      "Train Loss: 5.6692\n",
      "Epoch 3633 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 3634 of 5000\n",
      "Train Loss: 5.6836\n",
      "Epoch 3635 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 3636 of 5000\n",
      "Train Loss: 5.7018\n",
      "Epoch 3637 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 3638 of 5000\n",
      "Train Loss: 5.6788\n",
      "Epoch 3639 of 5000\n",
      "Train Loss: 5.6846\n",
      "Epoch 3640 of 5000\n",
      "Train Loss: 5.7138\n",
      "Epoch 3641 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 3642 of 5000\n",
      "Train Loss: 5.7120\n",
      "Epoch 3643 of 5000\n",
      "Train Loss: 5.6862\n",
      "Epoch 3644 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 3645 of 5000\n",
      "Train Loss: 5.6753\n",
      "Epoch 3646 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 3647 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 3648 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 3649 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 3650 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 3651 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 3652 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 3653 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 3654 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 3655 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 3656 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 3657 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 3658 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 3659 of 5000\n",
      "Train Loss: 5.6655\n",
      "Epoch 3660 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 3661 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 3662 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 3663 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 3664 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 3665 of 5000\n",
      "Train Loss: 5.6822\n",
      "Epoch 3666 of 5000\n",
      "Train Loss: 5.6761\n",
      "Epoch 3667 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 3668 of 5000\n",
      "Train Loss: 5.6764\n",
      "Epoch 3669 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 3670 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 3671 of 5000\n",
      "Train Loss: 5.6832\n",
      "Epoch 3672 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 3673 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 3674 of 5000\n",
      "Train Loss: 5.6844\n",
      "Epoch 3675 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 3676 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 3677 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 3678 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 3679 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 3680 of 5000\n",
      "Train Loss: 5.6648\n",
      "Epoch 3681 of 5000\n",
      "Train Loss: 5.7244\n",
      "Epoch 3682 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 3683 of 5000\n",
      "Train Loss: 5.7107\n",
      "Epoch 3684 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 3685 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 3686 of 5000\n",
      "Train Loss: 5.6715\n",
      "Epoch 3687 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 3688 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 3689 of 5000\n",
      "Train Loss: 5.6739\n",
      "Epoch 3690 of 5000\n",
      "Train Loss: 5.6737\n",
      "Epoch 3691 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 3692 of 5000\n",
      "Train Loss: 5.7388\n",
      "Epoch 3693 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 3694 of 5000\n",
      "Train Loss: 5.7143\n",
      "Epoch 3695 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 3696 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 3697 of 5000\n",
      "Train Loss: 5.6892\n",
      "Epoch 3698 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 3699 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 3700 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 3701 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 3702 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 3703 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 3704 of 5000\n",
      "Train Loss: 5.6795\n",
      "Epoch 3705 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 3706 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 3707 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 3708 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 3709 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 3710 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 3711 of 5000\n",
      "Train Loss: 5.7174\n",
      "Epoch 3712 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 3713 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 3714 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 3715 of 5000\n",
      "Train Loss: 5.6756\n",
      "Epoch 3716 of 5000\n",
      "Train Loss: 5.6826\n",
      "Epoch 3717 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 3718 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 3719 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 3720 of 5000\n",
      "Train Loss: 5.6651\n",
      "Epoch 3721 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 3722 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 3723 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 3724 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 3725 of 5000\n",
      "Train Loss: 5.7122\n",
      "Epoch 3726 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 3727 of 5000\n",
      "Train Loss: 5.7054\n",
      "Epoch 3728 of 5000\n",
      "Train Loss: 5.6604\n",
      "Epoch 3729 of 5000\n",
      "Train Loss: 5.7026\n",
      "Epoch 3730 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 3731 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 3732 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 3733 of 5000\n",
      "Train Loss: 5.6746\n",
      "Epoch 3734 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 3735 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 3736 of 5000\n",
      "Train Loss: 5.6846\n",
      "Epoch 3737 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 3738 of 5000\n",
      "Train Loss: 5.6765\n",
      "Epoch 3739 of 5000\n",
      "Train Loss: 5.7120\n",
      "Epoch 3740 of 5000\n",
      "Train Loss: 5.6839\n",
      "Epoch 3741 of 5000\n",
      "Train Loss: 5.6738\n",
      "Epoch 3742 of 5000\n",
      "Train Loss: 5.7138\n",
      "Epoch 3743 of 5000\n",
      "Train Loss: 5.6724\n",
      "Epoch 3744 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 3745 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 3746 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 3747 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 3748 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 3749 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 3750 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 3751 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 3752 of 5000\n",
      "Train Loss: 5.6825\n",
      "Epoch 3753 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 3754 of 5000\n",
      "Train Loss: 5.7070\n",
      "Epoch 3755 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 3756 of 5000\n",
      "Train Loss: 5.6886\n",
      "Epoch 3757 of 5000\n",
      "Train Loss: 5.6761\n",
      "Epoch 3758 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 3759 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 3760 of 5000\n",
      "Train Loss: 5.6808\n",
      "Epoch 3761 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 3762 of 5000\n",
      "Train Loss: 5.6746\n",
      "Epoch 3763 of 5000\n",
      "Train Loss: 5.7214\n",
      "Epoch 3764 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 3765 of 5000\n",
      "Train Loss: 5.6862\n",
      "Epoch 3766 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 3767 of 5000\n",
      "Train Loss: 5.7089\n",
      "Epoch 3768 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 3769 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 3770 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 3771 of 5000\n",
      "Train Loss: 5.7163\n",
      "Epoch 3772 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 3773 of 5000\n",
      "Train Loss: 5.6863\n",
      "Epoch 3774 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 3775 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 3776 of 5000\n",
      "Train Loss: 5.6958\n",
      "Epoch 3777 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 3778 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 3779 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 3780 of 5000\n",
      "Train Loss: 5.6880\n",
      "Epoch 3781 of 5000\n",
      "Train Loss: 5.7213\n",
      "Epoch 3782 of 5000\n",
      "Train Loss: 5.6865\n",
      "Epoch 3783 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 3784 of 5000\n",
      "Train Loss: 5.6836\n",
      "Epoch 3785 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 3786 of 5000\n",
      "Train Loss: 5.7161\n",
      "Epoch 3787 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 3788 of 5000\n",
      "Train Loss: 5.6869\n",
      "Epoch 3789 of 5000\n",
      "Train Loss: 5.6819\n",
      "Epoch 3790 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 3791 of 5000\n",
      "Train Loss: 5.6772\n",
      "Epoch 3792 of 5000\n",
      "Train Loss: 5.7063\n",
      "Epoch 3793 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 3794 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 3795 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 3796 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 3797 of 5000\n",
      "Train Loss: 5.6844\n",
      "Epoch 3798 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 3799 of 5000\n",
      "Train Loss: 5.7018\n",
      "Epoch 3800 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 3801 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 3802 of 5000\n",
      "Train Loss: 5.6891\n",
      "Epoch 3803 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 3804 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 3805 of 5000\n",
      "Train Loss: 5.7066\n",
      "Epoch 3806 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 3807 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 3808 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 3809 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 3810 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 3811 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 3812 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 3813 of 5000\n",
      "Train Loss: 5.6948\n",
      "Epoch 3814 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 3815 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 3816 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 3817 of 5000\n",
      "Train Loss: 5.7089\n",
      "Epoch 3818 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 3819 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 3820 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 3821 of 5000\n",
      "Train Loss: 5.7114\n",
      "Epoch 3822 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 3823 of 5000\n",
      "Train Loss: 5.6703\n",
      "Epoch 3824 of 5000\n",
      "Train Loss: 5.7070\n",
      "Epoch 3825 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 3826 of 5000\n",
      "Train Loss: 5.6733\n",
      "Epoch 3827 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 3828 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 3829 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 3830 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 3831 of 5000\n",
      "Train Loss: 5.6722\n",
      "Epoch 3832 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 3833 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 3834 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 3835 of 5000\n",
      "Train Loss: 5.6575\n",
      "Epoch 3836 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 3837 of 5000\n",
      "Train Loss: 5.6764\n",
      "Epoch 3838 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 3839 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 3840 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 3841 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 3842 of 5000\n",
      "Train Loss: 5.6826\n",
      "Epoch 3843 of 5000\n",
      "Train Loss: 5.6798\n",
      "Epoch 3844 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 3845 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 3846 of 5000\n",
      "Train Loss: 5.7059\n",
      "Epoch 3847 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 3848 of 5000\n",
      "Train Loss: 5.6986\n",
      "Epoch 3849 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 3850 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 3851 of 5000\n",
      "Train Loss: 5.6807\n",
      "Epoch 3852 of 5000\n",
      "Train Loss: 5.7220\n",
      "Epoch 3853 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 3854 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 3855 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 3856 of 5000\n",
      "Train Loss: 5.6842\n",
      "Epoch 3857 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 3858 of 5000\n",
      "Train Loss: 5.6636\n",
      "Epoch 3859 of 5000\n",
      "Train Loss: 5.7026\n",
      "Epoch 3860 of 5000\n",
      "Train Loss: 5.6773\n",
      "Epoch 3861 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 3862 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 3863 of 5000\n",
      "Train Loss: 5.6846\n",
      "Epoch 3864 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 3865 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 3866 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 3867 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 3868 of 5000\n",
      "Train Loss: 5.6865\n",
      "Epoch 3869 of 5000\n",
      "Train Loss: 5.6748\n",
      "Epoch 3870 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 3871 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 3872 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 3873 of 5000\n",
      "Train Loss: 5.7054\n",
      "Epoch 3874 of 5000\n",
      "Train Loss: 5.6834\n",
      "Epoch 3875 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 3876 of 5000\n",
      "Train Loss: 5.6712\n",
      "Epoch 3877 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 3878 of 5000\n",
      "Train Loss: 5.7186\n",
      "Epoch 3879 of 5000\n",
      "Train Loss: 5.6750\n",
      "Epoch 3880 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 3881 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 3882 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 3883 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 3884 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 3885 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 3886 of 5000\n",
      "Train Loss: 5.7056\n",
      "Epoch 3887 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 3888 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 3889 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 3890 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 3891 of 5000\n",
      "Train Loss: 5.6987\n",
      "Epoch 3892 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 3893 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 3894 of 5000\n",
      "Train Loss: 5.6839\n",
      "Epoch 3895 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 3896 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 3897 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 3898 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 3899 of 5000\n",
      "Train Loss: 5.6804\n",
      "Epoch 3900 of 5000\n",
      "Train Loss: 5.6788\n",
      "Epoch 3901 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 3902 of 5000\n",
      "Train Loss: 5.6733\n",
      "Epoch 3903 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 3904 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 3905 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 3906 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 3907 of 5000\n",
      "Train Loss: 5.6680\n",
      "Epoch 3908 of 5000\n",
      "Train Loss: 5.6810\n",
      "Epoch 3909 of 5000\n",
      "Train Loss: 5.7103\n",
      "Epoch 3910 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 3911 of 5000\n",
      "Train Loss: 5.7178\n",
      "Epoch 3912 of 5000\n",
      "Train Loss: 5.6701\n",
      "Epoch 3913 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 3914 of 5000\n",
      "Train Loss: 5.6641\n",
      "Epoch 3915 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 3916 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 3917 of 5000\n",
      "Train Loss: 5.6757\n",
      "Epoch 3918 of 5000\n",
      "Train Loss: 5.7003\n",
      "Epoch 3919 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 3920 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 3921 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 3922 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 3923 of 5000\n",
      "Train Loss: 5.7098\n",
      "Epoch 3924 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 3925 of 5000\n",
      "Train Loss: 5.7051\n",
      "Epoch 3926 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 3927 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 3928 of 5000\n",
      "Train Loss: 5.7110\n",
      "Epoch 3929 of 5000\n",
      "Train Loss: 5.6799\n",
      "Epoch 3930 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 3931 of 5000\n",
      "Train Loss: 5.6767\n",
      "Epoch 3932 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 3933 of 5000\n",
      "Train Loss: 5.7291\n",
      "Epoch 3934 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 3935 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 3936 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 3937 of 5000\n",
      "Train Loss: 5.6775\n",
      "Epoch 3938 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 3939 of 5000\n",
      "Train Loss: 5.6854\n",
      "Epoch 3940 of 5000\n",
      "Train Loss: 5.7016\n",
      "Epoch 3941 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 3942 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 3943 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 3944 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 3945 of 5000\n",
      "Train Loss: 5.6767\n",
      "Epoch 3946 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 3947 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 3948 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 3949 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 3950 of 5000\n",
      "Train Loss: 5.6629\n",
      "Epoch 3951 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 3952 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 3953 of 5000\n",
      "Train Loss: 5.6869\n",
      "Epoch 3954 of 5000\n",
      "Train Loss: 5.7094\n",
      "Epoch 3955 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 3956 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 3957 of 5000\n",
      "Train Loss: 5.7134\n",
      "Epoch 3958 of 5000\n",
      "Train Loss: 5.7106\n",
      "Epoch 3959 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 3960 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 3961 of 5000\n",
      "Train Loss: 5.7242\n",
      "Epoch 3962 of 5000\n",
      "Train Loss: 5.6777\n",
      "Epoch 3963 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 3964 of 5000\n",
      "Train Loss: 5.6901\n",
      "Epoch 3965 of 5000\n",
      "Train Loss: 5.7141\n",
      "Epoch 3966 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 3967 of 5000\n",
      "Train Loss: 5.6813\n",
      "Epoch 3968 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 3969 of 5000\n",
      "Train Loss: 5.7004\n",
      "Epoch 3970 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 3971 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 3972 of 5000\n",
      "Train Loss: 5.6830\n",
      "Epoch 3973 of 5000\n",
      "Train Loss: 5.6702\n",
      "Epoch 3974 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 3975 of 5000\n",
      "Train Loss: 5.7191\n",
      "Epoch 3976 of 5000\n",
      "Train Loss: 5.6880\n",
      "Epoch 3977 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 3978 of 5000\n",
      "Train Loss: 5.6613\n",
      "Epoch 3979 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 3980 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 3981 of 5000\n",
      "Train Loss: 5.7013\n",
      "Epoch 3982 of 5000\n",
      "Train Loss: 5.6851\n",
      "Epoch 3983 of 5000\n",
      "Train Loss: 5.6852\n",
      "Epoch 3984 of 5000\n",
      "Train Loss: 5.7134\n",
      "Epoch 3985 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 3986 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 3987 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 3988 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 3989 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 3990 of 5000\n",
      "Train Loss: 5.6754\n",
      "Epoch 3991 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 3992 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 3993 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 3994 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 3995 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 3996 of 5000\n",
      "Train Loss: 5.7215\n",
      "Epoch 3997 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 3998 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 3999 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 4000 of 5000\n",
      "Train Loss: 5.7210\n",
      "Epoch 4001 of 5000\n",
      "Train Loss: 5.7020\n",
      "Epoch 4002 of 5000\n",
      "Train Loss: 5.7297\n",
      "Epoch 4003 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 4004 of 5000\n",
      "Train Loss: 5.6828\n",
      "Epoch 4005 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 4006 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 4007 of 5000\n",
      "Train Loss: 5.6858\n",
      "Epoch 4008 of 5000\n",
      "Train Loss: 5.7248\n",
      "Epoch 4009 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 4010 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4011 of 5000\n",
      "Train Loss: 5.6763\n",
      "Epoch 4012 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 4013 of 5000\n",
      "Train Loss: 5.6747\n",
      "Epoch 4014 of 5000\n",
      "Train Loss: 5.6785\n",
      "Epoch 4015 of 5000\n",
      "Train Loss: 5.6826\n",
      "Epoch 4016 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 4017 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 4018 of 5000\n",
      "Train Loss: 5.7136\n",
      "Epoch 4019 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 4020 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 4021 of 5000\n",
      "Train Loss: 5.6759\n",
      "Epoch 4022 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 4023 of 5000\n",
      "Train Loss: 5.7002\n",
      "Epoch 4024 of 5000\n",
      "Train Loss: 5.6774\n",
      "Epoch 4025 of 5000\n",
      "Train Loss: 5.6781\n",
      "Epoch 4026 of 5000\n",
      "Train Loss: 5.6825\n",
      "Epoch 4027 of 5000\n",
      "Train Loss: 5.6719\n",
      "Epoch 4028 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 4029 of 5000\n",
      "Train Loss: 5.6647\n",
      "Epoch 4030 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 4031 of 5000\n",
      "Train Loss: 5.6774\n",
      "Epoch 4032 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 4033 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 4034 of 5000\n",
      "Train Loss: 5.7132\n",
      "Epoch 4035 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 4036 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 4037 of 5000\n",
      "Train Loss: 5.7130\n",
      "Epoch 4038 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 4039 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 4040 of 5000\n",
      "Train Loss: 5.6569\n",
      "Epoch 4041 of 5000\n",
      "Train Loss: 5.7068\n",
      "Epoch 4042 of 5000\n",
      "Train Loss: 5.7140\n",
      "Epoch 4043 of 5000\n",
      "Train Loss: 5.6762\n",
      "Epoch 4044 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 4045 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 4046 of 5000\n",
      "Train Loss: 5.6948\n",
      "Epoch 4047 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 4048 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 4049 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 4050 of 5000\n",
      "Train Loss: 5.6905\n",
      "Epoch 4051 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 4052 of 5000\n",
      "Train Loss: 5.6828\n",
      "Epoch 4053 of 5000\n",
      "Train Loss: 5.7195\n",
      "Epoch 4054 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 4055 of 5000\n",
      "Train Loss: 5.6711\n",
      "Epoch 4056 of 5000\n",
      "Train Loss: 5.6646\n",
      "Epoch 4057 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 4058 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 4059 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 4060 of 5000\n",
      "Train Loss: 5.7131\n",
      "Epoch 4061 of 5000\n",
      "Train Loss: 5.7188\n",
      "Epoch 4062 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 4063 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 4064 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 4065 of 5000\n",
      "Train Loss: 5.7157\n",
      "Epoch 4066 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 4067 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 4068 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 4069 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 4070 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 4071 of 5000\n",
      "Train Loss: 5.6746\n",
      "Epoch 4072 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 4073 of 5000\n",
      "Train Loss: 5.6989\n",
      "Epoch 4074 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 4075 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 4076 of 5000\n",
      "Train Loss: 5.6783\n",
      "Epoch 4077 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 4078 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 4079 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 4080 of 5000\n",
      "Train Loss: 5.6818\n",
      "Epoch 4081 of 5000\n",
      "Train Loss: 5.7156\n",
      "Epoch 4082 of 5000\n",
      "Train Loss: 5.7021\n",
      "Epoch 4083 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 4084 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 4085 of 5000\n",
      "Train Loss: 5.6824\n",
      "Epoch 4086 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 4087 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 4088 of 5000\n",
      "Train Loss: 5.6767\n",
      "Epoch 4089 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 4090 of 5000\n",
      "Train Loss: 5.6771\n",
      "Epoch 4091 of 5000\n",
      "Train Loss: 5.6708\n",
      "Epoch 4092 of 5000\n",
      "Train Loss: 5.7136\n",
      "Epoch 4093 of 5000\n",
      "Train Loss: 5.6955\n",
      "Epoch 4094 of 5000\n",
      "Train Loss: 5.6862\n",
      "Epoch 4095 of 5000\n",
      "Train Loss: 5.6770\n",
      "Epoch 4096 of 5000\n",
      "Train Loss: 5.6752\n",
      "Epoch 4097 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 4098 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 4099 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 4100 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 4101 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 4102 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 4103 of 5000\n",
      "Train Loss: 5.7240\n",
      "Epoch 4104 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 4105 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 4106 of 5000\n",
      "Train Loss: 5.6766\n",
      "Epoch 4107 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 4108 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 4109 of 5000\n",
      "Train Loss: 5.6934\n",
      "Epoch 4110 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 4111 of 5000\n",
      "Train Loss: 5.7076\n",
      "Epoch 4112 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 4113 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 4114 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 4115 of 5000\n",
      "Train Loss: 5.6694\n",
      "Epoch 4116 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 4117 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 4118 of 5000\n",
      "Train Loss: 5.7150\n",
      "Epoch 4119 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 4120 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 4121 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 4122 of 5000\n",
      "Train Loss: 5.7018\n",
      "Epoch 4123 of 5000\n",
      "Train Loss: 5.7054\n",
      "Epoch 4124 of 5000\n",
      "Train Loss: 5.6814\n",
      "Epoch 4125 of 5000\n",
      "Train Loss: 5.7087\n",
      "Epoch 4126 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 4127 of 5000\n",
      "Train Loss: 5.6764\n",
      "Epoch 4128 of 5000\n",
      "Train Loss: 5.6791\n",
      "Epoch 4129 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 4130 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 4131 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 4132 of 5000\n",
      "Train Loss: 5.6717\n",
      "Epoch 4133 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 4134 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 4135 of 5000\n",
      "Train Loss: 5.6870\n",
      "Epoch 4136 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4137 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 4138 of 5000\n",
      "Train Loss: 5.6769\n",
      "Epoch 4139 of 5000\n",
      "Train Loss: 5.6732\n",
      "Epoch 4140 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 4141 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 4142 of 5000\n",
      "Train Loss: 5.6637\n",
      "Epoch 4143 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 4144 of 5000\n",
      "Train Loss: 5.7056\n",
      "Epoch 4145 of 5000\n",
      "Train Loss: 5.7063\n",
      "Epoch 4146 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 4147 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 4148 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 4149 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 4150 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 4151 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 4152 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 4153 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 4154 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 4155 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 4156 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 4157 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 4158 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 4159 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 4160 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 4161 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 4162 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 4163 of 5000\n",
      "Train Loss: 5.7305\n",
      "Epoch 4164 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 4165 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 4166 of 5000\n",
      "Train Loss: 5.6786\n",
      "Epoch 4167 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 4168 of 5000\n",
      "Train Loss: 5.6981\n",
      "Epoch 4169 of 5000\n",
      "Train Loss: 5.6820\n",
      "Epoch 4170 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 4171 of 5000\n",
      "Train Loss: 5.6786\n",
      "Epoch 4172 of 5000\n",
      "Train Loss: 5.7127\n",
      "Epoch 4173 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 4174 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 4175 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 4176 of 5000\n",
      "Train Loss: 5.7121\n",
      "Epoch 4177 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 4178 of 5000\n",
      "Train Loss: 5.7089\n",
      "Epoch 4179 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 4180 of 5000\n",
      "Train Loss: 5.7107\n",
      "Epoch 4181 of 5000\n",
      "Train Loss: 5.6963\n",
      "Epoch 4182 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 4183 of 5000\n",
      "Train Loss: 5.6737\n",
      "Epoch 4184 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 4185 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 4186 of 5000\n",
      "Train Loss: 5.7257\n",
      "Epoch 4187 of 5000\n",
      "Train Loss: 5.6731\n",
      "Epoch 4188 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 4189 of 5000\n",
      "Train Loss: 5.6948\n",
      "Epoch 4190 of 5000\n",
      "Train Loss: 5.6901\n",
      "Epoch 4191 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 4192 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 4193 of 5000\n",
      "Train Loss: 5.6870\n",
      "Epoch 4194 of 5000\n",
      "Train Loss: 5.7078\n",
      "Epoch 4195 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 4196 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 4197 of 5000\n",
      "Train Loss: 5.6832\n",
      "Epoch 4198 of 5000\n",
      "Train Loss: 5.6840\n",
      "Epoch 4199 of 5000\n",
      "Train Loss: 5.6863\n",
      "Epoch 4200 of 5000\n",
      "Train Loss: 5.7172\n",
      "Epoch 4201 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 4202 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 4203 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 4204 of 5000\n",
      "Train Loss: 5.6883\n",
      "Epoch 4205 of 5000\n",
      "Train Loss: 5.6839\n",
      "Epoch 4206 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 4207 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 4208 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 4209 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 4210 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 4211 of 5000\n",
      "Train Loss: 5.6765\n",
      "Epoch 4212 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 4213 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 4214 of 5000\n",
      "Train Loss: 5.7139\n",
      "Epoch 4215 of 5000\n",
      "Train Loss: 5.6655\n",
      "Epoch 4216 of 5000\n",
      "Train Loss: 5.6632\n",
      "Epoch 4217 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 4218 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 4219 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 4220 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 4221 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 4222 of 5000\n",
      "Train Loss: 5.6998\n",
      "Epoch 4223 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 4224 of 5000\n",
      "Train Loss: 5.6665\n",
      "Epoch 4225 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 4226 of 5000\n",
      "Train Loss: 5.6887\n",
      "Epoch 4227 of 5000\n",
      "Train Loss: 5.7179\n",
      "Epoch 4228 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 4229 of 5000\n",
      "Train Loss: 5.6843\n",
      "Epoch 4230 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 4231 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 4232 of 5000\n",
      "Train Loss: 5.6778\n",
      "Epoch 4233 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 4234 of 5000\n",
      "Train Loss: 5.7101\n",
      "Epoch 4235 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 4236 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 4237 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 4238 of 5000\n",
      "Train Loss: 5.7080\n",
      "Epoch 4239 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 4240 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 4241 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 4242 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 4243 of 5000\n",
      "Train Loss: 5.6787\n",
      "Epoch 4244 of 5000\n",
      "Train Loss: 5.7053\n",
      "Epoch 4245 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 4246 of 5000\n",
      "Train Loss: 5.7039\n",
      "Epoch 4247 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 4248 of 5000\n",
      "Train Loss: 5.7009\n",
      "Epoch 4249 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 4250 of 5000\n",
      "Train Loss: 5.6753\n",
      "Epoch 4251 of 5000\n",
      "Train Loss: 5.6824\n",
      "Epoch 4252 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 4253 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 4254 of 5000\n",
      "Train Loss: 5.7019\n",
      "Epoch 4255 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 4256 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 4257 of 5000\n",
      "Train Loss: 5.6830\n",
      "Epoch 4258 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 4259 of 5000\n",
      "Train Loss: 5.6940\n",
      "Epoch 4260 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 4261 of 5000\n",
      "Train Loss: 5.7149\n",
      "Epoch 4262 of 5000\n",
      "Train Loss: 5.6982\n",
      "Epoch 4263 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 4264 of 5000\n",
      "Train Loss: 5.6804\n",
      "Epoch 4265 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 4266 of 5000\n",
      "Train Loss: 5.6748\n",
      "Epoch 4267 of 5000\n",
      "Train Loss: 5.7138\n",
      "Epoch 4268 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 4269 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 4270 of 5000\n",
      "Train Loss: 5.6770\n",
      "Epoch 4271 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 4272 of 5000\n",
      "Train Loss: 5.7058\n",
      "Epoch 4273 of 5000\n",
      "Train Loss: 5.6950\n",
      "Epoch 4274 of 5000\n",
      "Train Loss: 5.6724\n",
      "Epoch 4275 of 5000\n",
      "Train Loss: 5.6697\n",
      "Epoch 4276 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 4277 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 4278 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 4279 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 4280 of 5000\n",
      "Train Loss: 5.6784\n",
      "Epoch 4281 of 5000\n",
      "Train Loss: 5.6870\n",
      "Epoch 4282 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 4283 of 5000\n",
      "Train Loss: 5.7352\n",
      "Epoch 4284 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 4285 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 4286 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 4287 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 4288 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 4289 of 5000\n",
      "Train Loss: 5.6901\n",
      "Epoch 4290 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 4291 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 4292 of 5000\n",
      "Train Loss: 5.6851\n",
      "Epoch 4293 of 5000\n",
      "Train Loss: 5.6992\n",
      "Epoch 4294 of 5000\n",
      "Train Loss: 5.7120\n",
      "Epoch 4295 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 4296 of 5000\n",
      "Train Loss: 5.6805\n",
      "Epoch 4297 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 4298 of 5000\n",
      "Train Loss: 5.7174\n",
      "Epoch 4299 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 4300 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4301 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 4302 of 5000\n",
      "Train Loss: 5.6700\n",
      "Epoch 4303 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 4304 of 5000\n",
      "Train Loss: 5.6811\n",
      "Epoch 4305 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 4306 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 4307 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 4308 of 5000\n",
      "Train Loss: 5.7119\n",
      "Epoch 4309 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 4310 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 4311 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 4312 of 5000\n",
      "Train Loss: 5.6794\n",
      "Epoch 4313 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 4314 of 5000\n",
      "Train Loss: 5.6944\n",
      "Epoch 4315 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 4316 of 5000\n",
      "Train Loss: 5.7166\n",
      "Epoch 4317 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 4318 of 5000\n",
      "Train Loss: 5.6689\n",
      "Epoch 4319 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 4320 of 5000\n",
      "Train Loss: 5.6990\n",
      "Epoch 4321 of 5000\n",
      "Train Loss: 5.6576\n",
      "Epoch 4322 of 5000\n",
      "Train Loss: 5.6778\n",
      "Epoch 4323 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 4324 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 4325 of 5000\n",
      "Train Loss: 5.6930\n",
      "Epoch 4326 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 4327 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 4328 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 4329 of 5000\n",
      "Train Loss: 5.7220\n",
      "Epoch 4330 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 4331 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4332 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 4333 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 4334 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 4335 of 5000\n",
      "Train Loss: 5.6853\n",
      "Epoch 4336 of 5000\n",
      "Train Loss: 5.7145\n",
      "Epoch 4337 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 4338 of 5000\n",
      "Train Loss: 5.7080\n",
      "Epoch 4339 of 5000\n",
      "Train Loss: 5.7009\n",
      "Epoch 4340 of 5000\n",
      "Train Loss: 5.7041\n",
      "Epoch 4341 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 4342 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 4343 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 4344 of 5000\n",
      "Train Loss: 5.7058\n",
      "Epoch 4345 of 5000\n",
      "Train Loss: 5.6765\n",
      "Epoch 4346 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 4347 of 5000\n",
      "Train Loss: 5.7093\n",
      "Epoch 4348 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 4349 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 4350 of 5000\n",
      "Train Loss: 5.6903\n",
      "Epoch 4351 of 5000\n",
      "Train Loss: 5.7097\n",
      "Epoch 4352 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 4353 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 4354 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 4355 of 5000\n",
      "Train Loss: 5.6875\n",
      "Epoch 4356 of 5000\n",
      "Train Loss: 5.7168\n",
      "Epoch 4357 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 4358 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 4359 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 4360 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 4361 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 4362 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 4363 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 4364 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 4365 of 5000\n",
      "Train Loss: 5.7178\n",
      "Epoch 4366 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 4367 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4368 of 5000\n",
      "Train Loss: 5.6774\n",
      "Epoch 4369 of 5000\n",
      "Train Loss: 5.6652\n",
      "Epoch 4370 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 4371 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 4372 of 5000\n",
      "Train Loss: 5.6892\n",
      "Epoch 4373 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 4374 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4375 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 4376 of 5000\n",
      "Train Loss: 5.6628\n",
      "Epoch 4377 of 5000\n",
      "Train Loss: 5.6937\n",
      "Epoch 4378 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 4379 of 5000\n",
      "Train Loss: 5.6805\n",
      "Epoch 4380 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 4381 of 5000\n",
      "Train Loss: 5.6804\n",
      "Epoch 4382 of 5000\n",
      "Train Loss: 5.7184\n",
      "Epoch 4383 of 5000\n",
      "Train Loss: 5.7074\n",
      "Epoch 4384 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 4385 of 5000\n",
      "Train Loss: 5.6629\n",
      "Epoch 4386 of 5000\n",
      "Train Loss: 5.7118\n",
      "Epoch 4387 of 5000\n",
      "Train Loss: 5.7060\n",
      "Epoch 4388 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 4389 of 5000\n",
      "Train Loss: 5.6716\n",
      "Epoch 4390 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 4391 of 5000\n",
      "Train Loss: 5.6757\n",
      "Epoch 4392 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 4393 of 5000\n",
      "Train Loss: 5.6742\n",
      "Epoch 4394 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4395 of 5000\n",
      "Train Loss: 5.6647\n",
      "Epoch 4396 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 4397 of 5000\n",
      "Train Loss: 5.6814\n",
      "Epoch 4398 of 5000\n",
      "Train Loss: 5.6758\n",
      "Epoch 4399 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 4400 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 4401 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 4402 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 4403 of 5000\n",
      "Train Loss: 5.7000\n",
      "Epoch 4404 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 4405 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 4406 of 5000\n",
      "Train Loss: 5.6911\n",
      "Epoch 4407 of 5000\n",
      "Train Loss: 5.7062\n",
      "Epoch 4408 of 5000\n",
      "Train Loss: 5.6726\n",
      "Epoch 4409 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 4410 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 4411 of 5000\n",
      "Train Loss: 5.6462\n",
      "Epoch 4412 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 4413 of 5000\n",
      "Train Loss: 5.6768\n",
      "Epoch 4414 of 5000\n",
      "Train Loss: 5.7193\n",
      "Epoch 4415 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 4416 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 4417 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4418 of 5000\n",
      "Train Loss: 5.7144\n",
      "Epoch 4419 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 4420 of 5000\n",
      "Train Loss: 5.7059\n",
      "Epoch 4421 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 4422 of 5000\n",
      "Train Loss: 5.6679\n",
      "Epoch 4423 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 4424 of 5000\n",
      "Train Loss: 5.6707\n",
      "Epoch 4425 of 5000\n",
      "Train Loss: 5.6835\n",
      "Epoch 4426 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 4427 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 4428 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4429 of 5000\n",
      "Train Loss: 5.6774\n",
      "Epoch 4430 of 5000\n",
      "Train Loss: 5.6810\n",
      "Epoch 4431 of 5000\n",
      "Train Loss: 5.6815\n",
      "Epoch 4432 of 5000\n",
      "Train Loss: 5.7116\n",
      "Epoch 4433 of 5000\n",
      "Train Loss: 5.6735\n",
      "Epoch 4434 of 5000\n",
      "Train Loss: 5.6679\n",
      "Epoch 4435 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 4436 of 5000\n",
      "Train Loss: 5.7049\n",
      "Epoch 4437 of 5000\n",
      "Train Loss: 5.6742\n",
      "Epoch 4438 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 4439 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 4440 of 5000\n",
      "Train Loss: 5.7267\n",
      "Epoch 4441 of 5000\n",
      "Train Loss: 5.6772\n",
      "Epoch 4442 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 4443 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 4444 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 4445 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 4446 of 5000\n",
      "Train Loss: 5.6811\n",
      "Epoch 4447 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 4448 of 5000\n",
      "Train Loss: 5.6906\n",
      "Epoch 4449 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 4450 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 4451 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 4452 of 5000\n",
      "Train Loss: 5.6951\n",
      "Epoch 4453 of 5000\n",
      "Train Loss: 5.6939\n",
      "Epoch 4454 of 5000\n",
      "Train Loss: 5.6770\n",
      "Epoch 4455 of 5000\n",
      "Train Loss: 5.6762\n",
      "Epoch 4456 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 4457 of 5000\n",
      "Train Loss: 5.6938\n",
      "Epoch 4458 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 4459 of 5000\n",
      "Train Loss: 5.6929\n",
      "Epoch 4460 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 4461 of 5000\n",
      "Train Loss: 5.6931\n",
      "Epoch 4462 of 5000\n",
      "Train Loss: 5.7025\n",
      "Epoch 4463 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 4464 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 4465 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 4466 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 4467 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 4468 of 5000\n",
      "Train Loss: 5.6887\n",
      "Epoch 4469 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 4470 of 5000\n",
      "Train Loss: 5.7048\n",
      "Epoch 4471 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 4472 of 5000\n",
      "Train Loss: 5.6987\n",
      "Epoch 4473 of 5000\n",
      "Train Loss: 5.6644\n",
      "Epoch 4474 of 5000\n",
      "Train Loss: 5.7086\n",
      "Epoch 4475 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 4476 of 5000\n",
      "Train Loss: 5.7153\n",
      "Epoch 4477 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 4478 of 5000\n",
      "Train Loss: 5.7027\n",
      "Epoch 4479 of 5000\n",
      "Train Loss: 5.6956\n",
      "Epoch 4480 of 5000\n",
      "Train Loss: 5.6943\n",
      "Epoch 4481 of 5000\n",
      "Train Loss: 5.7076\n",
      "Epoch 4482 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4483 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 4484 of 5000\n",
      "Train Loss: 5.6876\n",
      "Epoch 4485 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 4486 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4487 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 4488 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 4489 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 4490 of 5000\n",
      "Train Loss: 5.6826\n",
      "Epoch 4491 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 4492 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 4493 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 4494 of 5000\n",
      "Train Loss: 5.6837\n",
      "Epoch 4495 of 5000\n",
      "Train Loss: 5.6860\n",
      "Epoch 4496 of 5000\n",
      "Train Loss: 5.6852\n",
      "Epoch 4497 of 5000\n",
      "Train Loss: 5.6784\n",
      "Epoch 4498 of 5000\n",
      "Train Loss: 5.6862\n",
      "Epoch 4499 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 4500 of 5000\n",
      "Train Loss: 5.6887\n",
      "Epoch 4501 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 4502 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 4503 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 4504 of 5000\n",
      "Train Loss: 5.6738\n",
      "Epoch 4505 of 5000\n",
      "Train Loss: 5.6873\n",
      "Epoch 4506 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 4507 of 5000\n",
      "Train Loss: 5.7137\n",
      "Epoch 4508 of 5000\n",
      "Train Loss: 5.6606\n",
      "Epoch 4509 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 4510 of 5000\n",
      "Train Loss: 5.7075\n",
      "Epoch 4511 of 5000\n",
      "Train Loss: 5.6999\n",
      "Epoch 4512 of 5000\n",
      "Train Loss: 5.6818\n",
      "Epoch 4513 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 4514 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 4515 of 5000\n",
      "Train Loss: 5.6682\n",
      "Epoch 4516 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 4517 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 4518 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 4519 of 5000\n",
      "Train Loss: 5.6989\n",
      "Epoch 4520 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 4521 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 4522 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 4523 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 4524 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 4525 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 4526 of 5000\n",
      "Train Loss: 5.7042\n",
      "Epoch 4527 of 5000\n",
      "Train Loss: 5.6872\n",
      "Epoch 4528 of 5000\n",
      "Train Loss: 5.6751\n",
      "Epoch 4529 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 4530 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 4531 of 5000\n",
      "Train Loss: 5.7067\n",
      "Epoch 4532 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 4533 of 5000\n",
      "Train Loss: 5.7026\n",
      "Epoch 4534 of 5000\n",
      "Train Loss: 5.7097\n",
      "Epoch 4535 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 4536 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 4537 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 4538 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 4539 of 5000\n",
      "Train Loss: 5.7058\n",
      "Epoch 4540 of 5000\n",
      "Train Loss: 5.7001\n",
      "Epoch 4541 of 5000\n",
      "Train Loss: 5.6892\n",
      "Epoch 4542 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 4543 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 4544 of 5000\n",
      "Train Loss: 5.6810\n",
      "Epoch 4545 of 5000\n",
      "Train Loss: 5.6891\n",
      "Epoch 4546 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 4547 of 5000\n",
      "Train Loss: 5.7113\n",
      "Epoch 4548 of 5000\n",
      "Train Loss: 5.6798\n",
      "Epoch 4549 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 4550 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 4551 of 5000\n",
      "Train Loss: 5.7094\n",
      "Epoch 4552 of 5000\n",
      "Train Loss: 5.6853\n",
      "Epoch 4553 of 5000\n",
      "Train Loss: 5.7010\n",
      "Epoch 4554 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 4555 of 5000\n",
      "Train Loss: 5.7012\n",
      "Epoch 4556 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 4557 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4558 of 5000\n",
      "Train Loss: 5.6965\n",
      "Epoch 4559 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 4560 of 5000\n",
      "Train Loss: 5.7019\n",
      "Epoch 4561 of 5000\n",
      "Train Loss: 5.6839\n",
      "Epoch 4562 of 5000\n",
      "Train Loss: 5.6896\n",
      "Epoch 4563 of 5000\n",
      "Train Loss: 5.6865\n",
      "Epoch 4564 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 4565 of 5000\n",
      "Train Loss: 5.6814\n",
      "Epoch 4566 of 5000\n",
      "Train Loss: 5.7250\n",
      "Epoch 4567 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 4568 of 5000\n",
      "Train Loss: 5.6863\n",
      "Epoch 4569 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 4570 of 5000\n",
      "Train Loss: 5.7227\n",
      "Epoch 4571 of 5000\n",
      "Train Loss: 5.6744\n",
      "Epoch 4572 of 5000\n",
      "Train Loss: 5.6855\n",
      "Epoch 4573 of 5000\n",
      "Train Loss: 5.6834\n",
      "Epoch 4574 of 5000\n",
      "Train Loss: 5.6998\n",
      "Epoch 4575 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 4576 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 4577 of 5000\n",
      "Train Loss: 5.6830\n",
      "Epoch 4578 of 5000\n",
      "Train Loss: 5.7017\n",
      "Epoch 4579 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 4580 of 5000\n",
      "Train Loss: 5.6861\n",
      "Epoch 4581 of 5000\n",
      "Train Loss: 5.6831\n",
      "Epoch 4582 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 4583 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 4584 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 4585 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 4586 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 4587 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 4588 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4589 of 5000\n",
      "Train Loss: 5.6743\n",
      "Epoch 4590 of 5000\n",
      "Train Loss: 5.6747\n",
      "Epoch 4591 of 5000\n",
      "Train Loss: 5.6869\n",
      "Epoch 4592 of 5000\n",
      "Train Loss: 5.6998\n",
      "Epoch 4593 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 4594 of 5000\n",
      "Train Loss: 5.6987\n",
      "Epoch 4595 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 4596 of 5000\n",
      "Train Loss: 5.6972\n",
      "Epoch 4597 of 5000\n",
      "Train Loss: 5.7083\n",
      "Epoch 4598 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 4599 of 5000\n",
      "Train Loss: 5.6871\n",
      "Epoch 4600 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 4601 of 5000\n",
      "Train Loss: 5.6866\n",
      "Epoch 4602 of 5000\n",
      "Train Loss: 5.6814\n",
      "Epoch 4603 of 5000\n",
      "Train Loss: 5.7124\n",
      "Epoch 4604 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 4605 of 5000\n",
      "Train Loss: 5.6977\n",
      "Epoch 4606 of 5000\n",
      "Train Loss: 5.6819\n",
      "Epoch 4607 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 4608 of 5000\n",
      "Train Loss: 5.6887\n",
      "Epoch 4609 of 5000\n",
      "Train Loss: 5.6954\n",
      "Epoch 4610 of 5000\n",
      "Train Loss: 5.7143\n",
      "Epoch 4611 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 4612 of 5000\n",
      "Train Loss: 5.6902\n",
      "Epoch 4613 of 5000\n",
      "Train Loss: 5.7019\n",
      "Epoch 4614 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 4615 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 4616 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4617 of 5000\n",
      "Train Loss: 5.6563\n",
      "Epoch 4618 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 4619 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 4620 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 4621 of 5000\n",
      "Train Loss: 5.6900\n",
      "Epoch 4622 of 5000\n",
      "Train Loss: 5.6740\n",
      "Epoch 4623 of 5000\n",
      "Train Loss: 5.6971\n",
      "Epoch 4624 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 4625 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 4626 of 5000\n",
      "Train Loss: 5.7022\n",
      "Epoch 4627 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 4628 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 4629 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 4630 of 5000\n",
      "Train Loss: 5.7033\n",
      "Epoch 4631 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 4632 of 5000\n",
      "Train Loss: 5.7153\n",
      "Epoch 4633 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 4634 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 4635 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 4636 of 5000\n",
      "Train Loss: 5.6788\n",
      "Epoch 4637 of 5000\n",
      "Train Loss: 5.6789\n",
      "Epoch 4638 of 5000\n",
      "Train Loss: 5.6869\n",
      "Epoch 4639 of 5000\n",
      "Train Loss: 5.7170\n",
      "Epoch 4640 of 5000\n",
      "Train Loss: 5.7142\n",
      "Epoch 4641 of 5000\n",
      "Train Loss: 5.6912\n",
      "Epoch 4642 of 5000\n",
      "Train Loss: 5.6644\n",
      "Epoch 4643 of 5000\n",
      "Train Loss: 5.6893\n",
      "Epoch 4644 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 4645 of 5000\n",
      "Train Loss: 5.6775\n",
      "Epoch 4646 of 5000\n",
      "Train Loss: 5.6783\n",
      "Epoch 4647 of 5000\n",
      "Train Loss: 5.6729\n",
      "Epoch 4648 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 4649 of 5000\n",
      "Train Loss: 5.6908\n",
      "Epoch 4650 of 5000\n",
      "Train Loss: 5.7119\n",
      "Epoch 4651 of 5000\n",
      "Train Loss: 5.6844\n",
      "Epoch 4652 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 4653 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 4654 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 4655 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 4656 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 4657 of 5000\n",
      "Train Loss: 5.6840\n",
      "Epoch 4658 of 5000\n",
      "Train Loss: 5.6772\n",
      "Epoch 4659 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 4660 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 4661 of 5000\n",
      "Train Loss: 5.6964\n",
      "Epoch 4662 of 5000\n",
      "Train Loss: 5.7032\n",
      "Epoch 4663 of 5000\n",
      "Train Loss: 5.6975\n",
      "Epoch 4664 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 4665 of 5000\n",
      "Train Loss: 5.6766\n",
      "Epoch 4666 of 5000\n",
      "Train Loss: 5.7146\n",
      "Epoch 4667 of 5000\n",
      "Train Loss: 5.6832\n",
      "Epoch 4668 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 4669 of 5000\n",
      "Train Loss: 5.6768\n",
      "Epoch 4670 of 5000\n",
      "Train Loss: 5.7097\n",
      "Epoch 4671 of 5000\n",
      "Train Loss: 5.6756\n",
      "Epoch 4672 of 5000\n",
      "Train Loss: 5.7009\n",
      "Epoch 4673 of 5000\n",
      "Train Loss: 5.6845\n",
      "Epoch 4674 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 4675 of 5000\n",
      "Train Loss: 5.6568\n",
      "Epoch 4676 of 5000\n",
      "Train Loss: 5.6942\n",
      "Epoch 4677 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 4678 of 5000\n",
      "Train Loss: 5.6993\n",
      "Epoch 4679 of 5000\n",
      "Train Loss: 5.6985\n",
      "Epoch 4680 of 5000\n",
      "Train Loss: 5.6842\n",
      "Epoch 4681 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 4682 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 4683 of 5000\n",
      "Train Loss: 5.6778\n",
      "Epoch 4684 of 5000\n",
      "Train Loss: 5.6752\n",
      "Epoch 4685 of 5000\n",
      "Train Loss: 5.7061\n",
      "Epoch 4686 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 4687 of 5000\n",
      "Train Loss: 5.6751\n",
      "Epoch 4688 of 5000\n",
      "Train Loss: 5.6988\n",
      "Epoch 4689 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 4690 of 5000\n",
      "Train Loss: 5.6962\n",
      "Epoch 4691 of 5000\n",
      "Train Loss: 5.7052\n",
      "Epoch 4692 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 4693 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 4694 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 4695 of 5000\n",
      "Train Loss: 5.7135\n",
      "Epoch 4696 of 5000\n",
      "Train Loss: 5.7018\n",
      "Epoch 4697 of 5000\n",
      "Train Loss: 5.6979\n",
      "Epoch 4698 of 5000\n",
      "Train Loss: 5.6889\n",
      "Epoch 4699 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 4700 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4701 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 4702 of 5000\n",
      "Train Loss: 5.7065\n",
      "Epoch 4703 of 5000\n",
      "Train Loss: 5.6888\n",
      "Epoch 4704 of 5000\n",
      "Train Loss: 5.6807\n",
      "Epoch 4705 of 5000\n",
      "Train Loss: 5.6791\n",
      "Epoch 4706 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 4707 of 5000\n",
      "Train Loss: 5.6989\n",
      "Epoch 4708 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 4709 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 4710 of 5000\n",
      "Train Loss: 5.7072\n",
      "Epoch 4711 of 5000\n",
      "Train Loss: 5.6732\n",
      "Epoch 4712 of 5000\n",
      "Train Loss: 5.6856\n",
      "Epoch 4713 of 5000\n",
      "Train Loss: 5.6801\n",
      "Epoch 4714 of 5000\n",
      "Train Loss: 5.6895\n",
      "Epoch 4715 of 5000\n",
      "Train Loss: 5.7044\n",
      "Epoch 4716 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 4717 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 4718 of 5000\n",
      "Train Loss: 5.6731\n",
      "Epoch 4719 of 5000\n",
      "Train Loss: 5.6672\n",
      "Epoch 4720 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 4721 of 5000\n",
      "Train Loss: 5.7035\n",
      "Epoch 4722 of 5000\n",
      "Train Loss: 5.6738\n",
      "Epoch 4723 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 4724 of 5000\n",
      "Train Loss: 5.6914\n",
      "Epoch 4725 of 5000\n",
      "Train Loss: 5.6947\n",
      "Epoch 4726 of 5000\n",
      "Train Loss: 5.6923\n",
      "Epoch 4727 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 4728 of 5000\n",
      "Train Loss: 5.7151\n",
      "Epoch 4729 of 5000\n",
      "Train Loss: 5.7028\n",
      "Epoch 4730 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 4731 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 4732 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 4733 of 5000\n",
      "Train Loss: 5.6840\n",
      "Epoch 4734 of 5000\n",
      "Train Loss: 5.6720\n",
      "Epoch 4735 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 4736 of 5000\n",
      "Train Loss: 5.6957\n",
      "Epoch 4737 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 4738 of 5000\n",
      "Train Loss: 5.6764\n",
      "Epoch 4739 of 5000\n",
      "Train Loss: 5.6627\n",
      "Epoch 4740 of 5000\n",
      "Train Loss: 5.6917\n",
      "Epoch 4741 of 5000\n",
      "Train Loss: 5.6894\n",
      "Epoch 4742 of 5000\n",
      "Train Loss: 5.6844\n",
      "Epoch 4743 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 4744 of 5000\n",
      "Train Loss: 5.6867\n",
      "Epoch 4745 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 4746 of 5000\n",
      "Train Loss: 5.6968\n",
      "Epoch 4747 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 4748 of 5000\n",
      "Train Loss: 5.7062\n",
      "Epoch 4749 of 5000\n",
      "Train Loss: 5.7148\n",
      "Epoch 4750 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 4751 of 5000\n",
      "Train Loss: 5.6795\n",
      "Epoch 4752 of 5000\n",
      "Train Loss: 5.6869\n",
      "Epoch 4753 of 5000\n",
      "Train Loss: 5.6932\n",
      "Epoch 4754 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 4755 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 4756 of 5000\n",
      "Train Loss: 5.6879\n",
      "Epoch 4757 of 5000\n",
      "Train Loss: 5.6946\n",
      "Epoch 4758 of 5000\n",
      "Train Loss: 5.7084\n",
      "Epoch 4759 of 5000\n",
      "Train Loss: 5.6757\n",
      "Epoch 4760 of 5000\n",
      "Train Loss: 5.7040\n",
      "Epoch 4761 of 5000\n",
      "Train Loss: 5.6825\n",
      "Epoch 4762 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 4763 of 5000\n",
      "Train Loss: 5.6959\n",
      "Epoch 4764 of 5000\n",
      "Train Loss: 5.7107\n",
      "Epoch 4765 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 4766 of 5000\n",
      "Train Loss: 5.6983\n",
      "Epoch 4767 of 5000\n",
      "Train Loss: 5.6857\n",
      "Epoch 4768 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 4769 of 5000\n",
      "Train Loss: 5.6770\n",
      "Epoch 4770 of 5000\n",
      "Train Loss: 5.7059\n",
      "Epoch 4771 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 4772 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 4773 of 5000\n",
      "Train Loss: 5.7168\n",
      "Epoch 4774 of 5000\n",
      "Train Loss: 5.6928\n",
      "Epoch 4775 of 5000\n",
      "Train Loss: 5.6919\n",
      "Epoch 4776 of 5000\n",
      "Train Loss: 5.6881\n",
      "Epoch 4777 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 4778 of 5000\n",
      "Train Loss: 5.6733\n",
      "Epoch 4779 of 5000\n",
      "Train Loss: 5.6633\n",
      "Epoch 4780 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 4781 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 4782 of 5000\n",
      "Train Loss: 5.6736\n",
      "Epoch 4783 of 5000\n",
      "Train Loss: 5.7088\n",
      "Epoch 4784 of 5000\n",
      "Train Loss: 5.6787\n",
      "Epoch 4785 of 5000\n",
      "Train Loss: 5.6960\n",
      "Epoch 4786 of 5000\n",
      "Train Loss: 5.6872\n",
      "Epoch 4787 of 5000\n",
      "Train Loss: 5.6916\n",
      "Epoch 4788 of 5000\n",
      "Train Loss: 5.7096\n",
      "Epoch 4789 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 4790 of 5000\n",
      "Train Loss: 5.6927\n",
      "Epoch 4791 of 5000\n",
      "Train Loss: 5.7031\n",
      "Epoch 4792 of 5000\n",
      "Train Loss: 5.6724\n",
      "Epoch 4793 of 5000\n",
      "Train Loss: 5.7181\n",
      "Epoch 4794 of 5000\n",
      "Train Loss: 5.6817\n",
      "Epoch 4795 of 5000\n",
      "Train Loss: 5.6926\n",
      "Epoch 4796 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 4797 of 5000\n",
      "Train Loss: 5.6822\n",
      "Epoch 4798 of 5000\n",
      "Train Loss: 5.7131\n",
      "Epoch 4799 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 4800 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 4801 of 5000\n",
      "Train Loss: 5.7007\n",
      "Epoch 4802 of 5000\n",
      "Train Loss: 5.7008\n",
      "Epoch 4803 of 5000\n",
      "Train Loss: 5.7057\n",
      "Epoch 4804 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 4805 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 4806 of 5000\n",
      "Train Loss: 5.6967\n",
      "Epoch 4807 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 4808 of 5000\n",
      "Train Loss: 5.6833\n",
      "Epoch 4809 of 5000\n",
      "Train Loss: 5.6775\n",
      "Epoch 4810 of 5000\n",
      "Train Loss: 5.6625\n",
      "Epoch 4811 of 5000\n",
      "Train Loss: 5.6829\n",
      "Epoch 4812 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 4813 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 4814 of 5000\n",
      "Train Loss: 5.6620\n",
      "Epoch 4815 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 4816 of 5000\n",
      "Train Loss: 5.6764\n",
      "Epoch 4817 of 5000\n",
      "Train Loss: 5.6838\n",
      "Epoch 4818 of 5000\n",
      "Train Loss: 5.6882\n",
      "Epoch 4819 of 5000\n",
      "Train Loss: 5.6925\n",
      "Epoch 4820 of 5000\n",
      "Train Loss: 5.6828\n",
      "Epoch 4821 of 5000\n",
      "Train Loss: 5.6980\n",
      "Epoch 4822 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 4823 of 5000\n",
      "Train Loss: 5.7085\n",
      "Epoch 4824 of 5000\n",
      "Train Loss: 5.6798\n",
      "Epoch 4825 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 4826 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 4827 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 4828 of 5000\n",
      "Train Loss: 5.6995\n",
      "Epoch 4829 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 4830 of 5000\n",
      "Train Loss: 5.6752\n",
      "Epoch 4831 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 4832 of 5000\n",
      "Train Loss: 5.6823\n",
      "Epoch 4833 of 5000\n",
      "Train Loss: 5.6784\n",
      "Epoch 4834 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 4835 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 4836 of 5000\n",
      "Train Loss: 5.6933\n",
      "Epoch 4837 of 5000\n",
      "Train Loss: 5.6769\n",
      "Epoch 4838 of 5000\n",
      "Train Loss: 5.7153\n",
      "Epoch 4839 of 5000\n",
      "Train Loss: 5.6887\n",
      "Epoch 4840 of 5000\n",
      "Train Loss: 5.6693\n",
      "Epoch 4841 of 5000\n",
      "Train Loss: 5.7091\n",
      "Epoch 4842 of 5000\n",
      "Train Loss: 5.7217\n",
      "Epoch 4843 of 5000\n",
      "Train Loss: 5.6758\n",
      "Epoch 4844 of 5000\n",
      "Train Loss: 5.6758\n",
      "Epoch 4845 of 5000\n",
      "Train Loss: 5.6776\n",
      "Epoch 4846 of 5000\n",
      "Train Loss: 5.6952\n",
      "Epoch 4847 of 5000\n",
      "Train Loss: 5.6920\n",
      "Epoch 4848 of 5000\n",
      "Train Loss: 5.6740\n",
      "Epoch 4849 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 4850 of 5000\n",
      "Train Loss: 5.7106\n",
      "Epoch 4851 of 5000\n",
      "Train Loss: 5.6859\n",
      "Epoch 4852 of 5000\n",
      "Train Loss: 5.6708\n",
      "Epoch 4853 of 5000\n",
      "Train Loss: 5.6966\n",
      "Epoch 4854 of 5000\n",
      "Train Loss: 5.6877\n",
      "Epoch 4855 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 4856 of 5000\n",
      "Train Loss: 5.6904\n",
      "Epoch 4857 of 5000\n",
      "Train Loss: 5.6852\n",
      "Epoch 4858 of 5000\n",
      "Train Loss: 5.6797\n",
      "Epoch 4859 of 5000\n",
      "Train Loss: 5.6848\n",
      "Epoch 4860 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 4861 of 5000\n",
      "Train Loss: 5.6899\n",
      "Epoch 4862 of 5000\n",
      "Train Loss: 5.6915\n",
      "Epoch 4863 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 4864 of 5000\n",
      "Train Loss: 5.6984\n",
      "Epoch 4865 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 4866 of 5000\n",
      "Train Loss: 5.6782\n",
      "Epoch 4867 of 5000\n",
      "Train Loss: 5.6830\n",
      "Epoch 4868 of 5000\n",
      "Train Loss: 5.6796\n",
      "Epoch 4869 of 5000\n",
      "Train Loss: 5.6909\n",
      "Epoch 4870 of 5000\n",
      "Train Loss: 5.6970\n",
      "Epoch 4871 of 5000\n",
      "Train Loss: 5.7037\n",
      "Epoch 4872 of 5000\n",
      "Train Loss: 5.6991\n",
      "Epoch 4873 of 5000\n",
      "Train Loss: 5.7069\n",
      "Epoch 4874 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 4875 of 5000\n",
      "Train Loss: 5.6832\n",
      "Epoch 4876 of 5000\n",
      "Train Loss: 5.6892\n",
      "Epoch 4877 of 5000\n",
      "Train Loss: 5.7050\n",
      "Epoch 4878 of 5000\n",
      "Train Loss: 5.6786\n",
      "Epoch 4879 of 5000\n",
      "Train Loss: 5.6711\n",
      "Epoch 4880 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 4881 of 5000\n",
      "Train Loss: 5.7126\n",
      "Epoch 4882 of 5000\n",
      "Train Loss: 5.6897\n",
      "Epoch 4883 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 4884 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 4885 of 5000\n",
      "Train Loss: 5.7014\n",
      "Epoch 4886 of 5000\n",
      "Train Loss: 5.6913\n",
      "Epoch 4887 of 5000\n",
      "Train Loss: 5.7064\n",
      "Epoch 4888 of 5000\n",
      "Train Loss: 5.6802\n",
      "Epoch 4889 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 4890 of 5000\n",
      "Train Loss: 5.6865\n",
      "Epoch 4891 of 5000\n",
      "Train Loss: 5.7023\n",
      "Epoch 4892 of 5000\n",
      "Train Loss: 5.6949\n",
      "Epoch 4893 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 4894 of 5000\n",
      "Train Loss: 5.6853\n",
      "Epoch 4895 of 5000\n",
      "Train Loss: 5.6872\n",
      "Epoch 4896 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4897 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 4898 of 5000\n",
      "Train Loss: 5.6800\n",
      "Epoch 4899 of 5000\n",
      "Train Loss: 5.7081\n",
      "Epoch 4900 of 5000\n",
      "Train Loss: 5.6974\n",
      "Epoch 4901 of 5000\n",
      "Train Loss: 5.6910\n",
      "Epoch 4902 of 5000\n",
      "Train Loss: 5.6750\n",
      "Epoch 4903 of 5000\n",
      "Train Loss: 5.6842\n",
      "Epoch 4904 of 5000\n",
      "Train Loss: 5.6821\n",
      "Epoch 4905 of 5000\n",
      "Train Loss: 5.6907\n",
      "Epoch 4906 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 4907 of 5000\n",
      "Train Loss: 5.6818\n",
      "Epoch 4908 of 5000\n",
      "Train Loss: 5.7047\n",
      "Epoch 4909 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 4910 of 5000\n",
      "Train Loss: 5.6996\n",
      "Epoch 4911 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 4912 of 5000\n",
      "Train Loss: 5.6810\n",
      "Epoch 4913 of 5000\n",
      "Train Loss: 5.6880\n",
      "Epoch 4914 of 5000\n",
      "Train Loss: 5.6921\n",
      "Epoch 4915 of 5000\n",
      "Train Loss: 5.7038\n",
      "Epoch 4916 of 5000\n",
      "Train Loss: 5.6761\n",
      "Epoch 4917 of 5000\n",
      "Train Loss: 5.6697\n",
      "Epoch 4918 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4919 of 5000\n",
      "Train Loss: 5.6701\n",
      "Epoch 4920 of 5000\n",
      "Train Loss: 5.7036\n",
      "Epoch 4921 of 5000\n",
      "Train Loss: 5.6874\n",
      "Epoch 4922 of 5000\n",
      "Train Loss: 5.6807\n",
      "Epoch 4923 of 5000\n",
      "Train Loss: 5.6865\n",
      "Epoch 4924 of 5000\n",
      "Train Loss: 5.7107\n",
      "Epoch 4925 of 5000\n",
      "Train Loss: 5.6809\n",
      "Epoch 4926 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 4927 of 5000\n",
      "Train Loss: 5.6885\n",
      "Epoch 4928 of 5000\n",
      "Train Loss: 5.6978\n",
      "Epoch 4929 of 5000\n",
      "Train Loss: 5.6654\n",
      "Epoch 4930 of 5000\n",
      "Train Loss: 5.7194\n",
      "Epoch 4931 of 5000\n",
      "Train Loss: 5.7005\n",
      "Epoch 4932 of 5000\n",
      "Train Loss: 5.7055\n",
      "Epoch 4933 of 5000\n",
      "Train Loss: 5.6864\n",
      "Epoch 4934 of 5000\n",
      "Train Loss: 5.6976\n",
      "Epoch 4935 of 5000\n",
      "Train Loss: 5.6776\n",
      "Epoch 4936 of 5000\n",
      "Train Loss: 5.6872\n",
      "Epoch 4937 of 5000\n",
      "Train Loss: 5.6878\n",
      "Epoch 4938 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4939 of 5000\n",
      "Train Loss: 5.6969\n",
      "Epoch 4940 of 5000\n",
      "Train Loss: 5.6577\n",
      "Epoch 4941 of 5000\n",
      "Train Loss: 5.6924\n",
      "Epoch 4942 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 4943 of 5000\n",
      "Train Loss: 5.6973\n",
      "Epoch 4944 of 5000\n",
      "Train Loss: 5.6763\n",
      "Epoch 4945 of 5000\n",
      "Train Loss: 5.6715\n",
      "Epoch 4946 of 5000\n",
      "Train Loss: 5.7029\n",
      "Epoch 4947 of 5000\n",
      "Train Loss: 5.6994\n",
      "Epoch 4948 of 5000\n",
      "Train Loss: 5.6847\n",
      "Epoch 4949 of 5000\n",
      "Train Loss: 5.6738\n",
      "Epoch 4950 of 5000\n",
      "Train Loss: 5.6935\n",
      "Epoch 4951 of 5000\n",
      "Train Loss: 5.6841\n",
      "Epoch 4952 of 5000\n",
      "Train Loss: 5.6890\n",
      "Epoch 4953 of 5000\n",
      "Train Loss: 5.7015\n",
      "Epoch 4954 of 5000\n",
      "Train Loss: 5.6844\n",
      "Epoch 4955 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 4956 of 5000\n",
      "Train Loss: 5.6827\n",
      "Epoch 4957 of 5000\n",
      "Train Loss: 5.6884\n",
      "Epoch 4958 of 5000\n",
      "Train Loss: 5.6918\n",
      "Epoch 4959 of 5000\n",
      "Train Loss: 5.6832\n",
      "Epoch 4960 of 5000\n",
      "Train Loss: 5.6868\n",
      "Epoch 4961 of 5000\n",
      "Train Loss: 5.7046\n",
      "Epoch 4962 of 5000\n",
      "Train Loss: 5.6725\n",
      "Epoch 4963 of 5000\n",
      "Train Loss: 5.6953\n",
      "Epoch 4964 of 5000\n",
      "Train Loss: 5.6763\n",
      "Epoch 4965 of 5000\n",
      "Train Loss: 5.6898\n",
      "Epoch 4966 of 5000\n",
      "Train Loss: 5.6945\n",
      "Epoch 4967 of 5000\n",
      "Train Loss: 5.6958\n",
      "Epoch 4968 of 5000\n",
      "Train Loss: 5.6849\n",
      "Epoch 4969 of 5000\n",
      "Train Loss: 5.7006\n",
      "Epoch 4970 of 5000\n",
      "Train Loss: 5.6930\n",
      "Epoch 4971 of 5000\n",
      "Train Loss: 5.6688\n",
      "Epoch 4972 of 5000\n",
      "Train Loss: 5.6675\n",
      "Epoch 4973 of 5000\n",
      "Train Loss: 5.6765\n",
      "Epoch 4974 of 5000\n",
      "Train Loss: 5.6682\n",
      "Epoch 4975 of 5000\n",
      "Train Loss: 5.7090\n",
      "Epoch 4976 of 5000\n",
      "Train Loss: 5.7030\n",
      "Epoch 4977 of 5000\n",
      "Train Loss: 5.6843\n",
      "Epoch 4978 of 5000\n",
      "Train Loss: 5.6922\n",
      "Epoch 4979 of 5000\n",
      "Train Loss: 5.6846\n",
      "Epoch 4980 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 4981 of 5000\n",
      "Train Loss: 5.7034\n",
      "Epoch 4982 of 5000\n",
      "Train Loss: 5.7045\n",
      "Epoch 4983 of 5000\n",
      "Train Loss: 5.6775\n",
      "Epoch 4984 of 5000\n",
      "Train Loss: 5.6700\n",
      "Epoch 4985 of 5000\n",
      "Train Loss: 5.6734\n",
      "Epoch 4986 of 5000\n",
      "Train Loss: 5.6850\n",
      "Epoch 4987 of 5000\n",
      "Train Loss: 5.7070\n",
      "Epoch 4988 of 5000\n",
      "Train Loss: 5.6806\n",
      "Epoch 4989 of 5000\n",
      "Train Loss: 5.6892\n",
      "Epoch 4990 of 5000\n",
      "Train Loss: 5.7011\n",
      "Epoch 4991 of 5000\n",
      "Train Loss: 5.7192\n",
      "Epoch 4992 of 5000\n",
      "Train Loss: 5.6837\n",
      "Epoch 4993 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 4994 of 5000\n",
      "Train Loss: 5.6941\n",
      "Epoch 4995 of 5000\n",
      "Train Loss: 5.6837\n",
      "Epoch 4996 of 5000\n",
      "Train Loss: 5.6685\n",
      "Epoch 4997 of 5000\n",
      "Train Loss: 5.7157\n",
      "Epoch 4998 of 5000\n",
      "Train Loss: 5.6789\n",
      "Epoch 4999 of 5000\n",
      "Train Loss: 5.7024\n",
      "Epoch 5000 of 5000\n",
      "Train Loss: 5.6997\n",
      "Epoch 5001 of 5000\n",
      "Train Loss: 5.7113\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "#criterion = scaled_mse_loss\n",
    "\n",
    "train_loss = []\n",
    "for beta in [1]:\n",
    "    for zdim in [2]:\n",
    "        curr = 0\n",
    "        epochs = 5000\n",
    "\n",
    "        \n",
    "        model = model_conv_bigger.VAE_CNN_version_2_24x24(zDim=zdim).to(device)\n",
    "        lr = 0.001\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        #criterion = nn.BCELoss(reduction='sum')\n",
    "        criterion = nn.MSELoss(reduction='sum')\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                            mode='min',\n",
    "                                                            factor=0.2,\n",
    "                                                            patience=20,\n",
    "                                                            min_lr=5e-6)\n",
    "\n",
    "\n",
    "        for epoch in range(curr, curr+epochs+1):\n",
    "            print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "            train_epoch_loss = fit(model, rois, batch_size=batch_size, beta=beta, zdim=zdim)\n",
    "            train_loss.append(train_epoch_loss)\n",
    "            print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "            if epoch % 5000 == 0:\n",
    "                torch.save(model.state_dict(), f\"/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/outputs/real_images/models/VAE_real_-1-7scaling_data_z_only_1_45deg_beta{beta}_{epoch}_zdim{zdim}_test_mask_loss.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "type",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20x1 and 2x2304)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m zs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinspace(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, n_steps)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 14\u001b[0m images \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecoder(zs)\n\u001b[1;32m     15\u001b[0m mus, logvar \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencoder(images)\n\u001b[1;32m     16\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/src/model_conv_bigger.py:337\u001b[0m, in \u001b[0;36mVAE_CNN_version_2_24x24.decoder\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecoder\u001b[39m(\u001b[39mself\u001b[39m, z):\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m     \u001b[39m# z is fed back into a fully-connected layers and then into two transpose convolutional layers\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[39m# The generated output is the same size of the original input\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mleaky_relu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecFC1(z))\n\u001b[1;32m    338\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m6\u001b[39m)\n\u001b[1;32m    340\u001b[0m     \u001b[39m#identity = x \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/compile_spline/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/compile_spline/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (20x1 and 2x2304)"
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    " \n",
    "with torch.no_grad():\n",
    "\n",
    "    #model = model_conv_bigger.VAE_CNN_version_2_24x24(zDim=1).to(device)\n",
    "    #model.load_state_dict(torch.load(f\"/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/outputs/real_images/models/VAE_real_-1-7scaling_data_z_only_1_45deg_beta1_5000.pt\"))\n",
    "    #model.load_state_dict(torch.load(f\"/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/outputs/real_images/models/VAE_real_-1-7scaling_data_z_only_1_45deg_beta1_0_zdim1_test_mask_loss.pt\"))\n",
    "    n_steps =20\n",
    "\n",
    "    xyz = torch.ones((n_steps,3))*32\n",
    "    zs = torch.linspace(-1, 1, n_steps).view(-1,1).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    images = model.decoder(zs)\n",
    "    mus, logvar = model.encoder(images)\n",
    "    images = images.cpu().detach().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(2,10, figsize=(25,5))\n",
    "    for j in range(images.shape[0]):\n",
    "        ax = axs[j//10][j%10]\n",
    "        ax.imshow(images[j,0,...], cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'z = {zs[j].item():.2} ({mus[j].item():.2}, {logvar[j].item():.2})')\n",
    "\n",
    "    #plt.savefig(f'/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/outputs/real_images/figures/traversal_VAE_real_-1-7scaling_data_z_only_1_45deg_beta1_5000', transparent=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model = model_conv_bigger.VAE_CNN_version_2_24x24(zDim=1).to(device)\n",
    "    model.load_state_dict(torch.load(f\"/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/outputs/real_images/models/VAE_real_-1-7scaling_data_z_only_1_45deg_beta1_5000.pt\"))\n",
    "\n",
    "    model.eval()\n",
    "    unshifted = paired_rois[:,0,:,:] \n",
    "    out, mu, logVar, z = model(unshifted.unsqueeze(1).to(device))\n",
    "    latent_space_unshifted = mu.detach().cpu().numpy()\n",
    "    latent_space_unshifted = latent_space_unshifted[np.logical_not(np.isnan(latent_space_unshifted))]\n",
    "    shifted = paired_rois[:,1,:,:] \n",
    "    out, mu, logVar, z = model(shifted.unsqueeze(1).to(device))\n",
    "    latent_space_shifted = mu.detach().cpu().numpy()\n",
    "    latent_space_shifted = latent_space_shifted[np.logical_not(np.isnan(latent_space_shifted))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #model = model_conv_bigger.VAE_CNN_version_2_24x24(zDim=1).to(device)\n",
    "    #model.load_state_dict(torch.load(f\"/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/outputs/real_images/models/VAE_real_-1-7scaling_data_z_only_1_45deg_beta1_5000.pt\"))\n",
    "\n",
    "    model.eval()\n",
    "    out, mu, logVar, z = model(rois.unsqueeze(1).to(device))\n",
    "    latent_space = mu.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Latent space real images -1-7')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(latent_space[:, 0], latent_space[:, 1], alpha=0.1)\n",
    "plt.title('Latent space real images -1-7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = model_conv_bigger.VAE_CNN_version_2_24x24(zDim=2).to(device)\n",
    "model.load_state_dict(torch.load(f\"/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/outputs/real_images/models/VAE_real_-1-7scaling_data_z_only_1_45deg_beta1_5000.pt\"))\n",
    "#model.load_state_dict(torch.load(f\"/home/axtr7550/Chromosome_project/cellbgnet/Axel_notebook/VAE_tutorial/outputs/real_images/models/VAE_real_-1-7scaling_data_z_only_1_45deg_beta1_0_zdim2_test_mask_loss.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    n_steps = 10\n",
    "    z_dim = 2\n",
    "    xyz = torch.ones((n_steps,3))*32\n",
    "    zs = torch.linspace(-1, 1, n_steps).view(-1,).to(device)\n",
    "\n",
    "    if z_dim == 2:\n",
    "        zs = torch.stack((zs.repeat(n_steps), zs.repeat_interleave(n_steps)), dim=1)\n",
    "\n",
    "    model.eval()\n",
    "    images = model.decoder(zs)\n",
    "    mus, logvar = model.encoder(images)\n",
    "    images = images.cpu().detach().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(10,10, figsize=(10,10), layout='tight')\n",
    "    for j in range(images.shape[0]):\n",
    "        ax = axs[j//10][j%10]\n",
    "        ax.imshow(images[j,0,...], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "type",
     "evalue": "name 'latent_space_unshifted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Remove large outliers in latent space\u001b[39;00m\n\u001b[1;32m      2\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[0;32m----> 3\u001b[0m plt\u001b[39m.\u001b[39mhist(latent_space_unshifted, bins\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39munshifted\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mhist(latent_space_shifted, bins\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mshifted\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m ax\u001b[39m.\u001b[39mset_title(\u001b[39m'\u001b[39m\u001b[39mLatent_space\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent_space_unshifted' is not defined"
     ]
    }
   ],
   "source": [
    "# Remove large outliers in latent space\n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(latent_space_unshifted, bins=100, label='unshifted')\n",
    "plt.hist(latent_space_shifted, bins=100, alpha=0.5, label='shifted')\n",
    "\n",
    "ax.set_title('Latent_space')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f449b5d8e90>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.linear_model as lm\n",
    "fig, ax = plt.subplots()\n",
    "diff = latent_space_shifted-latent_space_unshifted\n",
    "lr = lm.LinearRegression().fit(latent_space_unshifted.reshape(-1,1), diff.reshape(-1,1))\n",
    "\n",
    "plt.plot(latent_space_unshifted, diff, 'o')\n",
    "plt.plot(latent_space_unshifted, (lr.intercept_ + lr.coef_ * latent_space_unshifted)[0,:], 'b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3748770480529363"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(latent_space_unshifted.reshape(-1,1), diff.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb3810571d0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout\n",
    "\n",
    "n_steps = 1000\n",
    "zs = torch.linspace(-4, 4, n_steps).view(-1,1).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "images = model.decoder(zs)\n",
    "cpu_images = images.detach().to('cpu').numpy()\n",
    "plt.imshow(cpu_images[0, 0,...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slider =widgets.IntSlider(\n",
    "    min=0,\n",
    "    max=rois.shape[0]-1,\n",
    "    step=1,\n",
    "    description='Slider:',\n",
    "    value=3\n",
    ")\n",
    "btn = widgets.Button(description='Bad')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_rois = rois.to('cpu').numpy()\n",
    "\n",
    "model.eval()\n",
    "rois = rois.to(device)\n",
    "mu, logvar = model.encoder(rois.unsqueeze(1))\n",
    "mu = mu.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6448, 24, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb38031a7d0>]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "roi_base = create_rois(image_coords, images)\n",
    "print(roi_base.shape)\n",
    "intensity = roi_base.sum(axis=1).sum(axis=1)\n",
    "\n",
    "plt.plot(mu,intensity , 'o', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = widgets.Output(layout=Layout(width='70%', height='500px'))\n",
    "\n",
    "def on_value_changed(val):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        i = slider.value\n",
    "        image = cpu_rois[i,...]\n",
    "        plt.imshow(image, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "slider.observe(on_value_changed, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pos01_img_100.tif</th>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos01_img_101.tif</th>\n",
       "      <td>308</td>\n",
       "      <td>308</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos02_img_100.tif</th>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos02_img_101.tif</th>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos03_img_100.tif</th>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos03_img_101.tif</th>\n",
       "      <td>239</td>\n",
       "      <td>239</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos04_img_100.tif</th>\n",
       "      <td>372</td>\n",
       "      <td>372</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos04_img_101.tif</th>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos05_img_100.tif</th>\n",
       "      <td>408</td>\n",
       "      <td>408</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos05_img_101.tif</th>\n",
       "      <td>382</td>\n",
       "      <td>382</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos06_img_100.tif</th>\n",
       "      <td>356</td>\n",
       "      <td>356</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos06_img_101.tif</th>\n",
       "      <td>311</td>\n",
       "      <td>311</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos07_img_100.tif</th>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos07_img_101.tif</th>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos08_img_100.tif</th>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos08_img_101.tif</th>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos09_img_100.tif</th>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos09_img_101.tif</th>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos10_img_100.tif</th>\n",
       "      <td>466</td>\n",
       "      <td>466</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos10_img_101.tif</th>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Image    x    y\n",
       "img_name                          \n",
       "Pos01_img_100.tif    333  333  333\n",
       "Pos01_img_101.tif    308  308  308\n",
       "Pos02_img_100.tif    354  354  354\n",
       "Pos02_img_101.tif    340  340  340\n",
       "Pos03_img_100.tif    246  246  246\n",
       "Pos03_img_101.tif    239  239  239\n",
       "Pos04_img_100.tif    372  372  372\n",
       "Pos04_img_101.tif    351  351  351\n",
       "Pos05_img_100.tif    408  408  408\n",
       "Pos05_img_101.tif    382  382  382\n",
       "Pos06_img_100.tif    356  356  356\n",
       "Pos06_img_101.tif    311  311  311\n",
       "Pos07_img_100.tif    339  339  339\n",
       "Pos07_img_101.tif    310  310  310\n",
       "Pos08_img_100.tif    294  294  294\n",
       "Pos08_img_101.tif    230  230  230\n",
       "Pos09_img_100.tif    199  199  199\n",
       "Pos09_img_101.tif    179  179  179\n",
       "Pos10_img_100.tif    466  466  466\n",
       "Pos10_img_101.tif    431  431  431"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>img_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>407</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>380</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>465</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>527</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>615</td>\n",
       "      <td>Pos01_img_100.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6443</th>\n",
       "      <td>19</td>\n",
       "      <td>681</td>\n",
       "      <td>738</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6444</th>\n",
       "      <td>19</td>\n",
       "      <td>684</td>\n",
       "      <td>723</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6445</th>\n",
       "      <td>19</td>\n",
       "      <td>737</td>\n",
       "      <td>966</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6446</th>\n",
       "      <td>19</td>\n",
       "      <td>820</td>\n",
       "      <td>675</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6447</th>\n",
       "      <td>19</td>\n",
       "      <td>910</td>\n",
       "      <td>564</td>\n",
       "      <td>Pos10_img_101.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6448 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Image    x    y           img_name\n",
       "0         0  101  407  Pos01_img_100.tif\n",
       "1         0  103  380  Pos01_img_100.tif\n",
       "2         0  106  465  Pos01_img_100.tif\n",
       "3         0  111  527  Pos01_img_100.tif\n",
       "4         0  114  615  Pos01_img_100.tif\n",
       "...     ...  ...  ...                ...\n",
       "6443     19  681  738  Pos10_img_101.tif\n",
       "6444     19  684  723  Pos10_img_101.tif\n",
       "6445     19  737  966  Pos10_img_101.tif\n",
       "6446     19  820  675  Pos10_img_101.tif\n",
       "6447     19  910  564  Pos10_img_101.tif\n",
       "\n",
       "[6448 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces_to_remove = []\n",
    "roi_base = create_rois(image_coords, images)\n",
    "intensity = roi_base.sum(axis=1).sum(axis=1)\n",
    "\n",
    "def on_button_click(b):\n",
    "    with out:\n",
    "        plt.plot(mu,intensity , 'o', alpha=0.05)\n",
    "        plt.plot(mu[slider.value],intensity[slider.value], 'bo', alpha=1)\n",
    "        plt.show()\n",
    "\n",
    "btn.on_click(on_button_click)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb281a39fd8474fa84207345ad26b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='500px', width='70%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986c9b12d2e74220854963663bf3785c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Bad', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4535bd49e98e4da09ec5e6d4147c1e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=3, description='Slider:', max=6001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(out)\n",
    "display(btn)\n",
    "display(slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "slider_latent =widgets.IntSlider(\n",
    "    min=0,\n",
    "    max=999,\n",
    "    step=1,\n",
    "    description='Slider:',\n",
    "    value=3\n",
    ")\n",
    "#btn = widgets.Button(description='Bad')\n",
    "\n",
    "out_latent = widgets.Output(layout=Layout(width='70%', height='300px'))\n",
    "\n",
    "def on_latent_value_changed(val):\n",
    "    with out_latent:\n",
    "        out_latent.clear_output()\n",
    "        i = slider_latent.value\n",
    "        image = cpu_images[i,0,...]\n",
    "        plt.imshow(image, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "slider_latent.observe(on_latent_value_changed, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "type",
     "evalue": "name 'out_latent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(out_latent)\n\u001b[1;32m      2\u001b[0m display(slider_latent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_latent' is not defined"
     ]
    }
   ],
   "source": [
    "display(out_latent)\n",
    "display(slider_latent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compile_spline",
   "language": "python",
   "name": "compile_spline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
